{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Year Comparison"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Device Growth\n",
    "Article Count Growth\n",
    "Article Creation Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.3, but v1.0.4 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    }
   ],
   "source": [
    "# import wmfdata as wmf\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all, print_err\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import time\n",
    "import datetime as dt \n",
    "from datetime import datetime, timedelta, date\n",
    "import dateutil\n",
    "#from dateutil.relativedelta import relativedelta\n",
    "\n",
    "#%load_ext sql_magic\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_vars' (dict)\n",
      "Stored 'quality_vars' (dict)\n"
     ]
    }
   ],
   "source": [
    "%run 2b_data_handling.ipynb\n",
    "%store -r query_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Devices - country <a class=\"anchor\" id=\"devices\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_uds_r = spark.run(\"\"\"\n",
    "select\n",
    "    country,\n",
    "    year,\n",
    "    sum(uniques_estimate) \n",
    "from wmf.unique_devices_per_domain_monthly\n",
    "where\n",
    "    year >= 2016\n",
    "    AND country IN ({glow_countries})\n",
    "group by country, year\n",
    "\"\"\".format(**query_vars)) \n",
    "\n",
    "#  CONCAT(year,LPAD(month,2,'0')) >= 201809\n",
    "#  AND CONCAT(year,LPAD(month,2,'0')) < 201910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_uds = co_uds_r.pivot(index='country', columns='year', values='sum(uniques_estimate)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_uds.columns = ['2016','2017','2018', '2019', '2020_to_April']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "%precision %.2f\n",
    "co_uds['2016'] = pd.Series([\"{:,}\".format(val) for val in co_uds['2016']], index = co_uds.index)\n",
    "co_uds['2017'] = pd.Series([\"{:,}\".format(val) for val in co_uds['2017']], index = co_uds.index)\n",
    "co_uds['2018'] = pd.Series([\"{:,}\".format(val) for val in co_uds['2018']], index = co_uds.index)\n",
    "\n",
    "co_uds['2019_to_nov'] = pd.Series([\"{:,}\".format(val * 100) for val in co_uds['2019_to_nov']], index = co_uds.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_uds['CAGR'] = ((co_uds['2019']/co_uds['2016']) **(1/4)-1)\n",
    "\n",
    "#https://stackoverflow.com/questions/23981601/format-certain-floating-dataframe-columns-into-percentage-in-pandas\n",
    "co_uds['CAGR'] = pd.Series([\"{0:.2f}%\".format(val * 100) for val in co_uds['CAGR']], index = co_uds.index)\n",
    "\n",
    "co_uds['2019_monthly_avg'] = (co_uds['2019']/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#co_uds_cols = co_uds.columns.tolist()\n",
    "co_uds_cols = ['2016', '2017', '2018', '2019', 'CAGR', '2019_monthly_avg','2020_to_April']\n",
    "co_uds = co_uds[co_uds_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_uds.to_csv(\"../../data/processed/query_results/regional_counts/co_uds_2020_april.csv\", sep=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Devices - domain <a class=\"anchor\" id=\"devices\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_uds_r = hive.run(\"\"\"\n",
    "select\n",
    "    domain, \n",
    "    country,\n",
    "    year,\n",
    "    sum(uniques_estimate) \n",
    "from wmf.unique_devices_per_domain_monthly\n",
    "where\n",
    "    year >= 2016\n",
    "    AND country IN ({glow_countries})\n",
    "    AND domain IN ({glow_domains})\n",
    "group by country, domain, year\n",
    "\"\"\".format(**query_vars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/35414625/pandas-how-to-run-a-pivot-with-a-multi-index\n",
    "do_uds_r.set_index(['country', 'domain', 'year']).unstack(level=-1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "do_uds_r.to_csv(\"../../data/processed/query_results/regional_counts/devices.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Devices - domain India <a class=\"anchor\" id=\"devices\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds_domain_r = hive.run(\"\"\"\n",
    "select\n",
    "    domain, \n",
    "    country,\n",
    "    year,\n",
    "    sum(uniques_estimate) \n",
    "from wmf.unique_devices_per_domain_monthly\n",
    "where\n",
    "    year >= 2016\n",
    "    AND country_code IN ({india_countries})\n",
    "    AND domain IN ({india_domains})\n",
    "group by country, domain, year\n",
    "\"\"\".format(**query_vars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds_domain_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds_domain = india_uds_domain_r.pivot(\n",
    "                                    index='domain',\n",
    "                                    columns='year', \n",
    "                                    values='_c3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds_domain.columns = ['2016','2017','2018', '2019_to_nov']\n",
    "\n",
    "india_uds_domain['CAGR'] = ((india_uds_domain['2018']/india_uds_domain['2016']) **(1/3)-1)\n",
    "\n",
    "#https://stackoverflow.com/questions/23981601/format-certain-floating-dataframe-columns-into-percentage-in-pandas\n",
    "india_uds_domain['CAGR'] = pd.Series([\"{0:.2f}%\".format(val * 100) for val in india_uds_domain['CAGR']], index = india_uds_domain.index)\n",
    "\n",
    "india_uds_domain['2018_monthly_avg'] = (india_uds_domain['2018']/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "india_uds_domain.sort_values(['2018', 'CAGR'], \n",
    "               ascending=[False, False])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "india_uds_domain.to_csv(\"./data/glow/india_uds_domain.csv\", sep=',', encoding = 'utf-8')\n",
    "\n",
    "../../data/processed/query_results/regional_counts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Articles<a class=\"anchor\" id=\"editors\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Article Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data using local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see 5b_collect_wiki_hist_article_counts.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean historical counts data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wc20 = pd.read_csv(\"../../data/processed/query_results/regional_counts/wiki_counts_indic_2020.csv\", thousands=',', encoding = 'utf-8', parse_dates=True)\n",
    "wc20['date'] = pd.to_datetime(wc20['date'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "row = wc20.loc[wc20['count'].isnull(), :]\n",
    "row"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wc20.dropna(subset=['count'], inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wiki_art_count_results_1619 = pd.read_csv('../../data/processed/query_results/regional_counts/wiki_counts_India_GLOW_2016_2019.csv', thousands=',',  encoding = 'utf-8')\n",
    "wiki_art_count_results_1920 = pd.read_csv('../../data/processed/query_results/regional_counts/wiki_counts_India_GLOW_2019_2020.csv', thousands=',', encoding = 'utf-8')\n",
    "wiki_art_count_results_arabic = pd.read_csv('../../data/processed/query_results/regional_counts/arabic/wiki_counts_arabic_GLOW_2016_2020.csv', thousands=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counts = pd.read_csv('../../data/processed/query_results/regional_counts/indonesia_wiki_counts.csv', thousands=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine files\n",
    "#wiki_counts = pd.concat([wiki_art_count_results_1920, wiki_art_count_results_1619], sort=True).drop_duplicates(subset=['count', 'date', 'lang'], keep='first').reset_index(drop =True)\n",
    "#drop na\n",
    "wiki_counts= wiki_counts.dropna()\n",
    "#datetime handling\n",
    "wiki_counts['date'] = pd.to_datetime(wiki_counts['date'])\n",
    "wiki_counts['month_year']= wiki_counts['date'].dt.to_period('M')\n",
    "wiki_counts['count'] = wiki_counts['count'].astype(str).astype(int)\n",
    "#wiki_counts[\"year\"] = wiki_counts['date'].dt.year\n",
    "#wiki_counts['month'] = wiki_counts['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_counts.to_csv(\"../../data/processed/query_results/regional_counts/wiki_counts_India_2016_2020.csv\", sep=',', encoding = 'utf-8', index=False)\n",
    "#wiki_counts.to_csv(\"../../data/processed/query_results/regional_counts/arabic/wiki_counts_mena_2016_2020.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_counts = pd.read_csv('../../data/processed/query_results/regional_counts/indonesia_wiki_counts.csv', thousands=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current article Count <a class=\"anchor\" id=\"new_articles\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#adapted from:\n",
    "#https://github.com/wikimedia-research/wiki-segmentation\n",
    "\n",
    "#https://www.mediawiki.org/wiki/Manual:Site_stats_table\n",
    "ac = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "    database() AS wiki,\n",
    "    ss_good_articles AS total_article_COUNT\n",
    "FROM site_stats\n",
    "\"\"\", indonesia_glow_wiki_dbs_mariadb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.to_csv(\"../../data/processed/query_results/regional_counts/ac2020_{}.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API counts \n",
    "##### note: edited-pages - root pages-related metrics on the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://wikimedia.org/api/rest_v1/#/Edited%20pages%20data\n",
    "#https://wikitech.wikimedia.org/wiki/Analytics/AQS/Wikistats_2#Total_article_count\n",
    "#https://phabricator.wikimedia.org/T240253\n",
    "#https://phabricator.wikimedia.org/T220524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of project URLs (each one in a 1-tuple)\n",
    "#https://hi.wikipedia.org\n",
    "\n",
    "wp_domains = hive.run(\"\"\"\n",
    "select domain_name\n",
    "from canonical_data.wikis\n",
    "where database_group = \"wikipedia\"\n",
    "      AND database_code IN ({indonesia_wiki_dbs})\n",
    "\"\"\".format(**query_vars))\n",
    "\n",
    "wp_domains_tuple_list = [tuple(r) for r in wp_domains .to_numpy()]\n",
    "num_domains = len(wp_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edited-pages/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pages created stats \n",
    "#The following parameters are {wiki}/{editor type}/{page type}/{granularity}/{start}/{end}\n",
    "\n",
    "#adapted from:\n",
    "#https://github.com/wikimedia-research/Editing-movement-metrics/blob/74dd6575703125a4386bfd8fea6546053458e2a6/02-calculation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content metrics via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_PAGES_API = (\n",
    "    # Replaces \"https://wikimedia.org/api/rest_v1/metrics/\" due to https://phabricator.wikimedia.org/P8605\n",
    "    \"http://aqs1004.eqiad.wmnet:7232/analytics.wikimedia.org/v1/\" \n",
    "    \"edited-pages/new/{project}/user/content/monthly/{start}/{end}\"\n",
    ")\n",
    "\n",
    "def get_new_pages(\n",
    "    project=\"all-projects\",\n",
    "    start=query_vars[\"api_metrics_month_start\"],\n",
    "    end=query_vars[\"api_metrics_month_end\"]\n",
    "):\n",
    "    url = NEW_PAGES_API.format(\n",
    "        project = project,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"])\n",
    "    frame = frame.rename(columns={\"timestamp\": \"month\"})\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now on the 0th project of 4 (id.wikipedia.org)\n"
     ]
    }
   ],
   "source": [
    "# Query the API for each project and append records to a list\n",
    "results = []\n",
    "\n",
    "for idx, val in enumerate(wp_domains_tuple_list):\n",
    "    domain = val[0]\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        msg = \"Now on the {}th project of {} ({})\"\n",
    "        print_err(msg.format(idx, num_domains, domain))\n",
    "        \n",
    "    frame = get_new_pages(project=domain).reset_index()\n",
    "    frame[\"project\"] = domain\n",
    "    records = frame.to_dict(\"records\")\n",
    "    results.extend(records)\n",
    "    \n",
    "    # Sleep 20 milliseconds\n",
    "    time.sleep(0.02)\n",
    "\n",
    "# Turn the big list of records into a data frame\n",
    "new_per_wp = pd.DataFrame(results)\n",
    "\n",
    "# Sum across projects to get new Wikipedia articles per month\n",
    "new_wp = new_per_wp.groupby(\"month\").agg(\n",
    "    {\"new_pages\": \"sum\"}\n",
    ").rename(columns={\"new_pages\": \"net_new_Wikipedia_articles\"}).reset_index();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean epn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/venv/lib/python3.5/site-packages/pandas/core/arrays/datetimes.py:1269: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Strip timezones returned by API so our month columns merge nicely\n",
    "new_per_wp[\"date\"] = pd.to_datetime(new_per_wp[\"month\"])\n",
    "new_per_wp['month_year']= new_per_wp['date'].dt.to_period('M')\n",
    "new_per_wp.rename(columns={\"new_pages\": \"net_new_content_pages\"});\n",
    "\n",
    "#make sure month column is in datetime format\n",
    "#new_per_wp['month'] = pd.to_datetime(new_per_wp['month'])\n",
    "\n",
    "#create new column, 'year'\n",
    "#new_per_wp['year'] = new_per_wp['month'].dt.year"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sum across projects to get new Wikipedia articles per month\n",
    "new_wp = new_per_wp.groupby(\"month\").agg({\"new_pages\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new countries column\n",
    "new_per_wp['countries'] = new_per_wp['project'].apply(add_country_column).str[0]\n",
    "#rename the MENA entry in the countries column\n",
    "new_per_wp['countries'] = new_per_wp['countries'].replace({'M':'MENA'})\n",
    "\n",
    "#format datetime column\n",
    "#new_per_wp['month'] = new_per_wp['month'].map(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "del new_per_wp['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_per_wp.to_csv(\"../../data/processed/query_results/regional_counts/edited_pages_new_{}.csv\",sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_per_wp.to_csv(\"../../data/processed/query_results/regional_counts/edited_pages_new_indonesia.csv\",sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edited-pages/aggregate<a class=\"anchor\" id=\"new_articles\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDITED_PAGES_AGG_API = (\n",
    "    \"http://aqs1004.eqiad.wmnet:7232/analytics.wikimedia.org/v1/\" \n",
    "    \"edited-pages/aggregate/{project}/user/content/all-activity-levels/monthly/20000101/20191201\"\n",
    ")\n",
    "\n",
    "# Create container for results\n",
    "api_epa_results = []\n",
    "\n",
    "def get_edited_pages_agg_count(\n",
    "    project=\"all-projects\",\n",
    "    start=query_vars[\"api_metrics_month_first_day\"],\n",
    "    end=query_vars[\"api_metrics_month_day_after\"]\n",
    "    \n",
    "):\n",
    "    epa_url = EDITED_PAGES_AGG_API.format(\n",
    "        project = project,\n",
    "        start = start,\n",
    "        end = end\n",
    "    )\n",
    "    \n",
    "    r = requests.get(epa_url, headers=headers)\n",
    "    data = r.json()[\"items\"][0][\"results\"]\n",
    "    frame = pd.DataFrame(data)\n",
    "    frame[\"timestamp\"] = pd.to_datetime(frame[\"timestamp\"])\n",
    "    frame = frame.rename(columns={\"timestamp\": \"month\"})\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Query the API for each project and append records to a list\n",
    "epm_results = []\n",
    "\n",
    "for idx, val in enumerate(wp_domains):\n",
    "    domain = val[0]\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        msg = \"Now on the {}th project of {} ({})\"\n",
    "        print(msg.format(idx, num_domains, domain))\n",
    "        \n",
    "    epm_frame = get_edited_pages_agg_count(project=domain).reset_index()\n",
    "    epm_frame[\"project\"] = domain\n",
    "    epm_records = epm_frame.to_dict(\"records\")\n",
    "    epm_results.extend(epm_records)\n",
    "    \n",
    "    # Sleep 20 milliseconds\n",
    "    time.sleep(0.02)\n",
    "\n",
    "# Turn the big list of records into a data frame\n",
    "edited_pages_m_r = pd.DataFrame(epm_results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EPA_FILENAME = \"../../data/processed/query_results/regional_counts/edited_pages_aggregate.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
