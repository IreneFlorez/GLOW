{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Page Ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load_submitted_titles_get_survival_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.3, but v1.0.4 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_vars' (dict)\n",
      "Stored 'quality_vars' (dict)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import urllib\n",
    "from urllib.parse import unquote\n",
    "\n",
    "%run 2b_data_handling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/raw/articles/2019/Indonesia/compiled/articles.csv\")\n",
    "#trailing rstrip\n",
    "df['page_title'] = df['page_title'].str.rstrip('_')\n",
    "#encoded URL to decoded title -- if there are nulls it will provide a float type error\n",
    "df['page_title'] = df['page_title'].apply(lambda x: unquote(x) if pd.notnull(x) else x).copy(deep=False) #apply if value not null\n",
    "df['page_title'] = df['page_title'].replace(' ', '_', regex=True)\n",
    "\n",
    "#also, drop items after a hashtag. eg: https://min.wikipedia.org/wiki/Senja_di_Jakarta#Carito_singkek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_edit_timestamp_articles = []\n",
    "\n",
    "def get_clean_ids_hive(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with pageids including redirect status info\n",
    "    '''\n",
    "\n",
    "    get_date_info_mult_articles_query = \"\"\"\n",
    "    SELECT\n",
    "        page_title,\n",
    "        wiki_db AS database_code,\n",
    "        page_id\n",
    "    FROM wmf.mediawiki_history \n",
    "    WHERE\n",
    "        snapshot = '2020-07'\n",
    "        AND page_namespace = 0\n",
    "        AND wiki_db = '{wiki_db}'\n",
    "        AND page_title IN {raw_articles}\n",
    "    GROUP BY \n",
    "        page_title, wiki_db, page_id\n",
    "    \"\"\"\n",
    "    \n",
    "    get_date_info_single_articles_query = \"\"\"\n",
    "    SELECT\n",
    "        page_title,\n",
    "        wiki_db AS database_code,\n",
    "        page_id\n",
    "    FROM wmf.mediawiki_history \n",
    "    WHERE\n",
    "        snapshot = '2020-07'\n",
    "        AND page_namespace = 0\n",
    "        AND wiki_db = '{wiki_db}'\n",
    "        AND page_title = '{article}'\n",
    "    GROUP BY \n",
    "        page_title, wiki_db, page_id   \n",
    "    \"\"\"\n",
    "        \n",
    "    for wiki in df['database_code'].unique(): \n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_title'].apply(pd.DataFrame)\n",
    "        raw_articles = tuple(list(grouping[wiki]))\n",
    "        article = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        if len(raw_articles)>= 2:\n",
    "            date_results = spark.run(get_date_info_mult_articles_query.format(raw_articles=raw_articles, wiki_db=wiki_db ))\n",
    "        else: date_results = spark.run(get_date_info_single_articles_query.format(article=article, wiki_db=wiki_db ))\n",
    "        first_edit_timestamp_articles.append(date_results)\n",
    "    return(first_edit_timestamp_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "\n",
    "articles = []\n",
    "\n",
    "def get_clean_ids_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with pageids for non redirect articles\n",
    "    '''\n",
    "\n",
    "    clean_id_query = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id\n",
    "    FROM page AS p1 \n",
    "        AND p1.page_title IN {raw_articles}\n",
    "    '''\n",
    "    \n",
    "    clean_id_query_one_article = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id\n",
    "    FROM page AS p1  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title = '{raw_articles}'\n",
    "    '''\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        print('***')\n",
    "        print(wiki)\n",
    "        \n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_title'].apply(pd.DataFrame)\n",
    "        raw_articles = tuple(list(grouping[wiki]))\n",
    "        article = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        \n",
    "        if len(raw_articles)>= 2:\n",
    "            redirects_r = mariadb.run(clean_id_query.format(raw_articles=raw_articles), wiki )\n",
    "        else: redirects_r = mariadb.run(clean_id_query_one_article.format(raw_articles=article), wiki )\n",
    "        articles.append(redirects_r)   \n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read df and query using Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_ids_hive(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.concat(first_edit_timestamp_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_deduped_prep = results.sort_values(by=[\"page_title\", 'database_code', 'page_id'], na_position='last').groupby([\"page_title\", 'database_code'])[\"page_id\"].first().reset_index()\n",
    "results_deduped = results_deduped_prep.drop_duplicates(subset=[\"page_title\", 'database_code'], keep='first')\n",
    "articles_pageids = results_deduped.merge(df, how=\"right\", on=['page_title', 'database_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = articles_pageids[articles_pageids['page_id'].isnull()]\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual review and data entry for those which obtained NaN results in query\n",
    "articles_pageids.loc[(articles_pageids.database_code == 'minwiki') & (articles_pageids.page_id == np.nan) & (articles_pageids.page_title == 'Senja_di_Jakarta#Carito_singkek'),'page_id'] = 326575\n",
    "#manual review and data entry for those which obtained NaN results in query\n",
    "articles_pageids.loc[(articles_pageids.database_code == 'jvwiki') & (articles_pageids.page_id == np.nan) & (articles_pageids.page_title == 'Surabaya_Samator#Pemain_Surabaya_Samator_2019'),'page_id'] = 163855\n",
    "\n",
    "articles_pageids.at[1792, 'page_id'] = 326575\n",
    "articles_pageids.at[1793, 'page_id'] = 163855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "articles_pageids.loc[articles_pageids['page_title'] == 'Senja_di_Jakarta#Carito_singkek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_pageids.to_csv(\"../../data/processed/query_results/content_quality/indonesia/articles_pageids_CLEAN.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: query using MariaDB for remaining IDS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['page_title'] = df['page_title'].replace('_', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clean_ids_mariadb(articles_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indonesia_articles = pd.concat(missing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
