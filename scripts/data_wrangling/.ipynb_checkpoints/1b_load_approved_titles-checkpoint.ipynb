{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Approved Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.3, but v1.0.4 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Indonesia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "files = glob.glob('../../data/raw/articles/2019/Indonesia/*.csv')\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df['filename'] = file\n",
    "    dfs.append(df)\n",
    "articles_raw = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles_raw.copy(deep=True)\n",
    "\n",
    "articles = articles.rename(columns={'Wiki':'language_name',\n",
    "                                    'Type of article': 'contest_article_type',\n",
    "                                    'Page name'  : 'url'  \n",
    "                       })\n",
    "\n",
    "#replace strings to comply with db table values\n",
    "articles = articles.replace({'language_name' : { 'Bahasa Indonesia' : 'Indonesian',\n",
    "                                                 'Sunda'            : 'Sundanese',\n",
    "                                                 'Jawa'             : 'Javanese'\n",
    "                                               }\n",
    "                            })\n",
    "\n",
    "#add page title column\n",
    "articles['page_title'] =  articles['url'].str.split('.wikipedia.org/wiki/').str[1]\n",
    "\n",
    "#remove typos - trailing comma at end of page_titles\n",
    "remove_char_list = [',', '__', '_']\n",
    "articles['page_title'] = articles['page_title'].str.rstrip(',')\n",
    "articles['page_title'] = articles['page_title'].str.rstrip('_')\n",
    "articles['page_title'] = articles['page_title'].str.rstrip('__')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ensure article links and article info link\n",
    "articles['filename'] =  articles['filename'].str.split('../../data/raw/articles/2019/Indonesia/').str[1]\n",
    "articles['url_base'] =  articles['url'].str.split('https://').str[1]\n",
    "articles['url_base'] = articles['url_base'].str.split('/').str[0]\n",
    "articles['url_article_info'] = 'https://xtools.wmflabs.org/articleinfo/'+articles['url_base']+'/'+articles['page_title']\n",
    "\n",
    "df = articles[['page_title', 'language_name', 'contest_article_type', 'filename', 'url', 'url_article_info']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['page_title'].str.contains('%',na=False)])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decoded_df = df[df['page_title'].str.contains('%',na=False)]\n",
    "df = df[~df['page_title'].str.contains('%',na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read India data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "india_filename = '../../data/raw/articles/2019/Tiger 2.0 - article page stats.xlsx'\n",
    "\n",
    "\n",
    "cols2skip = [0,2,3]\n",
    "cols = [i for i in range(10) if i not in cols2skip]\n",
    "dfs = pd.read_excel(indonesia_filename, sheet_name=None,  header=None, usecols=cols) #skiprows=1\n",
    "\n",
    "df = pd.concat([df.assign(name=n) for n,df in dfs.items()], sort=True)\n",
    "#df.columns = ['page_title', 'language_name']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check the langauage names (aka sheets by wiki) and make sure we can identify each language\n",
    "#pnbwiki\tWestern Punjabi\thttps://pnb.wikipedia.org/\n",
    "#pawiki\tPunjabi\tPunjabi(Gurumukhi)\thttps://pa.wikipedia.org/\n",
    "\n",
    "df['language_name'] = df['language_name'].replace('Bengali', 'Bangla')\n",
    "df['language_name'] = df['language_name'].replace('Punjabi(Gurumukhi)', 'Punjabi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get database_code and language_code, confirm language_code (if needed)\n",
    "lang_names =tuple(df['language_name'].unique())\n",
    "\n",
    "ci = wmf.hive.run(\"\"\"\n",
    "SELECT  language_code, database_code, language_name\n",
    "FROM canonical_data.wikis\n",
    "WHERE language_name IN {lang_names} AND database_group = 'wikipedia'\n",
    "\"\"\".format(lang_names=lang_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "df_ci = df.merge(ci, how=\"left\", on=['language_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ci_articles = df_ci.loc[df_ci['page_title'].notnull(), ['page_title', \n",
    "                                                           'language_name', \n",
    "                                                           'database_code', \n",
    "                                                           'language_code',\n",
    "                                                           'filename',\n",
    "                                                           'contest_article_type', \n",
    "                                                           'url',\n",
    "                                                           'url_article_info'\n",
    "                                                          ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the titles\n",
    "df_ci_articles['page_title'] = df_ci_articles['page_title'].str.replace(' ', '_').copy(deep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_ci_articles.to_csv(\"../../data/raw/articles/2019/clean/articles.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ci_articles.to_csv(\"../../data/raw/articles/2019/Indonesia/compiled/articles.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
