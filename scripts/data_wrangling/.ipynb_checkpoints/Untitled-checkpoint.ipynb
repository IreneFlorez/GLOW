{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wmfdata as wmf\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "#https://pa.wikipedia.org/wiki/%E0%A8%A6%E0%A9%87%E0%A8%B5%E0%A8%BF%E0%A8%95%E0%A8%BE_%E0%A8%B0%E0%A8%BE%E0%A8%A3%E0%A9%80\n",
    "#https://xtools.wmflabs.org/articleinfo/pa.wikipedia.org/%E0%A8%A6%E0%A9%87%E0%A8%B5%E0%A8%BF%E0%A8%95%E0%A8%BE_%E0%A8%B0%E0%A8%BE%E0%A8%A3%E0%A9%80\n",
    "\n",
    "iwl_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  linked_item.ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM (\n",
    "      SELECT ips_item_id\n",
    "      FROM wb_items_per_site\n",
    "      WHERE ips_item_id = '465060'\n",
    "      AND ips_site_id = 'pawiki'\n",
    "    ) AS linked_item\n",
    "LEFT JOIN wb_items_per_site \n",
    "  ON linked_item.ips_item_id = wb_items_per_site.ips_item_id\n",
    "LEFT JOIN page \n",
    "  ON linked_item.ips_item_id = page.page_id\n",
    "GROUP BY page_id\n",
    "\"\"\", \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>iwsites</th>\n",
       "      <th>iwsitelinks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465060</td>\n",
       "      <td>hiwiki, enwiki, urwiki, idwiki, pnbwiki, kowik...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      QID                                            iwsites  iwsitelinks\n",
       "0  465060  hiwiki, enwiki, urwiki, idwiki, pnbwiki, kowik...           27"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwl_r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nppt_articles_edits.groupby(['database_code']).agg(['nunique'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9772 entries, 0 to 9771\n",
      "Data columns (total 4 columns):\n",
      "page_title       9173 non-null object\n",
      "language_name    9772 non-null object\n",
      "database_code    9772 non-null object\n",
      "language_code    9772 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 381.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_ci.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ci['language_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = tuple(list(nnpt['database_code'].unique()))\n",
    "\n",
    "wd_vars.update({'wikis': wikis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pawiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'pawiki', 'page_title']))\n",
    "\n",
    "#update the query variable to use it in queries\n",
    "wd_vars.update({'pawiki_titles_normalized': pawiki_titles_normalized,\n",
    "                })\n",
    "pa_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = pawiki_titles_normalized), 'pawiki')\n",
    "#add dbcolumn to each query df\n",
    "pa_ids_r['database_code'] = 'pawiki' \n",
    "\n",
    "nppt_ids = pd.concat([pa_ids_r, \n",
    "                      ml_ids_r,\n",
    "                      hi_ids_r,\n",
    "                      or_ids_r,\n",
    "                      ur_ids_r,\n",
    "                      ta_ids_r,\n",
    "                      kn_ids_r,\n",
    "                      mr_ids_r,\n",
    "                      gu_ids_r,\n",
    "                      te_ids_r,\n",
    "                      bn_ids_r,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_ids.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aswiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'aswiki', 'page_title']))\n",
    "bnwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'bnwiki', 'page_title']))\n",
    "guwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'guwiki', 'page_title']))\n",
    "hiwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'hiwiki', 'page_title']))\n",
    "knwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'knwiki', 'page_title']))\n",
    "mlwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'mlwiki', 'page_title']))\n",
    "mrwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'mrwiki', 'page_title']))\n",
    "orwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'orwiki', 'page_title']))\n",
    "pawiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'pawiki', 'page_title']))\n",
    "sawiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'sawiki', 'page_title']))\n",
    "satwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'satwiki', 'page_title']))\n",
    "tawiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'tawiki', 'page_title']))\n",
    "tewiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'tewiki', 'page_title']))\n",
    "tcywiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'tcywiki', 'page_title']))\n",
    "urwiki_titles_normalized = tuple(list(articles_s.loc[articles_s['language_code'] == 'urwiki', 'page_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dict of DataFrames by convert groupby object to tuples and then to dict:\n",
    "GLOW_India_article_lists_dict = dict(tuple(articles_s.groupby('language_code'))) #wiki is a 2 letter code dict(tuple(CIS_submission.groupby('wiki'))) #wiki is a 2 letter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GLOW_India_article_lists_dict = tuple(articles_s.groupby('language_code')) #wiki is a 2 letter code dict(tuple(CIS_submission.groupby('wiki'))) #wiki is a 2 letter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('as',                   page_title language_code\n",
       " 0        আনোৱাৰুদ্দিন চৌধুৰী            as\n",
       " 1              কাৱঁড় যাত্ৰা            as\n",
       " 2                      ভটিমা            as\n",
       " 3            বগা বাবাৰ মাজাৰ            as\n",
       " 4                  পোৱামক্কা            as\n",
       " ..                       ...           ...\n",
       " 205            টিলিঙা মন্দিৰ            as\n",
       " 206                      NaN            as\n",
       " 207  সুব্ৰহ্মণ্যন চন্দ্ৰশেখৰ            as\n",
       " 208                      NaN            as\n",
       " 209   অসম ৰাজ্যিক সংগ্ৰহালয়            as\n",
       " \n",
       " [210 rows x 2 columns])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df with 2 columns, wiki and titles, where titles are a list of values\n",
    "GLOW_India_article_lists_dict = articles_s.groupby('language_code').titles.apply(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def raw_pageids():\n",
    "    for each unique x in column:\n",
    "        page_ids = tuple(list(CIS_submission ['page_id']))\n",
    "\n",
    "def raw_pageids2():\n",
    "    for k in GLOW_India_article_lists_df:\n",
    "        page_ds = tuple(list(k[value]))\n",
    "        \n",
    "def run_code():\n",
    "    for list in GLOW_India_article_lists2:\n",
    "        run code below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "GLOW_India_article_lists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO Incentive type > categorical (can do ordering with pandas)..but Hive may not allow for categorical. parquet maybe\n",
    "\n",
    "1. Get list of articles from WAM and WLW and filter those out from the GLOW data. \n",
    "2. Get list of last years editors\n",
    "3. get list of jury? >> seperate table - jury "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = {\n",
    "    'PA': {'title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'namespace': 0},\n",
    "    'ML': {'title' :'പ്രധാന_താൾ', 'namespace': 0},\n",
    "    'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "    'HI': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "    'PNB': {'title' : 'پہلا_صفہ', 'namespace': 0},\n",
    "    'TA': {'title' : 'முதற்_பக்கம்', 'namespace': 0},\n",
    "    'TE': {'title' : 'మొదటి_పేజీ', 'namespace': 0}, \n",
    "    'AS': {'title' : 'বেটুপাত', 'namespace': 0},\n",
    "    'SA': {'title' : 'मुख्यपृष्ठम्', 'namespace': 0},\n",
    "    'KN': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "    'TCY': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "    'GU': {'title' : 'મુખપૃષ્ઠ', 'namespace': 0},\n",
    "    'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "    'MR': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "    'SAT': {'title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'namespace': 0},\n",
    "    'UR': {'title' : 'صفحۂ_اول', 'namespace': 0}, \n",
    "    'OR': {'title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'namespace': 0}   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = {\n",
    "        {'Punjabi': 'PA', mainpage = {'mainpage_title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'mainpage_namespace': 0}},\n",
    "        {'PNB' \n",
    "        mainpage = {'mainpage_title' : 'پہلا_صفہ', 'mainpage_namespace': 0}\n",
    "        \n",
    "        {'Malayalam': 'ML', mainpage = {'mainpage_title' :'പ്രധാന_താൾ', 'mainpage_namespace': 0}},\n",
    "        {'Bengali':  'BN', mainpage = {'mainpage_title' : 'প্রধান_পাতা', 'mainpage_namespace': 0}},\n",
    "        {'Hindi': 'HI', mainpage = {'mainpage_title' : 'मुखपृष्ठ', 'mainpage_namespace': 0}},\n",
    "        {'Tamil': 'TA', mainpage = {'mainpage_title' : 'முதற்_பக்கம்', 'mainpage_namespace': 0}},\n",
    "        {'Telugu': 'TE', mainpage = {'mainpage_title' : 'మొదటి_పేజీ', 'mainpage_namespace': 0}}, \n",
    "        {'Assamese': 'AS', mainpage = {'mainpage_title' : 'বেটুপাত', 'mainpage_namespace': 0}},\n",
    "        {'Sanskrit': 'SA', mainpage = {'mainpage_title' : 'मुख्यपृष्ठम्', 'mainpage_namespace': 0}},\n",
    "        {'Kannada': 'KN', mainpage = {'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', 'mainpage_namespace': 0}},\n",
    "        {'Gujarati': 'GU', mainpage = {'mainpage_title' : 'મુખપૃષ્ઠ', 'mainpage_namespace': 0}},\n",
    "        {'Bengali': 'BN', mainpage = {'mainpage_title' : 'প্রধান_পাতা', 'mainpage_namespace': 0}},\n",
    "        {'Tulu': 'TCY', mainpage = {'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', 'mainpage_namespace': 0}},\n",
    "        {'Marathi': 'MR', mainpage = {'mainpage_title' : 'मुखपृष्ठ', 'mainpage_namespace': 0}},\n",
    "        {'Santali': 'SAT', mainpage = {'mainpage_title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'mainpage_namespace': 0}},\n",
    "        {'Urdu': 'UR', mainpage = {'mainpage_title' : 'صفحۂ_اول', 'mainpage_namespace': 0}}, \n",
    "        {'Odia': 'OR', mainpage = {'mainpage_title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'mainpage_namespace': 0}}   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi2 = {\n",
    "    'PA': {\n",
    "            'wikiname': 'Punjabi'\n",
    "            'mainpage_title' : 'ਮੁੱਖ_ਸਫ਼ਾ', \n",
    "            'mainpage_namespace': 0},\n",
    "    \n",
    "    'ML': {'wikiname': 'Malayalam'\n",
    "        'mainpage_title' :'പ്രധാന_താൾ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'BN': {'wikiname': 'Bengali'\n",
    "           'mainpage_title' : 'প্রধান_পাতা', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'HI': {'wikiname': 'Hindi'\n",
    "           'mainpage_title' : 'मुखपृष्ठ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'PNB': {'wikiname': \n",
    "           'mainpage_title' : 'پہلا_صفہ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'TA': {'wikiname': 'Tamil'\n",
    "           'mainpage_title' : 'முதற்_பக்கம்', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'TE': {'wikiname': 'Telugu'\n",
    "        'mainpage_title' : 'మొదటి_పేజీ', \n",
    "           'mainpage_namespace': 0}, \n",
    "           \n",
    "    'AS': {'wikiname': 'Assamese'\n",
    "           'mainpage_title' : 'বেটুপাত', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'SA': {'wikiname': 'Sanskrit'\n",
    "           'mainpage_title' : 'मुख्यपृष्ठम्', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'KN': {'wikiname': 'Kannada'\n",
    "           'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', \n",
    "           'mainpage_namespace': 0},\n",
    "            \n",
    "    'GU': {'wikiname':  'Gujarati'\n",
    "           'mainpage_title' : 'મુખપૃષ્ઠ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'BN': {'wikiname': 'Bengali'\n",
    "           'mainpage_title' : 'প্রধান_পাতা', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'TCY': {'wikiname': 'Tulu'\n",
    "           'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'MR': {'wikiname': 'Marathi'\n",
    "           'mainpage_title' : 'मुखपृष्ठ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'SAT': {'wikiname': 'Santali'\n",
    "           'mainpage_title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'UR': {'wikiname': 'Urdu'\n",
    "           'mainpage_title' : 'صفحۂ_اول', \n",
    "           'mainpage_namespace': 0}, \n",
    "           \n",
    "    'OR': {'wikiname':'Odia'\n",
    "           'mainpage_title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', \n",
    "           'mainpage_namespace': 0}   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for dupes\n",
    "z = len(idc)-idc.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_name     288\n",
       "contest_code    8788\n",
       "dateAdded         13\n",
       "finish          8810\n",
       "start           8812\n",
       "user_name       8432\n",
       "wiki            8802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article_name',\n",
       " 'contest_code',\n",
       " 'dateAdded',\n",
       " 'finish',\n",
       " 'jury',\n",
       " 'start',\n",
       " 'user_name',\n",
       " 'wiki']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idc_r.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "                get_date = driver.find_element_by_xpath(date_xpath).text\n",
    "                get_wiki_name = driver.find_elements_by_xpath (wiki_xpath)\n",
    "                get_article_count = driver.find_elements_by_xpath (ac_xpath)\n",
    "            \n",
    "            \n",
    "                ac_dict['date'] = date = get_date\n",
    "                ac_dict['lang'] = \"\".join([element.text for element in get_wiki_name]) \n",
    "                ac_dict['count'] = \"\".join([element.text for element in get_article_count]) \n",
    "            \n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "           \n",
    "            #print(ac_dict)\n",
    "            t_ac.append(ac_dict.copy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prettify data\n",
    "lens = [len(item) for item in core['articles']]\n",
    "df = pd.DataFrame({\n",
    "    'wiki':np.repeat(core['wiki'].values, lens),\n",
    "    'code':np.repeat(core['code'].values, lens),\n",
    "    'finish':np.repeat(core['finish'].values, lens),\n",
    "    'start':np.repeat(core['start'].values, lens),\n",
    "    'jury':np.repeat(core['jury'].values, lens),\n",
    "    'articles':np.hstack(core['articles']),\n",
    "                      })\n",
    "df = pd.concat([df_out.drop(['articles'], axis=1), df_out['articles'].apply(pd.Series)],axis=1)\n",
    "df = df[['wiki', 'code', 'start', 'finish', 'jury', 'dateAdded', 'name', 'user']]\n",
    "df.columns = ['wiki', 'code', 'start', 'finish', 'jury', 'dateAdded', 'article_name', 'editor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "'PNB': {'title' : 'پہلا_صفہ', 'namespace': 0},\n",
    "'AS': {'title' : 'বেটুপাত', 'namespace': 0},\n",
    "'SA': {'title' : 'मुख्यपृष्ठम्', 'namespace': 0},\n",
    "'TCY': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "\n",
    "'GU': {'title' : 'મુખપૃષ્ઠ', 'namespace': 0},\n",
    "'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "\n",
    "\n",
    "'ML': {'title' :'പ്രധാന_താൾ', 'namespace': 0},\n",
    "'PA': {'title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'namespace': 0},\n",
    "'SAT': {'title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'namespace': 0},\n",
    "'UR': {'title' : 'صفحۂ_اول', 'namespace': 0}, \n",
    "'TA': {'title' : 'முதற்_பக்கம்', 'namespace': 0},\n",
    "'TE': {'title' : 'మొదటి_పేజీ', 'namespace': 0}, \n",
    "'OR': {'title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'namespace': 0} \n",
    "'MR': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "'KN': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "'HI': {'title' : 'मुखपृष्ठ', 'namespace': 0},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "contests = {'wlw_2019':{'url_base': 'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-{}','urls': []},\n",
    "            'ptp_2018':{'url_base': 'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-{}','urls':[]},\n",
    "            'wam_2019':{'url_base': 'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-{}', 'urls':[] }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-dictionaries-and-lists\n",
    "def gen_dict_extract(key, var):\n",
    "    if hasattr(var,'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attempt to get it to work for a dict of urls and add values back into dictionary\n",
    "\n",
    "#get working urls\n",
    "wikicodes = ['bn', 'ur', 'te', 'ta', 'sat', 'pa', 'mr', 'ml', 'kn', 'hi', 'gu', 'pnb', 'tcy', 'as', 'or','ml_or']\n",
    "\n",
    "url_bases = ['https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-',\n",
    "            'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-', #wlw_campaigns_2019\n",
    "            'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-']\n",
    "\n",
    "url_bases_gen = list(gen_dict_extract('url_base', contests))\n",
    "\n",
    "for url in url_bases_gen:\n",
    "    urls_to_review = list()\n",
    "    for wikicode in wikicodes:\n",
    "        urls = (url_base.format(wikicode))\n",
    "        urls_to_review.append(urls)\n",
    "\n",
    "        #'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-'\n",
    "        #'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-' #wlw_campaigns_2019\n",
    "\n",
    "    not_found_urls = list()\n",
    "    # Iterate here on the urls\n",
    "    # The below code could be executed for each url\n",
    "    for url in urls_to_review:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 404:\n",
    "            not_found_urls.append(url)\n",
    "\n",
    "    working_urls = list(set(urls_to_review)-set(not_found_urls))\n",
    "    \n",
    "    ### create new list of urls for each url in url_bases_gen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aneurysm\t'hiwiki'\t'धमनी_विस्फार' \n",
    "Flax\t'guwiki'\t'અળશી'\n",
    "Root_canal_treatment\t'tawiki'\t'பல்லுட்புறச்_சிகிச்சை'\n",
    "Victoria_Memorial,_Kolkata\t'mrwiki'\t'व्हिक्टोरिया_मेमोरियल'\n",
    "Sunil_Kumar_Desai\t'knwiki'\t'ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ'\n",
    "James_Ellsworth\t'tawiki'\t'சேம்சு_எல்சுவர்த்'\n",
    "International_Mother_Language_Day\t'tewiki'\t'అంతర్జాతీయ_మాతృభాషా_దినోత్సవం'\n",
    "Li-Fi\t'bnwiki'\t'লাই-ফাই'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "  \n",
    "# initialize list of lists \n",
    "data = [['hiwiki', 'धमनी_विस्फार'], \n",
    "['guwiki', 'અળશી'],\n",
    "['tawiki', 'பல்லுட்புறச்_சிகிச்சை'],\n",
    "['mrwiki', 'व्हिक्टोरिया_मेमोरियल'],\n",
    "['knwiki', 'ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ'],\n",
    "['tawiki', 'சேம்சு_எல்சுவர்த்'],\n",
    "['tewiki', 'అంతర్జాతీయ_మాతృభాషా_దినోత్సవం'],\n",
    "['bnwiki', 'লাই-ফাই']]\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(data, columns = ['db_code', 'page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_code</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hiwiki</td>\n",
       "      <td>धमनी_विस्फार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guwiki</td>\n",
       "      <td>અળશી</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tawiki</td>\n",
       "      <td>பல்லுட்புறச்_சிகிச்சை</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mrwiki</td>\n",
       "      <td>व्हिक्टोरिया_मेमोरियल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knwiki</td>\n",
       "      <td>ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tawiki</td>\n",
       "      <td>சேம்சு_எல்சுவர்த்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tewiki</td>\n",
       "      <td>అంతర్జాతీయ_మాతృభాషా_దినోత్సవం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bnwiki</td>\n",
       "      <td>লাই-ফাই</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  db_code                     page_title\n",
       "0  hiwiki                   धमनी_विस्फार\n",
       "1  guwiki                           અળશી\n",
       "2  tawiki          பல்லுட்புறச்_சிகிச்சை\n",
       "3  mrwiki          व्हिक्टोरिया_मेमोरियल\n",
       "4  knwiki           ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ\n",
       "5  tawiki              சேம்சு_எல்சுவர்த்\n",
       "6  tewiki  అంతర్జాతీయ_మాతృభాషా_దినోత్సవం\n",
       "7  bnwiki                        লাই-ফাই"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_code</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hiwiki</td>\n",
       "      <td>धमनी_विस्फार</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  db_code    page_title\n",
       "0  hiwiki  धमनी_विस्फार"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('db_code', sort=False)\n",
    "grouped.get_group('hiwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = df['wiki_code']\n",
    "    \n",
    "def gather_data(df, db, page_title):\n",
    "        \n",
    "    '''\n",
    "    Gather article IDs and edit date information. \n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param page_titles: list of page_titles to gather data for\n",
    "    :type page_titles: str\n",
    "    :param start_date: First date to gather data for\n",
    "    :type start_date: datetime.date\n",
    "    :param end_date: Last date to gather data for\n",
    "    :type end_date: datetime.date\n",
    "    '''\n",
    "\n",
    "    get_ids_query = \"\"\"\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN {titles_normalized}\n",
    "    \"\"\"\n",
    "\n",
    "    for row in df:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = df['page_title']), db=df['wiki_code'])\n",
    "        return ids_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_data(df=df, db=db, page_title=df['page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wmfdata as wmf \n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL query to get data on partial blocks, adapted from\n",
    "## https://github.com/dayllanmaza/wikireplicas-reports/blob/master/generators/partial_blocks.py\n",
    "\n",
    "def get_data(dataframe):\n",
    "    get_ids_query = '''\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN '{titles_normalized}'\n",
    "    '''\n",
    "    \n",
    "    pbs = []\n",
    "    for row in dataframe:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(\n",
    "            titles_normalized = df['page_title']), \n",
    "            db=df['wiki_code'])\n",
    "        pbs.append(ids_r)\n",
    "    return(pd.concat(pbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wmf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5c5d951fe4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-a4ea7adf3193>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmariadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ids_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'page_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wiki_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wmf' is not defined"
     ]
    }
   ],
   "source": [
    "get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        df = pd.read_sql_query(\n",
    "            pb_query.format(\n",
    "                titles_normalized = df[article_title],\n",
    "                db = df[wiki_code]),\n",
    "            df[wiki_code])\n",
    "\n",
    "        pbs.append(df)\n",
    "    \n",
    "    return(pd.concat(pbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    if not end_date:\n",
    "#        end_date = dt.date.today()\n",
    "# with db as data['wiki_db']: \n",
    "\n",
    "db = df['wiki_code']\n",
    "    \n",
    "def gather_data(df, db, page_title):\n",
    "        \n",
    "    '''\n",
    "    Gather article IDs and edit date information. \n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param page_titles: list of page_titles to gather data for\n",
    "    :type page_titles: str\n",
    "    :param start_date: First date to gather data for\n",
    "    :type start_date: datetime.date\n",
    "    :param end_date: Last date to gather data for\n",
    "    :type end_date: datetime.date\n",
    "    '''\n",
    "\n",
    "    get_ids_query = \n",
    "    \"\"\"\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN {titles_normalized}\n",
    "    \"\"\"\n",
    "\n",
    "    edits_query = \"\"\"\n",
    "    SELECT \n",
    "        page_id,\n",
    "        DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "        revactor_actor\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "    JOIN page ON rev_page = page.page_id\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 20191010000000 \n",
    "        AND (rev_deleted & 4) = 0\n",
    "        AND rev_page IN {titles_normalized}\n",
    "    GROUP BY revactor_rev\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    datapoints = []\n",
    "    \n",
    "    for row in df:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = df['page_title']))\n",
    "        return ids_r\n",
    "            \n",
    "            \n",
    "            \n",
    "            for row in x:\n",
    "                ## Default is that they were autoconfirmed when they made\n",
    "                ## their tenth edit:\n",
    "                ac_timestamp = row['wiki_db']\n",
    "                ac_timestamp = row['article_id']\n",
    "\n",
    "            datapoints.append(DataPoint(article_id, is_ac, ac_timestamp))\n",
    "\n",
    "    return(datapoints)\n",
    "\n",
    "    ---->    \n",
    "    \n",
    "    ids_r = wmf.mariadb.run(get_ids_query.format(**wd_vars), 'wiki_db')\n",
    "\n",
    "    # act on the results \n",
    "    #create a df \n",
    "    all_surviving_articles = ids_r[['page_id','page_title', 'page_len', 'database_code']] \n",
    "    #seperate the redirected items into their own df\n",
    "    redirects = ids_r.loc[ids_r['p1_is_redirect']==1]\n",
    "    #pull only p1.page_id, p1.page_title, p1.page_len \n",
    "    redirect_df = redirects[['page_id','page_title','page_len', 'database_code']] \n",
    "\n",
    "    #remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "    ids =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "    #create a new wikicode column using quality_vars['wiki_db']\n",
    "    #ffill could also work here\n",
    "    #articles['wikicode'] = quality_vars['wiki_db']\n",
    "    \n",
    "    \n",
    "    ---->\n",
    "    \n",
    "    edits_r = wmf.mariadb.run(edits_query.format(**wd_vars), 'wiki_db')\n",
    "    edits_r.reset_index(drop=True);\n",
    "    edits_r['edit_date'] = pd.to_datetime(edits_r['edit_date'], format=\"%y-%m-%d\")\n",
    "    print(edits_r.info())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def connect(server, database, config_file):\n",
    "    '''\n",
    "    Connect to a database server.\n",
    "    :param server: the hostname of the server\n",
    "    :type server: str\n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param config_file: path to the MySQL configuration file to use\n",
    "                       (os.path.expanduser() is called on this path)\n",
    "    :type config_file: str\n",
    "    '''\n",
    "    db_conn = None\n",
    "    try:\n",
    "        db_conn = MySQLdb.connect(db=database,\n",
    "                                  host=server,\n",
    "                                  read_default_file=os.path.expanduser(\n",
    "                                      config_file))\n",
    "    except MySQLdb.Error as e:\n",
    "        logging.error('unable to connect to database')\n",
    "        logging.error('{} : {}'.format(e[0], e[1]))\n",
    "\n",
    "    return(db_conn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "import bz2\n",
    "import signal\n",
    "import logging\n",
    "import datetime as dt\n",
    "\n",
    "import db\n",
    "\n",
    "class SurvivalAnalyzer:\n",
    "    def __init__(self):\n",
    "        ## Database setup\n",
    "        self.db_host = 'analytics-store.eqiad.wmnet'\n",
    "        self.db_name = 'staging'\n",
    "        self.db_conf = '~/.my.research.cnf'\n",
    "        self.db_conn = None # database connection when up\n",
    "\n",
    "        self.log_host = 'analytics-slave.eqiad.wmnet'\n",
    "        self.log_name = 'log'\n",
    "        self.log_conn = None # database connection to the log database when up\n",
    "\n",
    "    def db_connect(self):\n",
    "        '''\n",
    "        Connect to the databases.\n",
    "        '''\n",
    "\n",
    "        self.db_conn = db.connect(self.db_host, self.db_name, self.db_conf)\n",
    "        self.log_conn = db.connect(self.log_host, self.log_name, self.db_conf)\n",
    "        \n",
    "    def insert_creations(self, dataset_filename):\n",
    "        '''\n",
    "        Go through the given dataset of article creations and insert any\n",
    "        done by an autoconfirmed user into a table. Connect to the log database\n",
    "        and make a similar query for creations from it and insert those.\n",
    "        '''\n",
    "\n",
    "        ## Query to find non-autopatrolled creations by autoconfirmed users\n",
    "        creation_query = '''SELECT page_id, rev_timestamp,\n",
    "                                   performer_user_id, performer_user_edit_count,\n",
    "                                   IFNULL(\n",
    "                                       TIMESTAMPDIFF(SECOND,\n",
    "                                                 performer_user_registration_dt,\n",
    "                                                 rev_timestamp), 0)\n",
    "                                   AS performer_account_age\n",
    "                            FROM mediawiki_page_create_2\n",
    "                            WHERE `database` = \"enwiki\"\n",
    "                            AND page_namespace = 0\n",
    "                            AND page_is_redirect = 0\n",
    "                            AND NOT (performer_user_groups\n",
    "                                     REGEXP \"sysop|bot|autoreviewer\")\n",
    "                            AND performer_user_groups REGEXP \"confirmed\"\n",
    "                            AND rev_timestamp >= \"2017-07-21 00:00:00\"\n",
    "                            AND rev_timestamp < \"2018-01-01 00:00:00\"'''\n",
    "\n",
    "        insert_query = '''INSERT INTO nettrom_autoconfirmed_creations\n",
    "                          VALUES (%s, %s, %s, %s, %s, NULL)'''\n",
    "\n",
    "\n",
    "    def process_creations(self, start_date, end_date):\n",
    "        '''\n",
    "        Get deletions for pages created between `start_date` and `end_date`\n",
    "        '''\n",
    "\n",
    "        create_temp_query = '''CREATE TEMPORARY TABLE\n",
    "                               nettrom_autoconfirmed_deletions (\n",
    "                               event_page_id INT UNSIGNED NOT NULL,\n",
    "                               event_deletion_time DATETIME)'''\n",
    "\n",
    "        ## Page ID and deletion time. This query assumes a page only gets\n",
    "        ## deleted once. We need to check if that holds.\n",
    "        insert_temp_query = '''\n",
    "            INSERT INTO nettrom_autoconfirmed_deletions\n",
    "            SELECT log_page,\n",
    "                   STR_TO_DATE(log_timestamp, \"%Y%m%d%H%i%S\") AS log_time\n",
    "            FROM nettrom_autoconfirmed_creations ac\n",
    "            STRAIGHT_JOIN enwiki.logging l\n",
    "            ON event_page_id=log_page\n",
    "            WHERE log_type='delete'\n",
    "            AND log_action='delete'\n",
    "            AND log_timestamp > DATE_FORMAT(event_timestamp, \"%Y%m%d%H%i%S\")\n",
    "            AND DATE(event_timestamp) >= \"{start_date}\"\n",
    "            AND DATE(event_timestamp) < \"{last_date}\"'''\n",
    "\n",
    "\n",
    "        with db.cursor(self.db_conn, 'dict') as db_cursor:\n",
    "            db_cursor.execute(create_temp_query)\n",
    "            logging.info('created temporary table')\n",
    "            self.db_conn.commit()\n",
    "\n",
    "        cur_date = start_date\n",
    "        time_step = dt.timedelta(days=1)\n",
    "        ## Note: <= because we also need to process _on_ the last date\n",
    "        while cur_date <= end_date:\n",
    "            with db.cursor(self.db_conn) as db_cursor:\n",
    "                ## insert deletions\n",
    "                db_cursor.execute(insert_temp_query.format(\n",
    "                    start_date=cur_date, last_date=cur_date + time_step))\n",
    "            \n",
    "                ## update database\n",
    "                db_cursor.execute(update_query)\n",
    "                \n",
    "                ## commit\n",
    "                self.db_conn.commit()\n",
    "\n",
    "                db_cursor.execute(delete_temp_query)\n",
    "\n",
    "            logging.info('updated deletions from {} to {}'.format(\n",
    "                cur_date, cur_date + time_step))\n",
    "\n",
    "            ## ok, iterate\n",
    "            cur_date += time_step\n",
    "        \n",
    "\n",
    "        # ok, done\n",
    "        return()\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "\n",
    "    def valid_date(d):\n",
    "        try:\n",
    "            return(dt.datetime.strptime(d, \"%Y-%m-%d\").date())\n",
    "        except ValueError:\n",
    "            raise argparse.ArgumentTypeError(\"Please write dates in the preferred format (YYYY-MM-DD)\")\n",
    "    \n",
    "    cli_parser = argparse.ArgumentParser(\n",
    "        description=\"script to process datasets of article creations by autoconfirmed users to determine if the article was deleted\"\n",
    "    )\n",
    "\n",
    "    # Verbosity option\n",
    "    cli_parser.add_argument('-v', '--verbose', action='store_true',\n",
    "                            help='write informational output')\n",
    "\n",
    "    #cli_parser.add_argument('dataset_file', type=str,\n",
    "    #                        help='path to the dataset with historic creations')\n",
    "\n",
    "    cli_parser.add_argument('start_date', type=valid_date,\n",
    "                            help='start date for gathering data (format: YYYY-MM-DD)')\n",
    "\n",
    "    cli_parser.add_argument('end_date', type=valid_date,\n",
    "                            help='end date for gathering data (format: YYYY-MM-DD)')\n",
    "\n",
    "    \n",
    "    args = cli_parser.parse_args()\n",
    "\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    analyzer = SurvivalAnalyzer()\n",
    "    analyzer.db_connect()\n",
    "    ## analyzer.insert_creations(args.dataset_file)\n",
    "\n",
    "    ## no need to keep this connection alive\n",
    "    db.disconnect(analyzer.log_conn)\n",
    "    \n",
    "    analyzer.process_creations(args.start_date, args.end_date)\n",
    "\n",
    "    # ok, done\n",
    "    return()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Another example:\n",
    "def get_mw_regs(wikis, start_timestamp, end_timestamp):\n",
    "\n",
    "    ## Query to get self-registrations through MediaWiki.\n",
    "    ## Also grabbing usernames, bot-info in username, bot user group membership.\n",
    "\n",
    "    ## From Analytics Engineering: https://gerrit.wikimedia.org/r/#/c/analytics/refinery/source/+/504025/\n",
    "    botUsernamePattern = r\"^.*bot([^a-z].*$|$)\"\n",
    "    \n",
    "    ## Using \"CONVERT\" to make the regexp case-insensitive\n",
    "    mw_reg_query = '''\n",
    "    SELECT \"{wiki}\" AS wiki,\n",
    "           user_id, user_name,\n",
    "           IF(CONVERT (user_name USING utf8) REGEXP \"{bot_regex}\", 1, 0) AS bot_by_name,\n",
    "           IF(ug_user IS NOT NULL, 1, 0) AS bot_by_group\n",
    "    FROM user\n",
    "    JOIN actor\n",
    "    ON user_id = actor_user\n",
    "    JOIN logging\n",
    "    ON log_actor = actor_id\n",
    "    LEFT JOIN (\n",
    "        SELECT ug_user\n",
    "        FROM user_groups\n",
    "        WHERE ug_group = \"bot\"\n",
    "    ) AS ug\n",
    "    ON user_id = ug_user\n",
    "    WHERE user_registration >= \"{start_ts}\"\n",
    "    AND user_registration < \"{end_ts}\"\n",
    "    AND log_type = \"newusers\"\n",
    "    AND log_action = \"create\" -- only self-creations\n",
    "    '''\n",
    "    \n",
    "    regs = list()\n",
    "    for wiki in wikis:\n",
    "        regs.append(\n",
    "            mariadb.run(\n",
    "                mw_reg_query.format(\n",
    "                    wiki = wiki,\n",
    "                    bot_regex = botUsernamePattern,\n",
    "                    start_ts = start_timestamp.strftime(utils.mw_format),\n",
    "                    end_ts = end_timestamp.strftime(utils.mw_format)\n",
    "                ), wiki\n",
    "            )\n",
    "        )\n",
    "                   \n",
    "        \n",
    "    return(pd.concat(regs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_variant_status(df, timestamps):\n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a left-joined\n",
    "    DataFrame with information on experiment/variant status for all users.\n",
    "    '''\n",
    "\n",
    "    ## 0: add an 'experiment_group' column, set it to 'homepage' (the default)\n",
    "    df['experiment_group'] = 'homepage'\n",
    "    \n",
    "    ## 1: convert 'reg_ts' to a timestamp\n",
    "    df['user_reg_ts'] = pd.to_datetime(df['reg_ts'], format = '%Y-%m-%dT%H:%M:%SZ', errors = 'coerce')\n",
    "    \n",
    "    ## 2: set everyone registered after the Newcomer Task timestamp to that group\n",
    "    df.loc[df['user_reg_ts'] >= timestamps['newcomer_tasks'], 'experiment_group'] = 'nt_exp_1'\n",
    "    \n",
    "    ## 3: set everyone registered after the variant timestamp to that group\n",
    "    df.loc[df['user_reg_ts'] >= timestamps['variant_1'], 'experiment_group'] = 'nt_exp_2_var_nopre'\n",
    "    \n",
    "    ## 4: query to get info on all users who had pre-initialized Newcomer Tasks,\n",
    "    ##    set the user group accordingly\n",
    "    pre_init_query = '''\n",
    "    SELECT up_user AS user_id\n",
    "    FROM user_properties\n",
    "    WHERE up_property = \"growthexperiments-homepage-suggestededits-preactivated\"\n",
    "    '''\n",
    "    \n",
    "    for wiki in df[‘wiki’].unique():\n",
    "        preactivated_users = mariadb.run(pre_init_query, wiki)\n",
    "        \n",
    "        df.loc[(df['wiki'] == wiki) & (df['user_id'].isin(preactivated_users['user_id'])),\n",
    "               'experiment_group'] = 'nt_exp_2_var_pre'\n",
    "        \n",
    "    ## drop the user_reg_ts column\n",
    "    df.drop(columns = 'user_reg_ts', inplace = True)\n",
    "    \n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import time\n",
    "import datetime as dt \n",
    "from datetime import datetime, timedelta, date\n",
    "import dateutil\n",
    "\n",
    "\n",
    "#replace contest start/end date, and country for each GLOW project\n",
    "contest_start = '2019-10-10'\n",
    "contest_end = '2020-02-11'\n",
    "quality_vars = dict(\n",
    "    country_code  = \"IN\",\n",
    "    contest_start = contest_start,\n",
    "    contest_end   = contest_end,\n",
    "    contest_start_dt        = datetime.strptime(contest_start, '%Y-%m-%d'),\n",
    "    contest_end_dt          = datetime.strptime(contest_end, '%Y-%m-%d'),\n",
    "    contest_end_dt_month    = datetime.strptime(contest_end, '%Y-%m-%d').strftime('%m'),\n",
    "    contest_end_dt_day      = datetime.strptime(contest_end, '%Y-%m-%d').strftime('%d'),\n",
    "    contest_end_dt_1M_month = (datetime.strptime(contest_end, '%Y-%m-%d') + timedelta(days=30)).strftime('%m'),\n",
    "    contest_end_dt_1M_day   = (datetime.strptime(contest_end, '%Y-%m-%d') + timedelta(days=30)).strftime('%d'),\n",
    "    satwiki_ids =  satwiki_ids,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tcy.wiki'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_pvs_r = hive.run(\"\"\"\n",
    "SELECT \n",
    "   page_id,\n",
    "   SUM(view_count) AS views_1M\n",
    "FROM wmf.pageview_hourly \n",
    "WHERE \n",
    "  year = 2020\n",
    "  AND (month >= {contest_end_dt_month} AND day >= {contest_end_dt_day}) \n",
    "  AND (month <= {contest_end_dt_1M_month} AND day <= {contest_end_dt_1M_day}) \n",
    "  AND agent_type = 'user'\n",
    "  AND country_code = '{country_code}'\n",
    "  AND project = 'sa.wikipedia'\n",
    "  AND page_id IN {sawiki_ids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_pvs_all_r = hive.run(\"\"\"\n",
    "SELECT \n",
    "   page_id,\n",
    "   SUM(view_count) AS views_1M\n",
    "FROM wmf.pageview_hourly \n",
    "WHERE \n",
    "  year = 2020\n",
    "  AND agent_type = 'user'\n",
    "  AND project = 'sa.wikipedia'\n",
    "  AND page_id IN {sawiki_ids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb = spark.run(\"\"\"\n",
    "SELECT \n",
    "   SUM(view_count) AS views_feb\n",
    "FROM wmf.pageview_hourly \n",
    "WHERE \n",
    "  year = 2020\n",
    "  AND month = 2\n",
    "  AND agent_type = 'user'\n",
    "  AND country = 'India'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views_feb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175906</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175907</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175908</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175909</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175910</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7175911 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         views_feb\n",
       "0               30\n",
       "1                1\n",
       "2                2\n",
       "3                1\n",
       "4             9907\n",
       "...            ...\n",
       "7175906          1\n",
       "7175907          1\n",
       "7175908          1\n",
       "7175909          1\n",
       "7175910          1\n",
       "\n",
       "[7175911 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = feb['views_feb'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684422760"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nov = spark.run(\"\"\"\n",
    "SELECT \n",
    "   SUM(view_count) AS views_nov\n",
    "FROM wmf.pageview_hourly \n",
    "WHERE \n",
    "  year = 2019\n",
    "  AND month = 11\n",
    "  AND agent_type = 'user'\n",
    "  AND country = 'India'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>views_feb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088552</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088553</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088554</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088555</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7088556</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7088557 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         views_feb\n",
       "0             1785\n",
       "1             2801\n",
       "2             2178\n",
       "3               15\n",
       "4                1\n",
       "...            ...\n",
       "7088552          1\n",
       "7088553          1\n",
       "7088554          1\n",
       "7088555          1\n",
       "7088556          1\n",
       "\n",
       "[7088557 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764664872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN = nov['views_feb'].sum()\n",
    "TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "764,664,872\n",
    "684,422,760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
