{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#TODO Incentive type > categorical (can do ordering with pandas)..but Hive may not allow for categorical. parquet maybe\n",
    "\n",
    "1. Get list of articles from WAM and WLW and filter those out from the GLOW data. \n",
    "2. Get list of last years editors\n",
    "3. get list of jury? >> seperate table - jury "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = {\n",
    "    'PA': {'title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'namespace': 0},\n",
    "    'ML': {'title' :'പ്രധാന_താൾ', 'namespace': 0},\n",
    "    'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "    'HI': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "    'PNB': {'title' : 'پہلا_صفہ', 'namespace': 0},\n",
    "    'TA': {'title' : 'முதற்_பக்கம்', 'namespace': 0},\n",
    "    'TE': {'title' : 'మొదటి_పేజీ', 'namespace': 0}, \n",
    "    'AS': {'title' : 'বেটুপাত', 'namespace': 0},\n",
    "    'SA': {'title' : 'मुख्यपृष्ठम्', 'namespace': 0},\n",
    "    'KN': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "    'TCY': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "    'GU': {'title' : 'મુખપૃષ્ઠ', 'namespace': 0},\n",
    "    'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "    'MR': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "    'SAT': {'title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'namespace': 0},\n",
    "    'UR': {'title' : 'صفحۂ_اول', 'namespace': 0}, \n",
    "    'OR': {'title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'namespace': 0}   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = {\n",
    "        {'Punjabi': 'PA', mainpage = {'mainpage_title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'mainpage_namespace': 0}},\n",
    "        {'PNB' \n",
    "        mainpage = {'mainpage_title' : 'پہلا_صفہ', 'mainpage_namespace': 0}\n",
    "        \n",
    "        {'Malayalam': 'ML', mainpage = {'mainpage_title' :'പ്രധാന_താൾ', 'mainpage_namespace': 0}},\n",
    "        {'Bengali':  'BN', mainpage = {'mainpage_title' : 'প্রধান_পাতা', 'mainpage_namespace': 0}},\n",
    "        {'Hindi': 'HI', mainpage = {'mainpage_title' : 'मुखपृष्ठ', 'mainpage_namespace': 0}},\n",
    "        {'Tamil': 'TA', mainpage = {'mainpage_title' : 'முதற்_பக்கம்', 'mainpage_namespace': 0}},\n",
    "        {'Telugu': 'TE', mainpage = {'mainpage_title' : 'మొదటి_పేజీ', 'mainpage_namespace': 0}}, \n",
    "        {'Assamese': 'AS', mainpage = {'mainpage_title' : 'বেটুপাত', 'mainpage_namespace': 0}},\n",
    "        {'Sanskrit': 'SA', mainpage = {'mainpage_title' : 'मुख्यपृष्ठम्', 'mainpage_namespace': 0}},\n",
    "        {'Kannada': 'KN', mainpage = {'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', 'mainpage_namespace': 0}},\n",
    "        {'Gujarati': 'GU', mainpage = {'mainpage_title' : 'મુખપૃષ્ઠ', 'mainpage_namespace': 0}},\n",
    "        {'Bengali': 'BN', mainpage = {'mainpage_title' : 'প্রধান_পাতা', 'mainpage_namespace': 0}},\n",
    "        {'Tulu': 'TCY', mainpage = {'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', 'mainpage_namespace': 0}},\n",
    "        {'Marathi': 'MR', mainpage = {'mainpage_title' : 'मुखपृष्ठ', 'mainpage_namespace': 0}},\n",
    "        {'Santali': 'SAT', mainpage = {'mainpage_title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'mainpage_namespace': 0}},\n",
    "        {'Urdu': 'UR', mainpage = {'mainpage_title' : 'صفحۂ_اول', 'mainpage_namespace': 0}}, \n",
    "        {'Odia': 'OR', mainpage = {'mainpage_title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'mainpage_namespace': 0}}   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi2 = {\n",
    "    'PA': {\n",
    "            'wikiname': 'Punjabi'\n",
    "            'mainpage_title' : 'ਮੁੱਖ_ਸਫ਼ਾ', \n",
    "            'mainpage_namespace': 0},\n",
    "    \n",
    "    'ML': {'wikiname': 'Malayalam'\n",
    "        'mainpage_title' :'പ്രധാന_താൾ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'BN': {'wikiname': 'Bengali'\n",
    "           'mainpage_title' : 'প্রধান_পাতা', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'HI': {'wikiname': 'Hindi'\n",
    "           'mainpage_title' : 'मुखपृष्ठ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'PNB': {'wikiname': \n",
    "           'mainpage_title' : 'پہلا_صفہ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'TA': {'wikiname': 'Tamil'\n",
    "           'mainpage_title' : 'முதற்_பக்கம்', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'TE': {'wikiname': 'Telugu'\n",
    "        'mainpage_title' : 'మొదటి_పేజీ', \n",
    "           'mainpage_namespace': 0}, \n",
    "           \n",
    "    'AS': {'wikiname': 'Assamese'\n",
    "           'mainpage_title' : 'বেটুপাত', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'SA': {'wikiname': 'Sanskrit'\n",
    "           'mainpage_title' : 'मुख्यपृष्ठम्', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'KN': {'wikiname': 'Kannada'\n",
    "           'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', \n",
    "           'mainpage_namespace': 0},\n",
    "            \n",
    "    'GU': {'wikiname':  'Gujarati'\n",
    "           'mainpage_title' : 'મુખપૃષ્ઠ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'BN': {'wikiname': 'Bengali'\n",
    "           'mainpage_title' : 'প্রধান_পাতা', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'TCY': {'wikiname': 'Tulu'\n",
    "           'mainpage_title' : 'ಮುಖ್ಯ_ಪುಟ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'MR': {'wikiname': 'Marathi'\n",
    "           'mainpage_title' : 'मुखपृष्ठ', \n",
    "           'mainpage_namespace': 0},\n",
    "           \n",
    "    'SAT': {'wikiname': 'Santali'\n",
    "           'mainpage_title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', \n",
    "            'mainpage_namespace': 0},\n",
    "            \n",
    "    'UR': {'wikiname': 'Urdu'\n",
    "           'mainpage_title' : 'صفحۂ_اول', \n",
    "           'mainpage_namespace': 0}, \n",
    "           \n",
    "    'OR': {'wikiname':'Odia'\n",
    "           'mainpage_title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', \n",
    "           'mainpage_namespace': 0}   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for dupes\n",
    "z = len(idc)-idc.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_name     288\n",
       "contest_code    8788\n",
       "dateAdded         13\n",
       "finish          8810\n",
       "start           8812\n",
       "user_name       8432\n",
       "wiki            8802\n",
       "dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article_name',\n",
       " 'contest_code',\n",
       " 'dateAdded',\n",
       " 'finish',\n",
       " 'jury',\n",
       " 'start',\n",
       " 'user_name',\n",
       " 'wiki']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idc_r.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "try:\n",
    "                get_date = driver.find_element_by_xpath(date_xpath).text\n",
    "                get_wiki_name = driver.find_elements_by_xpath (wiki_xpath)\n",
    "                get_article_count = driver.find_elements_by_xpath (ac_xpath)\n",
    "            \n",
    "            \n",
    "                ac_dict['date'] = date = get_date\n",
    "                ac_dict['lang'] = \"\".join([element.text for element in get_wiki_name]) \n",
    "                ac_dict['count'] = \"\".join([element.text for element in get_article_count]) \n",
    "            \n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "           \n",
    "            #print(ac_dict)\n",
    "            t_ac.append(ac_dict.copy())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prettify data\n",
    "lens = [len(item) for item in core['articles']]\n",
    "df = pd.DataFrame({\n",
    "    'wiki':np.repeat(core['wiki'].values, lens),\n",
    "    'code':np.repeat(core['code'].values, lens),\n",
    "    'finish':np.repeat(core['finish'].values, lens),\n",
    "    'start':np.repeat(core['start'].values, lens),\n",
    "    'jury':np.repeat(core['jury'].values, lens),\n",
    "    'articles':np.hstack(core['articles']),\n",
    "                      })\n",
    "df = pd.concat([df_out.drop(['articles'], axis=1), df_out['articles'].apply(pd.Series)],axis=1)\n",
    "df = df[['wiki', 'code', 'start', 'finish', 'jury', 'dateAdded', 'name', 'user']]\n",
    "df.columns = ['wiki', 'code', 'start', 'finish', 'jury', 'dateAdded', 'article_name', 'editor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "'PNB': {'title' : 'پہلا_صفہ', 'namespace': 0},\n",
    "'AS': {'title' : 'বেটুপাত', 'namespace': 0},\n",
    "'SA': {'title' : 'मुख्यपृष्ठम्', 'namespace': 0},\n",
    "'TCY': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "\n",
    "'GU': {'title' : 'મુખપૃષ્ઠ', 'namespace': 0},\n",
    "'BN': {'title' : 'প্রধান_পাতা', 'namespace': 0},\n",
    "\n",
    "\n",
    "'ML': {'title' :'പ്രധാന_താൾ', 'namespace': 0},\n",
    "'PA': {'title' : 'ਮੁੱਖ_ਸਫ਼ਾ', 'namespace': 0},\n",
    "'SAT': {'title' : 'ᱢᱩᱬᱩᱛ_ᱥᱟᱦᱴᱟ', 'namespace': 0},\n",
    "'UR': {'title' : 'صفحۂ_اول', 'namespace': 0}, \n",
    "'TA': {'title' : 'முதற்_பக்கம்', 'namespace': 0},\n",
    "'TE': {'title' : 'మొదటి_పేజీ', 'namespace': 0}, \n",
    "'OR': {'title' : 'ପ୍ରଧାନ_ପୃଷ୍ଠା', 'namespace': 0} \n",
    "'MR': {'title' : 'मुखपृष्ठ', 'namespace': 0},\n",
    "'KN': {'title' : 'ಮುಖ್ಯ_ಪುಟ', 'namespace': 0},\n",
    "'HI': {'title' : 'मुखपृष्ठ', 'namespace': 0},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "contests = {'wlw_2019':{'url_base': 'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-{}','urls': []},\n",
    "            'ptp_2018':{'url_base': 'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-{}','urls':[]},\n",
    "            'wam_2019':{'url_base': 'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-{}', 'urls':[] }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/9807634/find-all-occurrences-of-a-key-in-nested-dictionaries-and-lists\n",
    "def gen_dict_extract(key, var):\n",
    "    if hasattr(var,'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#attempt to get it to work for a dict of urls and add values back into dictionary\n",
    "\n",
    "#get working urls\n",
    "wikicodes = ['bn', 'ur', 'te', 'ta', 'sat', 'pa', 'mr', 'ml', 'kn', 'hi', 'gu', 'pnb', 'tcy', 'as', 'or','ml_or']\n",
    "\n",
    "url_bases = ['https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-',\n",
    "            'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-', #wlw_campaigns_2019\n",
    "            'https://tools.wmflabs.org/fountain/editathons/asian-month-2019-']\n",
    "\n",
    "url_bases_gen = list(gen_dict_extract('url_base', contests))\n",
    "\n",
    "for url in url_bases_gen:\n",
    "    urls_to_review = list()\n",
    "    for wikicode in wikicodes:\n",
    "        urls = (url_base.format(wikicode))\n",
    "        urls_to_review.append(urls)\n",
    "\n",
    "        #'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-'\n",
    "        #'https://tools.wmflabs.org/fountain/editathons/wlwsa2020-' #wlw_campaigns_2019\n",
    "\n",
    "    not_found_urls = list()\n",
    "    # Iterate here on the urls\n",
    "    # The below code could be executed for each url\n",
    "    for url in urls_to_review:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 404:\n",
    "            not_found_urls.append(url)\n",
    "\n",
    "    working_urls = list(set(urls_to_review)-set(not_found_urls))\n",
    "    \n",
    "    ### create new list of urls for each url in url_bases_gen"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Aneurysm\t'hiwiki'\t'धमनी_विस्फार' \n",
    "Flax\t'guwiki'\t'અળશી'\n",
    "Root_canal_treatment\t'tawiki'\t'பல்லுட்புறச்_சிகிச்சை'\n",
    "Victoria_Memorial,_Kolkata\t'mrwiki'\t'व्हिक्टोरिया_मेमोरियल'\n",
    "Sunil_Kumar_Desai\t'knwiki'\t'ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ'\n",
    "James_Ellsworth\t'tawiki'\t'சேம்சு_எல்சுவர்த்'\n",
    "International_Mother_Language_Day\t'tewiki'\t'అంతర్జాతీయ_మాతృభాషా_దినోత్సవం'\n",
    "Li-Fi\t'bnwiki'\t'লাই-ফাই'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "  \n",
    "# initialize list of lists \n",
    "data = [['hiwiki', 'धमनी_विस्फार'], \n",
    "['guwiki', 'અળશી'],\n",
    "['tawiki', 'பல்லுட்புறச்_சிகிச்சை'],\n",
    "['mrwiki', 'व्हिक्टोरिया_मेमोरियल'],\n",
    "['knwiki', 'ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ'],\n",
    "['tawiki', 'சேம்சு_எல்சுவர்த்'],\n",
    "['tewiki', 'అంతర్జాతీయ_మాతృభాషా_దినోత్సవం'],\n",
    "['bnwiki', 'লাই-ফাই']]\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "df = pd.DataFrame(data, columns = ['db_code', 'page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_code</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hiwiki</td>\n",
       "      <td>धमनी_विस्फार</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guwiki</td>\n",
       "      <td>અળશી</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tawiki</td>\n",
       "      <td>பல்லுட்புறச்_சிகிச்சை</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mrwiki</td>\n",
       "      <td>व्हिक्टोरिया_मेमोरियल</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>knwiki</td>\n",
       "      <td>ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tawiki</td>\n",
       "      <td>சேம்சு_எல்சுவர்த்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tewiki</td>\n",
       "      <td>అంతర్జాతీయ_మాతృభాషా_దినోత్సవం</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bnwiki</td>\n",
       "      <td>লাই-ফাই</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  db_code                     page_title\n",
       "0  hiwiki                   धमनी_विस्फार\n",
       "1  guwiki                           અળશી\n",
       "2  tawiki          பல்லுட்புறச்_சிகிச்சை\n",
       "3  mrwiki          व्हिक्टोरिया_मेमोरियल\n",
       "4  knwiki           ಸುನೀಲ್_ಕುಮಾರ್_ದೇಸಾಯಿ\n",
       "5  tawiki              சேம்சு_எல்சுவர்த்\n",
       "6  tewiki  అంతర్జాతీయ_మాతృభాషా_దినోత్సవం\n",
       "7  bnwiki                        লাই-ফাই"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_code</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hiwiki</td>\n",
       "      <td>धमनी_विस्फार</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  db_code    page_title\n",
       "0  hiwiki  धमनी_विस्फार"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby('db_code', sort=False)\n",
    "grouped.get_group('hiwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#db = df['wiki_code']\n",
    "    \n",
    "def gather_data(df, db, page_title):\n",
    "        \n",
    "    '''\n",
    "    Gather article IDs and edit date information. \n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param page_titles: list of page_titles to gather data for\n",
    "    :type page_titles: str\n",
    "    :param start_date: First date to gather data for\n",
    "    :type start_date: datetime.date\n",
    "    :param end_date: Last date to gather data for\n",
    "    :type end_date: datetime.date\n",
    "    '''\n",
    "\n",
    "    get_ids_query = \"\"\"\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN {titles_normalized}\n",
    "    \"\"\"\n",
    "\n",
    "    for row in df:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = df['page_title']), db=df['wiki_code'])\n",
    "        return ids_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_data(df=df, db=db, page_title=df['page_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -rf ~/.cache/pip/http/*\n",
    "pip install --upgrade git+https://github.com/neilpquinn/wmfdata.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b0297b585dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcharting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmariadb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpct_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd_display_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m You can find the source for `wmfdata` at {1}\"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mremote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_remote_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_newer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     update_message = \"\"\"You are using wmfdata {0}. A newer version is available.\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/utils.py\u001b[0m in \u001b[0;36mcheck_remote_version\u001b[0;34m(local_version)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/neilpquinn/wmfdata/release/wmfdata/__init__.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mremote_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(([0-9]+\\\\.?){2,3})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     d = {\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m'version'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremote_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wmfdata as wmf \n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b0297b585dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcharting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmariadb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpct_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd_display_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m You can find the source for `wmfdata` at {1}\"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mremote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_remote_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_newer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     update_message = \"\"\"You are using wmfdata {0}. A newer version is available.\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/utils.py\u001b[0m in \u001b[0;36mcheck_remote_version\u001b[0;34m(local_version)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/neilpquinn/wmfdata/release/wmfdata/__init__.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mremote_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(([0-9]+\\\\.?){2,3})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     d = {\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m'version'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremote_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wmfdata as wmf \n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3cc843729490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcharting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmariadb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwmfdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpct_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd_display_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m You can find the source for `wmfdata` at {1}\"\"\"\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mremote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_remote_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mremote\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_newer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     update_message = \"\"\"You are using wmfdata {0}. A newer version is available.\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/wmfdata/utils.py\u001b[0m in \u001b[0;36mcheck_remote_version\u001b[0;34m(local_version)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/neilpquinn/wmfdata/release/wmfdata/__init__.py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mremote_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(([0-9]+\\\\.?){2,3})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     d = {\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m'version'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mremote_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wmfdata as wmf \n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL query to get data on partial blocks, adapted from\n",
    "## https://github.com/dayllanmaza/wikireplicas-reports/blob/master/generators/partial_blocks.py\n",
    "\n",
    "def get_data(dataframe):\n",
    "    get_ids_query = '''\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN '{titles_normalized}'\n",
    "    '''\n",
    "    \n",
    "    pbs = []\n",
    "    for row in dataframe:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(\n",
    "            titles_normalized = df['page_title']), \n",
    "            db=df['wiki_code'])\n",
    "        pbs.append(ids_r)\n",
    "    return(pd.concat(pbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wmf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5c5d951fe4b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-a4ea7adf3193>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mids_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmariadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ids_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'page_title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wiki_code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mpbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wmf' is not defined"
     ]
    }
   ],
   "source": [
    "get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        df = pd.read_sql_query(\n",
    "            pb_query.format(\n",
    "                titles_normalized = df[article_title],\n",
    "                db = df[wiki_code]),\n",
    "            df[wiki_code])\n",
    "\n",
    "        pbs.append(df)\n",
    "    \n",
    "    return(pd.concat(pbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    if not end_date:\n",
    "#        end_date = dt.date.today()\n",
    "# with db as data['wiki_db']: \n",
    "\n",
    "db = df['wiki_code']\n",
    "    \n",
    "def gather_data(df, db, page_title):\n",
    "        \n",
    "    '''\n",
    "    Gather article IDs and edit date information. \n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param page_titles: list of page_titles to gather data for\n",
    "    :type page_titles: str\n",
    "    :param start_date: First date to gather data for\n",
    "    :type start_date: datetime.date\n",
    "    :param end_date: Last date to gather data for\n",
    "    :type end_date: datetime.date\n",
    "    '''\n",
    "\n",
    "    get_ids_query = \n",
    "    \"\"\"\n",
    "    SELECT \n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN {titles_normalized}\n",
    "    \"\"\"\n",
    "\n",
    "    edits_query = \"\"\"\n",
    "    SELECT \n",
    "        page_id,\n",
    "        DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "        revactor_actor\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "    JOIN page ON rev_page = page.page_id\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 20191010000000 \n",
    "        AND (rev_deleted & 4) = 0\n",
    "        AND rev_page IN {titles_normalized}\n",
    "    GROUP BY revactor_rev\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    datapoints = []\n",
    "    \n",
    "    for row in df:\n",
    "        ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = df['page_title']))\n",
    "        return ids_r\n",
    "            \n",
    "            \n",
    "            \n",
    "            for row in x:\n",
    "                ## Default is that they were autoconfirmed when they made\n",
    "                ## their tenth edit:\n",
    "                ac_timestamp = row['wiki_db']\n",
    "                ac_timestamp = row['article_id']\n",
    "\n",
    "            datapoints.append(DataPoint(article_id, is_ac, ac_timestamp))\n",
    "\n",
    "    return(datapoints)\n",
    "\n",
    "    ---->    \n",
    "    \n",
    "    ids_r = wmf.mariadb.run(get_ids_query.format(**wd_vars), 'wiki_db')\n",
    "\n",
    "    # act on the results \n",
    "    #create a df \n",
    "    all_surviving_articles = ids_r[['page_id','page_title', 'page_len', 'database_code']] \n",
    "    #seperate the redirected items into their own df\n",
    "    redirects = ids_r.loc[ids_r['p1_is_redirect']==1]\n",
    "    #pull only p1.page_id, p1.page_title, p1.page_len \n",
    "    redirect_df = redirects[['page_id','page_title','page_len', 'database_code']] \n",
    "\n",
    "    #remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "    ids =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "    #create a new wikicode column using quality_vars['wiki_db']\n",
    "    #ffill could also work here\n",
    "    #articles['wikicode'] = quality_vars['wiki_db']\n",
    "    \n",
    "    \n",
    "    ---->\n",
    "    \n",
    "    edits_r = wmf.mariadb.run(edits_query.format(**wd_vars), 'wiki_db')\n",
    "    edits_r.reset_index(drop=True);\n",
    "    edits_r['edit_date'] = pd.to_datetime(edits_r['edit_date'], format=\"%y-%m-%d\")\n",
    "    print(edits_r.info())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def connect(server, database, config_file):\n",
    "    '''\n",
    "    Connect to a database server.\n",
    "    :param server: the hostname of the server\n",
    "    :type server: str\n",
    "    :param database: the name of the database to use\n",
    "    :type database: str\n",
    "    :param config_file: path to the MySQL configuration file to use\n",
    "                       (os.path.expanduser() is called on this path)\n",
    "    :type config_file: str\n",
    "    '''\n",
    "    db_conn = None\n",
    "    try:\n",
    "        db_conn = MySQLdb.connect(db=database,\n",
    "                                  host=server,\n",
    "                                  read_default_file=os.path.expanduser(\n",
    "                                      config_file))\n",
    "    except MySQLdb.Error as e:\n",
    "        logging.error('unable to connect to database')\n",
    "        logging.error('{} : {}'.format(e[0], e[1]))\n",
    "\n",
    "    return(db_conn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "import bz2\n",
    "import signal\n",
    "import logging\n",
    "import datetime as dt\n",
    "\n",
    "import db\n",
    "\n",
    "class SurvivalAnalyzer:\n",
    "    def __init__(self):\n",
    "        ## Database setup\n",
    "        self.db_host = 'analytics-store.eqiad.wmnet'\n",
    "        self.db_name = 'staging'\n",
    "        self.db_conf = '~/.my.research.cnf'\n",
    "        self.db_conn = None # database connection when up\n",
    "\n",
    "        self.log_host = 'analytics-slave.eqiad.wmnet'\n",
    "        self.log_name = 'log'\n",
    "        self.log_conn = None # database connection to the log database when up\n",
    "\n",
    "    def db_connect(self):\n",
    "        '''\n",
    "        Connect to the databases.\n",
    "        '''\n",
    "\n",
    "        self.db_conn = db.connect(self.db_host, self.db_name, self.db_conf)\n",
    "        self.log_conn = db.connect(self.log_host, self.log_name, self.db_conf)\n",
    "        \n",
    "    def insert_creations(self, dataset_filename):\n",
    "        '''\n",
    "        Go through the given dataset of article creations and insert any\n",
    "        done by an autoconfirmed user into a table. Connect to the log database\n",
    "        and make a similar query for creations from it and insert those.\n",
    "        '''\n",
    "\n",
    "        ## Query to find non-autopatrolled creations by autoconfirmed users\n",
    "        creation_query = '''SELECT page_id, rev_timestamp,\n",
    "                                   performer_user_id, performer_user_edit_count,\n",
    "                                   IFNULL(\n",
    "                                       TIMESTAMPDIFF(SECOND,\n",
    "                                                 performer_user_registration_dt,\n",
    "                                                 rev_timestamp), 0)\n",
    "                                   AS performer_account_age\n",
    "                            FROM mediawiki_page_create_2\n",
    "                            WHERE `database` = \"enwiki\"\n",
    "                            AND page_namespace = 0\n",
    "                            AND page_is_redirect = 0\n",
    "                            AND NOT (performer_user_groups\n",
    "                                     REGEXP \"sysop|bot|autoreviewer\")\n",
    "                            AND performer_user_groups REGEXP \"confirmed\"\n",
    "                            AND rev_timestamp >= \"2017-07-21 00:00:00\"\n",
    "                            AND rev_timestamp < \"2018-01-01 00:00:00\"'''\n",
    "\n",
    "        insert_query = '''INSERT INTO nettrom_autoconfirmed_creations\n",
    "                          VALUES (%s, %s, %s, %s, %s, NULL)'''\n",
    "\n",
    "\n",
    "    def process_creations(self, start_date, end_date):\n",
    "        '''\n",
    "        Get deletions for pages created between `start_date` and `end_date`\n",
    "        '''\n",
    "\n",
    "        create_temp_query = '''CREATE TEMPORARY TABLE\n",
    "                               nettrom_autoconfirmed_deletions (\n",
    "                               event_page_id INT UNSIGNED NOT NULL,\n",
    "                               event_deletion_time DATETIME)'''\n",
    "\n",
    "        ## Page ID and deletion time. This query assumes a page only gets\n",
    "        ## deleted once. We need to check if that holds.\n",
    "        insert_temp_query = '''\n",
    "            INSERT INTO nettrom_autoconfirmed_deletions\n",
    "            SELECT log_page,\n",
    "                   STR_TO_DATE(log_timestamp, \"%Y%m%d%H%i%S\") AS log_time\n",
    "            FROM nettrom_autoconfirmed_creations ac\n",
    "            STRAIGHT_JOIN enwiki.logging l\n",
    "            ON event_page_id=log_page\n",
    "            WHERE log_type='delete'\n",
    "            AND log_action='delete'\n",
    "            AND log_timestamp > DATE_FORMAT(event_timestamp, \"%Y%m%d%H%i%S\")\n",
    "            AND DATE(event_timestamp) >= \"{start_date}\"\n",
    "            AND DATE(event_timestamp) < \"{last_date}\"'''\n",
    "\n",
    "\n",
    "        with db.cursor(self.db_conn, 'dict') as db_cursor:\n",
    "            db_cursor.execute(create_temp_query)\n",
    "            logging.info('created temporary table')\n",
    "            self.db_conn.commit()\n",
    "\n",
    "        cur_date = start_date\n",
    "        time_step = dt.timedelta(days=1)\n",
    "        ## Note: <= because we also need to process _on_ the last date\n",
    "        while cur_date <= end_date:\n",
    "            with db.cursor(self.db_conn) as db_cursor:\n",
    "                ## insert deletions\n",
    "                db_cursor.execute(insert_temp_query.format(\n",
    "                    start_date=cur_date, last_date=cur_date + time_step))\n",
    "            \n",
    "                ## update database\n",
    "                db_cursor.execute(update_query)\n",
    "                \n",
    "                ## commit\n",
    "                self.db_conn.commit()\n",
    "\n",
    "                db_cursor.execute(delete_temp_query)\n",
    "\n",
    "            logging.info('updated deletions from {} to {}'.format(\n",
    "                cur_date, cur_date + time_step))\n",
    "\n",
    "            ## ok, iterate\n",
    "            cur_date += time_step\n",
    "        \n",
    "\n",
    "        # ok, done\n",
    "        return()\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "\n",
    "    def valid_date(d):\n",
    "        try:\n",
    "            return(dt.datetime.strptime(d, \"%Y-%m-%d\").date())\n",
    "        except ValueError:\n",
    "            raise argparse.ArgumentTypeError(\"Please write dates in the preferred format (YYYY-MM-DD)\")\n",
    "    \n",
    "    cli_parser = argparse.ArgumentParser(\n",
    "        description=\"script to process datasets of article creations by autoconfirmed users to determine if the article was deleted\"\n",
    "    )\n",
    "\n",
    "    # Verbosity option\n",
    "    cli_parser.add_argument('-v', '--verbose', action='store_true',\n",
    "                            help='write informational output')\n",
    "\n",
    "    #cli_parser.add_argument('dataset_file', type=str,\n",
    "    #                        help='path to the dataset with historic creations')\n",
    "\n",
    "    cli_parser.add_argument('start_date', type=valid_date,\n",
    "                            help='start date for gathering data (format: YYYY-MM-DD)')\n",
    "\n",
    "    cli_parser.add_argument('end_date', type=valid_date,\n",
    "                            help='end date for gathering data (format: YYYY-MM-DD)')\n",
    "\n",
    "    \n",
    "    args = cli_parser.parse_args()\n",
    "\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    analyzer = SurvivalAnalyzer()\n",
    "    analyzer.db_connect()\n",
    "    ## analyzer.insert_creations(args.dataset_file)\n",
    "\n",
    "    ## no need to keep this connection alive\n",
    "    db.disconnect(analyzer.log_conn)\n",
    "    \n",
    "    analyzer.process_creations(args.start_date, args.end_date)\n",
    "\n",
    "    # ok, done\n",
    "    return()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
