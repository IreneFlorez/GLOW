{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOW  - Content Quality & Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "* [Content](#top)\n",
    "    1. Identify and filter out redirects\n",
    "    [num editors/article](#nea)\n",
    "    2. translation\n",
    "    3. [pagelen & pagelen relative (composition)](#pagelen)\n",
    "    4. wikidata item\n",
    "    5. article creation date\n",
    "    6. [pageviews(use/utility)](#pv)\n",
    "    7. filter for new articles\n",
    "    8. edits & editors per article\n",
    "    9. edits & timestamps\n",
    "    10. editors per article\n",
    "    11. [talk page activity](#tpa) \n",
    "    12. article watch count\n",
    "    13. [revert rate(use/utility)](#rr)\n",
    "    14. [links(importance/integration)](#ol)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    require(['notebook/js/codecell'], function(codecell) {\n",
       "      // https://github.com/jupyter/notebook/issues/2453\n",
       "      codecell.CodeCell.options_default.highlight_modes['magic_text/x-sql'] = {'reg':[/^%read_sql/, /.*=\\s*%read_sql/,\n",
       "                                                                                      /^%%read_sql/]};\n",
       "      Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "          console.log('BBBBB');\n",
       "          Jupyter.notebook.get_cells().map(function(cell){\n",
       "              if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "      });\n",
       "    });\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import wmfdata as wmf\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from urllib import request\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "import weakref\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "import jupyter_contrib_nbextensions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime as dt \n",
    "from datetime import datetime, timedelta, date\n",
    "import dateutil\n",
    "\n",
    "%load_ext sql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_vars' (dict)\n"
     ]
    }
   ],
   "source": [
    "%run 2b_data_handling.ipynb\n",
    "%store -r IN_median_vi\n",
    "#%run ./data_collection/collecting_articles.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Content Quality<a class=\"anchor\" id=\"stage1\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%store -r aswiki_titles_normalized\n",
    "%store -r bnwiki_titles_normalized\n",
    "%store -r guwiki_titles_normalized\n",
    "%store -r hiwiki_titles_normalized \n",
    "%store -r knwiki_titles_normalized\n",
    "%store -r mlwiki_titles_normalized\n",
    "%store -r mrwiki_titles_normalized\n",
    "%store -r orwiki_titles_normalized\n",
    "%store -r pawiki_titles_normalized\n",
    "%store -r sawiki_titles_normalized\n",
    "%store -r satwiki_titles_normalized\n",
    "%store -r tcywiki_titles_normalized \n",
    "%store -r tawiki_titles_normalized\n",
    "%store -r tewiki_titles_normalized \n",
    "%store -r urwiki_titles_normalized \n",
    "%store -r sawiki_titles_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read files and set variables which will be used in the notebook for querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 1) wcode & 2)article list for each wiki // AND remember to 3)rename the interim filename 4) and final 5) and above\n",
    "wcode = '{}'  #2\n",
    "article_list = {}wiki_titles_normalized      #3      # \"ਲਿੰਡਸੇ_ਵੋਨ\", \"ਇੰਗਮੇਰ_ਸਟੈਨਮਾਰਕ\", \"ਬਸਟਰ_ਕੀਟਨ\", \"ਮਨੋਰੰਜਨ\"...\n",
    "\n",
    "#replace contest start/end date, and country for each GLOW project\n",
    "contest_start = '2019-10-10'\n",
    "contest_end = '2020-02-11'\n",
    "wiki = wcode+'wiki'\n",
    "quality_vars = dict(\n",
    "    raw_articles  = article_list, # fyi, this later turns into clean_page_ids > clean_new_page_ids\n",
    "    project       = wcode+'.wikipedia',\n",
    "    country_code  = \"IN\",\n",
    "    wiki_db       = wiki,\n",
    "    contest_start = contest_start,\n",
    "    contest_end   = contest_end,\n",
    "    contest_start_dt        = datetime.strptime(contest_start, '%Y-%m-%d'),\n",
    "    contest_end_dt          = datetime.strptime(contest_end, '%Y-%m-%d'),\n",
    "    contest_end_dt_month    = datetime.strptime(contest_end, '%Y-%m-%d').strftime('%m'),\n",
    "    contest_end_dt_day      = datetime.strptime(contest_end, '%Y-%m-%d').strftime('%d'),\n",
    "    contest_end_dt_1M_month = (datetime.strptime(contest_end, '%Y-%m-%d') + timedelta(days=30)).strftime('%m'),\n",
    "    contest_end_dt_1M_day   = (datetime.strptime(contest_end, '%Y-%m-%d') + timedelta(days=30)).strftime('%d'),\n",
    "    MWH_SNAPSHOT = last_month.strftime(\"%Y-%m\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get clean article list: id, wiki, redirects <a class=\"anchor\" id=\"get_clean_list\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "       \n",
    "redirects_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {raw_articles}\n",
    "\"\"\".format(**quality_vars), wiki)\n",
    "\n",
    "\n",
    "#\"\"\".format(start=\"2017-06\", end=\"2018-06\"), \"wikishared\")\n",
    "#\"\"\".format(pa_articles_2018=pa_articles_2018)\n",
    "# MIN(p1.page_touched) AS last_modified,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((redirects_r['p1_is_redirect']==1) & (redirects_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((redirects_r['p1_is_redirect']==1) | (redirects_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>p1_is_redirect</th>\n",
       "      <th>page_len</th>\n",
       "      <th>rpage_id</th>\n",
       "      <th>rpage_title</th>\n",
       "      <th>rpage_len</th>\n",
       "      <th>is_double_redirect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [page_id, page_title, p1_is_redirect, page_len, rpage_id, rpage_title, rpage_len, is_double_redirect]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do not want any duplicates here\n",
    "redirects_r[redirects_r.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results from redirects_r\n",
    "\n",
    "#create a df \n",
    "all_surviving_articles = redirects_r[['page_id','page_title', 'page_len']] \n",
    "\n",
    "#seperate the redirected items into their own df\n",
    "redirects = redirects_r.loc[redirects_r['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title','page_len']] \n",
    "\n",
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "#create a new wikicode column using quality_vars['wiki_db']\n",
    "#ffill could also work here\n",
    "articles['wikicode'] = quality_vars['wiki_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tuple of clean non redirect page_ids to query \n",
    "clean_pageids = tuple(list(articles['page_id']))\n",
    "clean_pagetitles = (articles['page_title'])\n",
    "\n",
    "#non-normalized titles, use spaces instead of underscores and may include namespace name\n",
    "#create a tuple of clean page_titles denormalized (with spaces instead of underscores) for pulling wiki data items (see below)\n",
    "clean_titles_denormalized = tuple(list(clean_pagetitles.replace('_', ' ', regex=True)))\n",
    "\n",
    "#update the query variable to use it in queries\n",
    "quality_vars.update({'clean_pageids': clean_pageids,\n",
    "                     'clean_titles_denormalized': clean_titles_denormalized})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_pagetitles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.mediawiki.org/wiki/Content_translation\n",
    "https://www.mediawiki.org/wiki/Content_translation/Machine_Translation\n",
    "https://www.mediawiki.org/wiki/Extension:Translate\n",
    "https://www.mediawiki.org/wiki/Content_translation/Published_translations\n",
    "https://en.wikipedia.org//w/api.php?action=query&format=json&list=cxpublishedtranslations&to=hi&offset=500\n",
    "https://en.wikipedia.org//w/api.php?action=query&format=json&list=cxpublishedtranslations&to=hi&offset=500\n",
    "https://paws-public.wmflabs.org/paws-public/User:Isaac_(WMF)/content-translation-basics.ipynb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#https://phabricator.wikimedia.org/T201539\n",
    "\n",
    "#get list of articles that have an edit associated with the content translation tool\n",
    "att = hive.run(\"\"\"\n",
    "SELECT\n",
    "    page_id, \n",
    "    revision_tags\n",
    "FROM wmf.mediawiki_history\n",
    "WHERE\n",
    "    snapshot = \"{MWH_SNAPSHOT}\"\n",
    "    AND event_timestamp >=\"{contest_start}\"\n",
    "    AND event_timestamp <\"{contest_end}\"\n",
    "    AND page_namespace = 0\n",
    "    AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "    AND wiki_db = '{wiki_db}' \n",
    "    AND page_id IN {clean_pageids}\n",
    "GROUP BY \n",
    "    page_id, revision_tags\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#https://phabricator.wikimedia.org/T201539\n",
    "\n",
    "#get list of articles that have an edit associated with the content translation tool\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "att_r ='''\n",
    "SELECT\n",
    "    page_id, \n",
    "    revision_tags AS at_edits\n",
    "FROM wmf.mediawiki_history\n",
    "WHERE\n",
    "    snapshot = \"{MWH_SNAPSHOT}\"\n",
    "    AND event_timestamp >=\"{contest_start}\"\n",
    "    AND event_timestamp <\"{contest_end}\"\n",
    "    AND page_namespace = 0\n",
    "    AND event_entity = 'revision'\n",
    "    AND revision_is_identity_reverted = False \n",
    "    AND revision_is_deleted_by_page_deletion = False\n",
    "    AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "    AND wiki_db = '{wiki_db}' \n",
    "    AND page_id IN {clean_pageids}\n",
    "GROUP BY \n",
    "    page_id, revision_tags\n",
    "'''.format(**quality_vars)\n",
    "\n",
    "att = spark.sql(att_r.format(**quality_vars)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://phabricator.wikimedia.org/T201539\n",
    "\n",
    "#get list of articles that have an edit associated with the content translation tool\n",
    "\n",
    "at_edits = spark.run(\"\"\"\n",
    "SELECT\n",
    "    page_id, \n",
    "    revision_tags AS at_edits\n",
    "FROM wmf.mediawiki_history\n",
    "WHERE\n",
    "    snapshot = \"{MWH_SNAPSHOT}\"\n",
    "    AND event_timestamp >=\"{contest_start}\"\n",
    "    AND event_timestamp <\"{contest_end}\"\n",
    "    AND page_namespace = 0\n",
    "    AND event_entity = 'revision'\n",
    "    AND revision_is_identity_reverted = False \n",
    "    AND revision_is_deleted_by_page_deletion = False\n",
    "    AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "    AND wiki_db = '{wiki_db}' \n",
    "    AND page_id IN {clean_pageids}\n",
    "GROUP BY \n",
    "    page_id, revision_tags\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://phabricator.wikimedia.org/T201539\n",
    "#get list of articles that were created, instep with use of the article translation tool \n",
    "at_create = spark.run(\"\"\"\n",
    "SELECT\n",
    "    page_id, \n",
    "    revision_tags AS at_create\n",
    "FROM wmf.mediawiki_history\n",
    "WHERE\n",
    "    snapshot = \"{MWH_SNAPSHOT}\"\n",
    "    AND event_timestamp >=\"{contest_start}\"\n",
    "    AND event_timestamp <\"{contest_end}\"\n",
    "    AND page_namespace = 0\n",
    "    AND event_entity = 'page'\n",
    "    AND event_type = 'create'\n",
    "    AND revision_is_identity_reverted = False \n",
    "    AND revision_is_deleted_by_page_deletion = False\n",
    "    AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "    AND wiki_db = '{wiki_db}' \n",
    "    AND page_id IN {clean_pageids}\n",
    "GROUP BY \n",
    "    page_id, revision_tags\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge two df above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(at_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(at_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative length"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#see notebook '_collect_relative...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(IN_median_vi, on='wikicode', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['relative_page_len'] = articles['page_len']/articles['mpl_index']\n",
    "articles['relative_page_len'] = articles['relative_page_len'].clip(upper=1)\n",
    "del articles['mpl_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns\n",
    "articles = articles[[\n",
    " 'wikicode',\n",
    " 'page_id',\n",
    " 'page_title',\n",
    " 'page_len',\n",
    " 'relative_page_len',\n",
    " 'at_edits',\n",
    " 'at_create'   \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikidata Q item"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#create a tuple of clean page_titles denormalized (with spaces instead of underscores) for pulling wiki data items (see below)\n",
    "clean_titles_denormalized = all_surviving_non_r_articles.replace('_', '', regex=True)\n",
    "clean_titles_denormalized = tuple(list(all_surviving_non_r_articles['page_title'].str.replace('_', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Wikibase/Schema/wb_items_per_site\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#wb_items_per_site site:quarry.wmflabs.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_simple_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS page_title,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = '{wiki_db}' \n",
    "AND ips_site_page IN {clean_titles_denormalized}\n",
    "\"\"\".format(**quality_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the page_title back to normalized state by replacing spaces with underscores\n",
    "qid_simple_r['page_title'] = qid_simple_r['page_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(qid_simple_r[['QID', 'page_title']], on='page_title', how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Creation Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS first_edited\n",
    "first_edit_timestamp = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id, \n",
    "    rev_timestamp AS first_edited\n",
    "FROM revision \n",
    "JOIN page ON page_id = rev_page\n",
    "WHERE rev_page IN {clean_pageids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_edit_timestamp['first_edited']= pd.to_datetime(first_edit_timestamp['first_edited']) \n",
    "first_edit_timestamp['first_edited'] = first_edit_timestamp['first_edited'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(first_edit_timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pageviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_1M = hive.run(\"\"\"\n",
    "SELECT \n",
    "   page_id,\n",
    "   SUM(view_count) AS views_1M\n",
    "FROM wmf.pageview_hourly \n",
    "WHERE \n",
    "  year = 2020\n",
    "  AND (month >= {contest_end_dt_month} AND day >= {contest_end_dt_day}) \n",
    "  AND (month <= {contest_end_dt_1M_month} AND day <= {contest_end_dt_1M_day}) \n",
    "  AND agent_type = 'user'\n",
    "  AND country_code = '{country_code}'\n",
    "  AND project = '{project}'\n",
    "  AND page_id IN {clean_pageids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#if this provides results, there are dupes in pv query results\n",
    "dupe_test = pd.concat(g for _, g in pv_1M.groupby(\"page_id\") if len(g) > 1)\n",
    "\n",
    "#see duped rows\n",
    "duplicateRowsDF = pv_1M[pv_1M.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#articles = articles.merge(pv_1M.drop_duplicates(subset=['page_id']), how='left').fillna(0) #not a good method as this cuts some of the counts\n",
    "\n",
    "merge_in_content(pv_1M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for new articles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify article types based on time of first edit: 'new', 'expanded', or 'post'\n",
    "articles['article_type'] = articles['first_edited'].apply(create_fill_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change file name below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to csv file at this point\n",
    "articles.to_csv(\"../../data/raw/articles/2019/query_results/content_quality/per_wiki/{}_interim_articles.csv\", sep=',', encoding = 'utf-8', index=False) #4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In order to fairly compare the output of different campaigns, we will look only at new articles created during the contest from here out. \n",
    "This allows us to start the slate at 0 for all articles we assess.\n",
    "We will not use articles written prior as those had a 'running start' to gather higher counts across the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop articles created after the contest ended\n",
    "articles_n = articles[articles.article_type !='post']\n",
    "\n",
    "#create articles_new df only for newly created contest articles\n",
    "#articles_new = first_edit_timestamp[(first_edit_timestamp['first_edited'] >= quality_vars.get('contest_start_dt')) & (first_edit_timestamp['first_edited'] <= quality_vars.get('contest_end_dt'))]  #this actually keeps all of the articles\n",
    "\n",
    "articles_new = articles[articles.article_type =='new']\n",
    "articles_new = articles_n.merge(articles_new, on=['page_id'], how='inner', suffixes=('', '_y'))\n",
    "\n",
    "articles_new.drop(list(articles_new.filter(regex='_y$')), axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tuple of clean NEW page_ids to query \n",
    "clean_new_pageids = tuple(list(articles_new['page_id']))\n",
    "clean_new_pagetitles_list = list(articles_new['page_title'])\n",
    "quality_vars.update({'clean_new_pageids': clean_new_pageids})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edits & Editors per article for time filtering <a class=\"anchor\" id=\"editors_active\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edits & editors: all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://phabricator.wikimedia.org/T231598\n",
    "#The (rev_deleted & 4) = 0 condition is to exclude revisions where the user has been RevDeled, as we don't want to leak information about how many such users there are.\n",
    "\n",
    "edits_editors_all = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,    \n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {clean_new_pageids}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**quality_vars), wiki)\n",
    "\n",
    "edits_editors_all['edit_date'] = pd.to_datetime(edits_editors_all['edit_date'], format=\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with articles_new: first_edit_timestamp df which was filtered for only new articles and put into 'articles_new' df\n",
    "ee_fe = pd.merge(edits_editors_all, articles[['page_id','page_title', 'first_edited']], on='page_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edits & editors: registered, non-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_editors_reg_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,    \n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "JOIN actor ON (revactor_actor = actor_id)\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND actor_user IS NOT NULL -- user cannot be non-registered\n",
    "    AND actor_user NOT IN (SELECT ug_user FROM user_groups WHERE ug_group = \"bot\") -- not a bot\n",
    "    AND rev_page IN {clean_new_pageids}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**quality_vars), wiki)\n",
    "\n",
    "eern = edits_editors_reg_r.copy()\n",
    "eern['edit_date'] = pd.to_datetime(eern['edit_date'], format=\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with articles_new: first_edit_timestamp df which was filtered for only new articles and put into 'articles_new' df\n",
    "eern_fe = pd.merge(eern, articles_new[['page_id','page_title', 'first_edited']], on='page_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a timedelta column\n",
    "eern_fe['edit_td'] = eern_fe['edit_date']-eern_fe['first_edited']\n",
    "\n",
    "# filter for only edits in first 30 days\n",
    "m1 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(30, unit='d')]\n",
    "\n",
    "# filter for only edits in first 60 days\n",
    "m2 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(60, unit='d')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#calculate total edits - ## TO DO update code here to allow the 'total_edits' column creation\n",
    "eern_fe['total_edits'] = eern_fe.groupby(['page_id', 'page_title'])['edit_date'].agg('count').reset_index(name='total_edits')\n",
    "\n",
    "m = eern_fe.groupby(['page_id', 'page_title'])['edit_date'].agg('count').reset_index(name='total_edits')\n",
    "\n",
    "# merge \n",
    "eern_fe = pd.merge(eern, m[['page_id','page_title', 'total_edits']], on='page_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter for edits by time period and use those in groupby counts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1['revactor_actor'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only edits in first 30 days\n",
    "m1 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(30, unit='d')]\n",
    "\n",
    "# filter for only edits in first 60 days\n",
    "m2 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(60, unit='d')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### editor counts per article at 1M, 2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_M1_counts = m1.groupby(['page_id', 'page_title'])['revactor_actor'].nunique().reset_index(name='editors_1stM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_M2_counts = m2.groupby(['page_id', 'page_title'])['revactor_actor'].nunique().reset_index(name='editors_2ndM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_by_M_calculations = pd.merge(editors_M1_counts, editors_M2_counts[['page_id', 'editors_2ndM']],\n",
    "                               on='page_id', \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edit counts per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_counts_1M = m1.groupby(['page_id', 'page_title'])['edit_date'].agg('count').reset_index(name='edits_1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_counts_2M = m2.groupby(['page_id', 'page_title'])['edit_date'].agg('count').reset_index(name='edits_2M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_by_M_calculations = pd.merge(edit_counts_1M, edit_counts_2M[['page_id', 'edits_2M']],\n",
    "                               on='page_id', \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.merge(articles, editors_M1_counts[['page_id', 'editors_1stM']],\n",
    "                               on='page_id', \n",
    "                               how='left').merge(edit_counts_1M[['page_id', 'edits_1M']], \n",
    "                               on='page_id', \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edits & timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This timestamp is updated whenever the page changes in a way requiring it to be re-rendered, invalidating caches. \n",
    "#Aside from editing, this includes permission changes, creation or deletion of linked pages, and alteration of contained templates. \n",
    "#[[mw:Manual:Revision_table]] and [[mw:Manual:Page_table]]. Only show latest edits does an inner join from revision table to page table on rev_id = page_latest .\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "#https://github.com/x-tools/xtools/blob/master/src/AppBundle/Repository/ArticleInfoRepository.php#L162-L171\n",
    "#https://xtools.wmflabs.org/articleinfo/pa.wikipedia.org/ਏਸ਼ੀਆ\n",
    "\n",
    "edits = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    rev_page AS page_id, \n",
    "    COUNT(rev_id) AS num_edits_all_time,\n",
    "    SUM(rev_minor_edit) AS minor_edits_all_time\n",
    "FROM revision\n",
    "JOIN page ON page_id = rev_page\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND rev_page IN {clean_new_pageids}\n",
    "GROUP BY rev_page\n",
    "\"\"\".format(**quality_vars), wiki)\n",
    "\n",
    "timestamps = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    rev_page AS page_id, \n",
    "    max(rev_timestamp) AS last_edited \n",
    "FROM revision \n",
    "JOIN page \n",
    "  ON page_id = rev_page\n",
    "WHERE rev_page IN {clean_new_pageids}\n",
    "AND rev_id = page_latest\n",
    "GROUP BY rev_page\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps['last_edited']= pd.to_datetime(timestamps['last_edited']) \n",
    "timestamps['last_edited'] = timestamps['last_edited'].dt.normalize()\n",
    "\n",
    "merge_in_content(edits)\n",
    "merge_in_content(timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editors: total number of unique editors, including IP editors and bots...all editors of all edits, microcontributions\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_actor_temp_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "#https://www.mediawiki.org/wiki/Help:RevisionDelete\n",
    "#adapted from https://phabricator.wikimedia.org/T231598#5465711\n",
    "#questioned in https://phabricator.wikimedia.org/T234560#5545319\n",
    "#taken into account: rev_deleted to avoid leaking information re: how many distinct users were involved in revision-deleted edits\n",
    "tuepa_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page.page_id AS page_id,\n",
    "    COUNT(DISTINCT revactor_actor) AS all_editors_of_all_edits\n",
    "FROM revision_actor_temp\n",
    "  JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "  JOIN page ON rev_page = page.page_id  \n",
    "WHERE rev_page = page.page_id \n",
    "  AND (rev_deleted & 4) = 0\n",
    "  AND page.page_id IN {clean_new_pageids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query post Morten review\n",
    "#editors: total, unique, non-bot, registered editors that made non-minor edits\n",
    "tunbre_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "  revision.rev_page AS page_id,\n",
    "  COUNT(DISTINCT revactor_actor) AS editors_nm\n",
    "FROM revision_actor_temp\n",
    "  JOIN revision ON (revactor_rev = rev_id)\n",
    "  JOIN actor ON (revactor_actor = actor_id)\n",
    "WHERE (rev_deleted & 4) = 0\n",
    "  AND rev_minor_edit = 0\n",
    "  AND actor_user IS NOT NULL -- user cannot be non-registered\n",
    "  AND actor_user NOT IN (SELECT ug_user FROM user_groups WHERE ug_group = \"bot\") -- not a bot\n",
    "  AND revision.rev_page IN {clean_new_pageids}\n",
    "GROUP BY revision.rev_page\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editors: total number of unique editors, including IP editors and bots...all editors of all edits, microcontributions\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_actor_temp_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "#https://www.mediawiki.org/wiki/Help:RevisionDelete\n",
    "#adapted from https://phabricator.wikimedia.org/T231598#5465711\n",
    "#questioned in https://phabricator.wikimedia.org/T234560#5545319\n",
    "#taken into account: rev_deleted to avoid leaking information re: how many distinct users were involved in revision-deleted edits\n",
    "tueme_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page.page_id,\n",
    "    COUNT(DISTINCT revactor_actor) AS micro_editors\n",
    "FROM revision_actor_temp\n",
    "  JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "  JOIN page ON rev_page = page.page_id  \n",
    "WHERE rev_page = page.page_id \n",
    "  AND (rev_deleted & 4) = 0\n",
    "  AND page.page_id IN {clean_new_pageids}\n",
    "  AND rev_minor_edit = 1\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated!\n",
    "#total unique IP editors, \n",
    "#see also: https://phabricator.wikimedia.org/T231605\n",
    "#https://meta.wikimedia.org/wiki/IP_Editing:_Privacy_Enhancement_and_Abuse_Mitigation/Research\n",
    "#https://github.com/nettrom/AHT-block-effectiveness-2018\n",
    "#https://github.com/wikimedia-research/AHT-IP-edits-2019/blob/master/edit_usefulness.ipynb\n",
    "#https://meta.wikimedia.org/wiki/User:Benjamin_Mako_Hill/Research_on_the_value_of_IP_Editing\n",
    "tuipe_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "  revision.rev_page AS page_id,\n",
    "  COUNT(DISTINCT revactor_actor) AS IP_editors\n",
    "FROM revision\n",
    "  JOIN revision_actor_temp ON (rev_id = revactor_rev)\n",
    "  JOIN actor ON (revactor_actor = actor_id)\n",
    "WHERE (rev_deleted & 4) = 0\n",
    "  AND actor_user IS NULL -- non-registered user\n",
    "  AND revision.rev_page IN {clean_new_pageids}\n",
    "GROUP BY rev_page\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### editor calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_calculations = pd.merge(tuepa_r, tunbre_r[['page_id', 'editors_nm']], on='page_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_calculations = pd.merge(editor_calculations, tuipe_r[['page_id', 'IP_editors']],\n",
    "                               on='page_id', \n",
    "                               how='left').merge(tueme_r[['page_id', 'micro_editors']], \n",
    "                               on='page_id', \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(editor_calculations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article's talk page activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://meta.wikimedia.org/wiki/Research:Usage_of_talk_pages/2019-11-11#arwiki\n",
    "#https://meta.wikimedia.org/wiki/Research:Newsletter/2011/August\n",
    "#http://jodischneider.com/pubs/sac2011.pdf\n",
    "#https://meta.wikimedia.org/wiki/Research:Newsletter/2015/May#Editors_who_use_user_talk_pages_are_more_involved_in_high-quality_articles\n",
    "#https://meta.wikimedia.org/wiki/Research:New     vI will be happy to take a look at your queries. I will be happy to take a look at your queries. vsletter/2017/May#cite_note-9\n",
    "#https://www.opensym.org/wp-content/uploads/2018/07/OpenSym2018_paper_14.pdf\n",
    "#https://phabricator.wikimedia.org/T214935 -- on talk page click through rates\n",
    "#SQL:\n",
    "#https://github.com/wikimedia-research/Talkcicity/blob/master/retrieve_talkpage_data.R\n",
    "#https://github.com/x-tools/xtools/blob/master/src/AppBundle/Repository/ArticleInfoRepository.php#L221-L226\n",
    "#https://github.com/wikimedia-research/2019-10-talk-pages-baseline-metrics/blob/master/2019-10-talk-page-contributors-analysis.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_page_edits = wmf.mariadb.run(\"\"\" \n",
    "SELECT \n",
    "    pa.page_id, \n",
    "    SUM(IF(rev_id IS NOT NULL, 1, 0)) AS talk_page_edits\n",
    "FROM page pa\n",
    "LEFT JOIN page pt\n",
    "    ON pa.page_title = pt.page_title\n",
    "    AND pt.page_namespace = 1\n",
    "LEFT JOIN revision\n",
    "    ON pt.page_id = rev_page\n",
    "WHERE pa.page_id IN {clean_new_pageids}\n",
    "    AND pt.page_namespace = 1\n",
    "    AND (rev_deleted & 4) = 0\n",
    "GROUP BY pa.page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(talk_page_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watchlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phabricator.wikimedia.org/source/mediawiki/browse/master/maintenance/tables.sql\n",
    "\n",
    "article_watch_count = wmf.mariadb.run(\"\"\"\n",
    "SELECT page_id,\n",
    "       COUNT(*) AS watch_count\n",
    "FROM watchlist\n",
    "JOIN page \n",
    "     ON (wl_title = page_title AND wl_namespace = page_namespace)\n",
    "WHERE page_namespace = 0 \n",
    "    AND page_id IN {clean_new_pageids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(article_watch_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revert rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the revert rate (rr) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_replicas = wmf.mariadb.run(\"\"\"\n",
    "SELECT page_id, \n",
    "       COUNT(DISTINCT rev_id) AS revertrate\n",
    "FROM revision\n",
    " JOIN change_tag \n",
    "     ON ct_rev_id = rev_id\n",
    " JOIN change_tag_def \n",
    "     ON ct_id = ctd_id\n",
    " JOIN page \n",
    "     ON rev_page = page_id\n",
    "WHERE ctd_name IN ('mw-rollback', 'mw-undo')\n",
    "    AND page_id IN {clean_new_pageids}\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(rr_replicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# links <a class=\"anchor\" id=\"links\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagelinks and redirects examples\n",
    "#https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L227\n",
    "\n",
    "#linking\n",
    "#https://en.wikipedia.org/wiki/Special:WhatLinksHere/Wikipedia:Manual_of_Style/Linking\n",
    "#https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#General_principles\n",
    "#https://www.mediawiki.org/wiki/API:Links\n",
    "\n",
    "#backlinks+\n",
    "#https://dispenser.info.tm/~dispenser/cgi-bin/backlinkscount.py (backlinks)\n",
    "#https://github.com/wikimedia/mediawiki-api-demos/blob/master/python/get_backlinks.py\n",
    "\n",
    "#tables\n",
    "#pagelinks contains links to other pages on the same wiki...provide cohesion and utility\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table (internal links in the same wiki, from the page)\n",
    "\n",
    "#externallinks contains links to elsewhere, outside of all wikis\n",
    "#https://www.mediawiki.org/wiki/Manual:Externallinks_table (external links, from the page)\n",
    "\n",
    "#interwikilinks links an article in one language to the same article in another language. For most articles these are stored on Wikidata. \n",
    "#https://en.wikipedia.org/wiki/Help:Interwiki_linking\n",
    "#https://www.mediawiki.org/wiki/Manual:Iwlinks_table\n",
    "\n",
    "#langlinks links that point to a page on another wiki (e.g. [[mw:Product Analytics]] links to the PA team’s page on MediaWiki-wiki.\n",
    "#https://en.wikipedia.org/wiki/Help:Interlanguage_links#Local_links\n",
    "#https://www.mediawiki.org/wiki/Manual:Langlinks_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# * pagelinks contains links to other pages on the same wiki, \n",
    "\n",
    "* externallinks contains links to elsewhere, outside of all wikis\n",
    "\n",
    "* interwikilinks links an article in one language to the same article in another language. For most articles these are stored on Wikidata. \n",
    "https://en.wikipedia.org/wiki/Help:Interwiki_linking\n",
    "\n",
    "* langlinks links that point to a page on another wiki (e.g. [[mw:Product Analytics]] links to the PA team’s page on MediaWiki-wiki.\n",
    "https://en.wikipedia.org/wiki/Help:Interlanguage_links#Local_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pagelinks <a class=\"anchor\" id=\"pagelinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TECHNICAL NOTE FROM MORTEN: `pl.pl_namespace = 0` is not needed in the `WHERE` clause because you're operating on page IDs. Unless you also want to specificy that only articles are to be included, though. But I would in that case restrict that much earlier, when converting page titles to page IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagelinks: linking to articles within the same wiki\n",
    "opl_r = wmf.mariadb.run(\"\"\"\n",
    "    SELECT \n",
    "        pl.pl_from, \n",
    "        link.page_id AS plpage,\n",
    "        link.page_title AS plpage_title,\n",
    "        redir.page_id AS rpage,\n",
    "        redir.page_title AS rpage_title,\n",
    "        redir.page_is_redirect AS is_double_redirect\n",
    "    FROM pagelinks AS pl\n",
    "    JOIN page AS link\n",
    "        ON (pl.pl_namespace=link.page_namespace\n",
    "        AND pl.pl_title=link.page_title)\n",
    "    LEFT JOIN redirect AS rd\n",
    "        ON link.page_id=rd.rd_from\n",
    "    LEFT JOIN page AS redir\n",
    "        ON (rd.rd_namespace=redir.page_namespace\n",
    "        AND rd.rd_title=redir.page_title)\n",
    "    WHERE pl.pl_namespace=0 AND pl.pl_from IN {clean_new_pageids}\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any of the page_ids are double redirects\n",
    "((opl_r['is_double_redirect']==1).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if an anchor's target is duplicated...for duplicate instances of a link within a single page\n",
    "#pagelinks_r[(pagelinks_r.duplicated('pl_from') & pagelinks_r.duplicated('lpage'))] #checks for duplicates in either column\n",
    "opl_r[opl_r.duplicated(['pl_from','plpage'])] #checks for duplicates in two columns at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there are no duplicated targets from each anchor, we can count the number of occurrences for each anchor as the target_count\n",
    "opls = opl_r['pl_from'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'pl_from':'oplinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(opls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### external links <a class=\"anchor\" id=\"extlinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# externallinks \n",
    "#https://www.mediawiki.org/wiki/Manual:Externallinks_table\n",
    "oel_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT el_from, \n",
    "       el_to\n",
    "FROM externallinks AS el\n",
    "WHERE el.el_from IN {clean_new_pageids}\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if any of the targets are duplicated in the article\n",
    "oel_r.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no external links are duplicated, then count targets arising from each anchor:\n",
    "#because there are no duplicated targets from each anchor, we can count the number of occurrences for each anchor as the target_count\n",
    "oextlinks = oel_r['el_from'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'el_from':'oelinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(oextlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorize opls & oextlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bins to seperate number of pagelinks into categories\n",
    "#seperate number of links into bins defined for oel_bins \n",
    "opls[\"binned_links\"] = pd.cut(opls[\"oplinks\"], oel_bins, right=False)\n",
    "\n",
    "#sort\n",
    "opls[\"binned_links\"].value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use bins to seperate number of pagelinks into categories\n",
    "#seperate number of links into bins defined for oext_bins \n",
    "oextlinks[\"binned_links\"] = pd.cut(oextlinks[\"oelinks\"], oext_bins, right=False)\n",
    "\n",
    "#sort\n",
    "oextlinks[\"binned_links\"].value_counts(sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoming pagelinks <a class=\"anchor\" id=\"inpagelinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES FROM MORTEN:\n",
    "Use backlinks query from here: https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L229 It already uses a list of page IDs as the basis for the query, but does limit links to within the article namespace.\n",
    "\n",
    "Then I'd just remove `AS ilc_page_id` and change `AS ilc_numlinks` to `AS numlinks` or something. That query is pretty optimized, and also counts inlinks coming in through redirects. You'd definitely want to do something like this using the replicated MW databases than going through any API. If you're working with large numbers of articles (e.g. thousands), I can show you how to iterate over them in groups to speed things up, by the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incoming pagelinks: links from within the same wiki\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#resource: https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L229\n",
    "#\tpl_from, pl_from_namespace #anchor\n",
    "#\tpl_namespace\tpl_title #target\n",
    "incoming_pagelinks_r = wmf.mariadb.run(\"\"\"\n",
    "    SELECT \n",
    "        link.page_id AS page_id,\n",
    "        pl.pl_title AS page_title,\n",
    "        pl.pl_from AS in_pagelinks\n",
    "    FROM pagelinks AS pl\n",
    "    JOIN page AS link\n",
    "        ON (pl.pl_namespace=link.page_namespace\n",
    "        AND pl.pl_title=link.page_title)\n",
    "    LEFT JOIN redirect AS rd\n",
    "        ON link.page_id=rd.rd_from\n",
    "    LEFT JOIN page AS redir\n",
    "        ON (rd.rd_namespace=redir.page_namespace\n",
    "        AND rd.rd_title=redir.page_title)\n",
    "    WHERE pl.pl_namespace=0\n",
    "        AND link.page_id IN {clean_new_pageids}\n",
    "\"\"\".format(**quality_vars), wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_pagelinks = incoming_pagelinks_r['page_id'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'page_id':'ipl_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_in_content(incoming_pagelinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iwsitelinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create qid list from column for pulling categories (use quid_simple_r from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_simple_r.rename(columns={'page_title':'title','q_id':'QID'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = tuple(list(qid_simple_r['QID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_vars.update({'qids': qids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "        \n",
    "ips_sites = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  linked_item.ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM (\n",
    "      SELECT ips_item_id\n",
    "      FROM wb_items_per_site\n",
    "      WHERE ips_site_id = '{wiki_db}' AND ips_item_id IN {qids}\n",
    "    ) AS linked_item\n",
    "LEFT JOIN wb_items_per_site \n",
    "  ON linked_item.ips_item_id = wb_items_per_site.ips_item_id\n",
    "LEFT JOIN page \n",
    "  ON linked_item.ips_item_id = page.page_id\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**quality_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(ips_sites[['iwsitelinks', 'iwsites', 'QID']], on='QID', how=\"left\").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv(\"../../data/processed/query_results/content_quality/per_wiki/{}_articles.csv\", sep=',', encoding = 'utf-8', index=False) #5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
