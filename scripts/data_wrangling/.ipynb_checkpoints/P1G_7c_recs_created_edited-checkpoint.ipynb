{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "1. ['%' topic lists (topicality)](#tprcnt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Guiding Question:\n",
    "Q: how many articles were created from the suggestions? \n",
    "\n",
    "Process:\n",
    "1. Combine and clean suggestion lists\n",
    "2. get wikidata items for all of the suggestions provided:\n",
    "   a. groupby type and get wikidata items for translations using enwiki\n",
    "   b. groupby type and get wikidata items for the editing list using the local language\n",
    "3. get page ids\n",
    "   a. editing list - check each whether the ids were edited during the contest period \n",
    "   b. translation list - check each whether the ids exist; for those that don't exist: filter for any matches by QID or iwlinks\n",
    "\n",
    "TODO:\n",
    "get latest topic model - see Isaac\n",
    "Answer the following questions in the future, if time/project level interest:\n",
    "Q: which topics did editors write about?\n",
    "Q: what did editors select from these suggestions? \n",
    "Q: which topics most resonated or were most popular to write about from these lists? by wiki?\n",
    "Q: which topics did our partner pass on to us...which search terms made their way to us?\n",
    "\n",
    "NOTES:\n",
    "If the editor uses Content Translation, it should automatically assign the right QID\n",
    "If the editor doesn't use CT, either they or someone else has to assign the QID\n",
    "We will miss articles that were not created via Content Translation and don't have a manually added QID and/or the editor changed the suggested title to something new. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the creators of the suggestions:\n",
    "\"As a reminder, we have 2 lists: a list of suggested topics that exist in the local language but could be edited to be more complete based on the corresponding English page, and a list of topics that can be translated from English to the local language.  Based on feedback from the initial Project Tiger, we've separated out the topics by categories so editors can focus on the areas they like to write about.  The lists are ordered by popularity of what local language users are looking for.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata 0.1.0 (latest).\n",
      "\n",
      "You can find the source for `wmfdata` at https://github.com/neilpquinn/wmfdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import urllib\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile all of the lists\n",
    "\n",
    "f_mask = r'../../../GLOW/data/raw/g_topic_lists/*.xlsx'\n",
    "\n",
    "gtl = \\\n",
    "pd.concat([gtl.assign(file=os.path.splitext(os.path.basename(f))[0],\n",
    "                     sheet=sheet)\n",
    "           for f in glob(f_mask)\n",
    "           for sheet, gtl in pd.read_excel(f, sheet_name=None).items()],\n",
    "          ignore_index=True, sort=True)\n",
    "\n",
    "full_topic_rec_df = gtl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Topic & entity name > article_suggestion\n",
    "full_topic_rec_df['article_suggestion'] = full_topic_rec_df['entity_name'].combine_first(full_topic_rec_df['Topic'])\n",
    "#rename 'sheet' to Google_topic\n",
    "full_topic_rec_df = full_topic_rec_df.rename(columns={'sheet':'g_category', \n",
    "                                                      'Topic': 'g_suggested_en_title', \n",
    "                                                      'entity_name': 'g_suggested_local_title',\n",
    "                                                      'english_wikipedia':'english_wikipedia_URL', \n",
    "                                                      'local_wikipedia':'local_wikipedia_URL'\n",
    "                                                     })\n",
    "\n",
    "#extract wiki name & suggestion_type (translation or edit)\n",
    "#full_topic_rec_df['wiki'] = full_topic_rec_df['file'].str.extract('(^[A-Z_]+([^\\(-]+))', expand=True)\n",
    "full_topic_rec_df[['language_name', 'suggestion_type']] = full_topic_rec_df['file'].str.split(\" \", 1, expand=True)\n",
    "full_topic_rec_df['suggestion_type'] = full_topic_rec_df['file'].str.rsplit(\"for \").str[-1]\n",
    "\n",
    "#extract url title\n",
    "full_topic_rec_df['local_encoded_title'] = full_topic_rec_df['local_wikipedia_URL'].str.extract('([^\\/]+$)', expand=True)\n",
    "\n",
    "#extract lang code\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['local_wikipedia_URL'].str.rsplit(\"http://\").str[-1]\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['url_language_code'].str.extract('([^.]+)', expand=True)\n",
    "\n",
    "#reorder for visual skimming's sake\n",
    "full_topic_rec_df = full_topic_rec_df[['article_suggestion', 'local_encoded_title','g_category', 'language_name','suggestion_type', 'file']] #'url_language_code'\n",
    "\n",
    "#replace double coded translation entries\n",
    "full_topic_rec_df['suggestion_type']=full_topic_rec_df['suggestion_type'].replace('Translating EXTERNAL', 'Translation EXTERNAL')\n",
    "full_topic_rec_df['language_name'] = full_topic_rec_df['language_name'].replace('Bengali', 'Bangla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_suggestion</th>\n",
       "      <th>local_encoded_title</th>\n",
       "      <th>g_category</th>\n",
       "      <th>language_name</th>\n",
       "      <th>suggestion_type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_suggestion, local_encoded_title, g_category, language_name, suggestion_type, file]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_topic_rec_df[full_topic_rec_df['language_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get database_code and language_code, confirm language_code (if needed)\n",
    "lang_names =tuple(full_topic_rec_df['language_name'].unique())\n",
    "\n",
    "ci = wmf.hive.run(\"\"\"\n",
    "SELECT  language_code, database_code, language_name\n",
    "FROM canonical_data.wikis\n",
    "WHERE language_name IN {lang_names} AND database_group = 'wikipedia'\n",
    "\"\"\".format(lang_names=lang_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "full_topic_rec_df_ci = full_topic_rec_df.merge(ci, how=\"left\", on=['language_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 34295 entries, 0 to 34294\n",
      "Data columns (total 8 columns):\n",
      "article_suggestion     34295 non-null object\n",
      "local_encoded_title    20102 non-null object\n",
      "g_category             34295 non-null object\n",
      "language_name          34295 non-null object\n",
      "suggestion_type        34295 non-null object\n",
      "file                   34295 non-null object\n",
      "language_code          34295 non-null object\n",
      "database_code          34295 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "full_topic_rec_df_ci.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "full_topic_rec_df_ci['article_suggestion'] = full_topic_rec_df_ci['article_suggestion'].str.replace(' ', '_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pnb article_suggestions exist, confirm language_code = url_language_code(see code above to add in url_language_code column)\n",
    "#pnb\tpnbwiki\tWestern Punjabi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "translation_topic_rec_df = full_topic_rec_df_ci[full_topic_rec_df_ci['suggestion_type'] == 'Translation EXTERNAL'].copy(deep=False)\n",
    "\n",
    "#get clean list - drop duplicates\n",
    "translation_topic_rec_df_CLEAN = translation_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'local_encoded_title','g_category','suggestion_type', 'language_name', 'language_code', 'database_code', 'file'], keep='first').copy(deep=False)\n",
    "\n",
    "#keep just the duplicates - for checking data later on\n",
    "translation_topic_rec_df_Dupes = pd.concat([translation_topic_rec_df, translation_topic_rec_df_CLEAN]).loc[translation_topic_rec_df.index.symmetric_difference(translation_topic_rec_df_CLEAN.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "editing_topic_rec_df = full_topic_rec_df_ci.loc[full_topic_rec_df_ci['suggestion_type'] == 'Editing EXTERNAL'].copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded URL to decoded title\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['local_encoded_title'].apply(lambda x: unquote(x)).copy(deep=False)\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['page_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#encoded URL to decoded title\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['local_encoded_title'].apply(lambda x: unquote(x))\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['page_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clean list - drop duplicates\n",
    "editing_topic_rec_df_CLEAN = editing_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'page_title','local_encoded_title','g_category', 'suggestion_type', 'language_name', 'language_code', 'database_code', 'file'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20102 entries, 0 to 30319\n",
      "Data columns (total 9 columns):\n",
      "article_suggestion     20102 non-null object\n",
      "local_encoded_title    20102 non-null object\n",
      "g_category             20102 non-null object\n",
      "language_name          20102 non-null object\n",
      "suggestion_type        20102 non-null object\n",
      "file                   20102 non-null object\n",
      "language_code          20102 non-null object\n",
      "database_code          20102 non-null object\n",
      "page_title             20102 non-null object\n",
      "dtypes: object(9)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "editing_topic_rec_df_CLEAN.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_vars = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASSESS THE DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total values in full: 34295\n",
      "***\n",
      "total values in translation: 14193\n",
      "total values in editing list 20102\n",
      "***\n",
      "total values in clean translation editing: 14155\n",
      "total values in clean editing: 20102\n"
     ]
    }
   ],
   "source": [
    "print(\"total values in full:\", len(full_topic_rec_df_ci))\n",
    "print('***')\n",
    "print(\"total values in translation:\", len(translation_topic_rec_df))\n",
    "print(\"total values in editing list\", len(editing_topic_rec_df))\n",
    "print('***')\n",
    "print(\"total values in clean translation editing:\", len(translation_topic_rec_df_CLEAN))\n",
    "print(\"total values in clean editing:\", len(editing_topic_rec_df_CLEAN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation Dupes DF len 38\n",
      "putting it all together: 34295\n"
     ]
    }
   ],
   "source": [
    "editing_diff = len(editing_topic_rec_df)-len(editing_topic_rec_df_CLEAN)\n",
    "translation_diff = len(translation_topic_rec_df)-len(translation_topic_rec_df_CLEAN)\n",
    "diffs_sum = editing_diff+translation_diff\n",
    "print(\"Translation Dupes DF len\", len(translation_topic_rec_df_Dupes))\n",
    "print(\"putting it all together:\",len(translation_topic_rec_df_Dupes) +len(translation_topic_rec_df_CLEAN)+len(editing_topic_rec_df_CLEAN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get article ids, redirects <a class=\"anchor\" id=\"get_clean_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpt = editing_topic_rec_df_CLEAN.loc[editing_topic_rec_df_CLEAN['page_title'].notnull(), ['database_code', 'page_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20102 entries, 0 to 30319\n",
      "Data columns (total 2 columns):\n",
      "database_code    20102 non-null object\n",
      "page_title       20102 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 471.1+ KB\n"
     ]
    }
   ],
   "source": [
    "nnpt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mlwiki', 'pawiki', 'hiwiki', 'orwiki', 'urwiki', 'tawiki',\n",
       "       'knwiki', 'mrwiki', 'guwiki', 'tewiki', 'bnwiki'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt['database_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = tuple(list(nnpt['database_code'].unique()))\n",
    "\n",
    "wd_vars.update({'wikis': wikis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pawiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'pawiki', 'page_title']))\n",
    "mlwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'mlwiki', 'page_title']))\n",
    "hiwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'hiwiki', 'page_title']))\n",
    "orwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'orwiki', 'page_title']))\n",
    "urwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'urwiki', 'page_title']))\n",
    "tawiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'tawiki', 'page_title']))\n",
    "knwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'knwiki', 'page_title']))\n",
    "mrwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'mrwiki', 'page_title']))\n",
    "guwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'guwiki', 'page_title']))\n",
    "tewiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'tewiki', 'page_title']))\n",
    "bnwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'bnwiki', 'page_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the query variable to use it in queries\n",
    "wd_vars.update({'pawiki_titles_normalized': pawiki_titles_normalized,\n",
    "                     'mlwiki_titles_normalized': mlwiki_titles_normalized,\n",
    "                     'hiwiki_titles_normalized': hiwiki_titles_normalized,\n",
    "                     'orwiki_titles_normalized': orwiki_titles_normalized,\n",
    "                     'urwiki_titles_normalized': urwiki_titles_normalized,\n",
    "                     'tawiki_titles_normalized': tawiki_titles_normalized,\n",
    "                     'knwiki_titles_normalized': knwiki_titles_normalized,\n",
    "                     'mrwiki_titles_normalized': mrwiki_titles_normalized,\n",
    "                     'guwiki_titles_normalized': guwiki_titles_normalized,\n",
    "                     'tewiki_titles_normalized': tewiki_titles_normalized,\n",
    "                     'bnwiki_titles_normalized': bnwiki_titles_normalized,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "hi_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {hiwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'hiwiki')\n",
    "\n",
    "\n",
    "or_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {orwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'orwiki')\n",
    "\n",
    "ur_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {urwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'urwiki')\n",
    "\n",
    "\n",
    "ta_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {tawiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'tawiki')\n",
    "\n",
    "kn_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {knwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'knwiki')\n",
    "\n",
    "mr_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {mrwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'mrwiki')\n",
    "\n",
    "gu_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {guwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'guwiki')\n",
    "\n",
    "\n",
    "te_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {tewiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'tewiki')\n",
    "\n",
    "    \n",
    "bn_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {bnwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'bnwiki')\n",
    "    \n",
    "    \n",
    "pa_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {pawiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'pawiki')\n",
    "\n",
    "\n",
    "ml_id_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_title AS page_title,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p1.page_len AS page_len,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "FROM page AS p1 \n",
    "LEFT JOIN redirect AS rd \n",
    "    ON p1.page_id=rd.rd_from \n",
    "LEFT JOIN page AS p2 \n",
    "    ON (rd_namespace = p2.page_namespace)\n",
    "        AND rd_title = p2.page_title  \n",
    "WHERE p1.page_namespace = 0\n",
    "      AND p1.page_title IN {mlwiki_titles_normalized}\n",
    "\"\"\".format(**wd_vars), 'mlwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dbcolumn to each query df\n",
    "pa_id_r['database_code'] = 'pawiki' \n",
    "ml_id_r['database_code'] = 'mlwiki'\n",
    "hi_id_r['database_code'] = 'hiwiki'\n",
    "or_id_r['database_code'] = 'orwiki'\n",
    "ur_id_r['database_code'] = 'urwiki'\n",
    "ta_id_r['database_code'] = 'tawiki'\n",
    "kn_id_r['database_code'] = 'knwiki'\n",
    "mr_id_r['database_code'] = 'mrwiki'\n",
    "gu_id_r['database_code'] = 'guwiki'\n",
    "te_id_r['database_code'] = 'tewiki'\n",
    "bn_id_r['database_code'] = 'bnwiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_ids = pd.concat([pa_id_r, \n",
    "                      ml_id_r,\n",
    "                      hi_id_r,\n",
    "                      or_id_r,\n",
    "                      ur_id_r,\n",
    "                      ta_id_r,\n",
    "                      kn_id_r,\n",
    "                      mr_id_r,\n",
    "                      gu_id_r,\n",
    "                      te_id_r,\n",
    "                      bn_id_r,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_ids.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_code</th>\n",
       "      <th>is_double_redirect</th>\n",
       "      <th>p1_is_redirect</th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_len</th>\n",
       "      <th>page_title</th>\n",
       "      <th>rpage_id</th>\n",
       "      <th>rpage_len</th>\n",
       "      <th>rpage_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database_code, is_double_redirect, p1_is_redirect, page_id, page_len, page_title, rpage_id, rpage_len, rpage_title]\n",
       "Index: []"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do not want any duplicates here\n",
    "nppt_ids[nppt_ids.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((nppt_ids['p1_is_redirect']==1) & (nppt_ids['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((nppt_ids['p1_is_redirect']==1) | (nppt_ids['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results from nppt_ids\n",
    "#create a df \n",
    "all_surviving_articles = nppt_ids[['page_id','page_title', 'page_len', 'database_code']] \n",
    "#seperate the redirected items into their own df\n",
    "redirects = nppt_ids.loc[nppt_ids['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title','page_len', 'database_code']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "nppt_articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "#create a new wikicode column using quality_vars['wiki_db']\n",
    "#ffill could also work here\n",
    "#articles['wikicode'] = quality_vars['wiki_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19316 entries, 0 to 19528\n",
      "Data columns (total 4 columns):\n",
      "page_id          19316 non-null float64\n",
      "page_title       19316 non-null object\n",
      "page_len         19316 non-null float64\n",
      "database_code    19316 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 754.5+ KB\n"
     ]
    }
   ],
   "source": [
    "nppt_articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing articles - edit date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pawiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'pawiki', 'page_title']))\n",
    "mlwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'mlwiki', 'page_title']))\n",
    "hiwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'hiwiki', 'page_title']))\n",
    "orwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'orwiki', 'page_title']))\n",
    "urwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'urwiki', 'page_title']))\n",
    "tawiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'tawiki', 'page_title']))\n",
    "knwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'knwiki', 'page_title']))\n",
    "mrwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'mrwiki', 'page_title']))\n",
    "guwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'guwiki', 'page_title']))\n",
    "tewiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'tewiki', 'page_title']))\n",
    "bnwiki_titles_norm_ed = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'bnwiki', 'page_title']))\n",
    "\n",
    "#update the query variable to use it in queries\n",
    "wd_vars.update({'pawiki_titles_norm_ed': pawiki_titles_norm_ed,\n",
    "                     'mlwiki_titles_norm_ed': mlwiki_titles_norm_ed,\n",
    "                     'hiwiki_titles_norm_ed': hiwiki_titles_norm_ed,\n",
    "                     'orwiki_titles_norm_ed': orwiki_titles_norm_ed,\n",
    "                     'urwiki_titles_norm_ed': urwiki_titles_norm_ed,\n",
    "                     'tawiki_titles_norm_ed': tawiki_titles_norm_ed,\n",
    "                     'knwiki_titles_norm_ed': knwiki_titles_norm_ed,\n",
    "                     'mrwiki_titles_norm_ed': mrwiki_titles_norm_ed,\n",
    "                     'guwiki_titles_norm_ed': guwiki_titles_norm_ed,\n",
    "                     'tewiki_titles_norm_ed': tewiki_titles_norm_ed,\n",
    "                     'bnwiki_titles_norm_ed': bnwiki_titles_norm_ed,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Manual:Revision_table#rev_timestamp\n",
    "#https://www.mediawiki.org/wiki/Manual:Timestamp\n",
    "\n",
    "#filter for those edited during the contest - 10th oct 2019 & 11th jan 2020 ---> 20191010000000\n",
    "#yyyymmddhhmmss --  August 9th, 2010 00:30:06 --- 20100809003006\n",
    "\n",
    "ta_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {tawiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'tawiki')\n",
    "\n",
    "ml_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {mlwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'mlwiki')\n",
    "\n",
    "hi_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {hiwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'hiwiki')\n",
    "\n",
    "or_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {orwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'orwiki')\n",
    "\n",
    "ur_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {urwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'urwiki')\n",
    "\n",
    "kn_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {knwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'knwiki')\n",
    "\n",
    "mr_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {mrwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'mrwiki')\n",
    "\n",
    "gu_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {guwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'guwiki')\n",
    "\n",
    "te_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {tewiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'tewiki')\n",
    "\n",
    "bn_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {bnwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'bnwiki')\n",
    "\n",
    "pa_edits_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {pawiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'pawiki')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add dbcolumn to each query df\n",
    "pa_edits_r['database_code'] = 'pawiki' \n",
    "ml_edits_r['database_code'] = 'mlwiki'\n",
    "hi_edits_r['database_code'] = 'hiwiki'\n",
    "or_edits_r['database_code'] = 'orwiki'\n",
    "ur_edits_r['database_code'] = 'urwiki'\n",
    "ta_edits_r['database_code'] = 'tawiki'\n",
    "kn_edits_r['database_code'] = 'knwiki'\n",
    "mr_edits_r['database_code'] = 'mrwiki'\n",
    "gu_edits_r['database_code'] = 'guwiki'\n",
    "te_edits_r['database_code'] = 'tewiki'\n",
    "bn_edits_r['database_code'] = 'bnwiki'\n",
    "\n",
    "nppt_articles_edits = pd.concat([pa_edits_r, \n",
    "                      ml_edits_r,\n",
    "                      hi_edits_r,\n",
    "                      or_edits_r,\n",
    "                      ur_edits_r,\n",
    "                      ta_edits_r,\n",
    "                      kn_edits_r,\n",
    "                      mr_edits_r,\n",
    "                      gu_edits_r,\n",
    "                      te_edits_r,\n",
    "                      bn_edits_r,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_articles_edits.reset_index(drop=True);\n",
    "\n",
    "nppt_articles_edits['edit_date'] = pd.to_datetime(nppt_articles_edits['edit_date'], format=\"%y-%m-%d\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_code</th>\n",
       "      <th>edit_date</th>\n",
       "      <th>page_id</th>\n",
       "      <th>revactor_actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database_code, edit_date, page_id, revactor_actor]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nppt_articles_edits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > How many stump articles from the Google list were edited during the GLOW contest? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0 articles from the Google provided 'editing' list of articles have been edited since the contest started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "GROUP BY revactor_rev\n",
    "LIMIT 10\n",
    "\"\"\", 'pawiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      "page_id           10 non-null int64\n",
      "edit_date         10 non-null object\n",
      "revactor_actor    10 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_table#rev_timestamp\n",
    "#https://www.mediawiki.org/wiki/Manual:Timestamp\n",
    "\n",
    "#filter for those edited during the contest - 10th oct 2019 & 11th jan 2020 ---> 20191010000000\n",
    "#yyyymmddhhmmss --  August 9th, 2010 00:30:06 --- 20100809003006\n",
    "\n",
    "ta_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {tawiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'tawiki')\n",
    "\n",
    "ml_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {mlwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'mlwiki')\n",
    "\n",
    "hi_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {hiwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'hiwiki')\n",
    "\n",
    "or_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {orwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'orwiki')\n",
    "\n",
    "ur_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {urwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'urwiki')\n",
    "\n",
    "kn_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {knwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'knwiki')\n",
    "\n",
    "mr_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {mrwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'mrwiki')\n",
    "\n",
    "gu_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {guwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'guwiki')\n",
    "\n",
    "te_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {tewiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'tewiki')\n",
    "\n",
    "bn_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {bnwiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'bnwiki')\n",
    "\n",
    "pa_edits_test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 0 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND rev_page IN {pawiki_titles_norm_ed}\n",
    "GROUP BY revactor_rev\n",
    "\"\"\".format(**wd_vars), 'pawiki')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add dbcolumn to each query df\n",
    "pa_edits_test['database_code'] = 'pawiki' \n",
    "ml_edits_test['database_code'] = 'mlwiki'\n",
    "hi_edits_test['database_code'] = 'hiwiki'\n",
    "or_edits_test['database_code'] = 'orwiki'\n",
    "ur_edits_test['database_code'] = 'urwiki'\n",
    "ta_edits_test['database_code'] = 'tawiki'\n",
    "kn_edits_test['database_code'] = 'knwiki'\n",
    "mr_edits_test['database_code'] = 'mrwiki'\n",
    "gu_edits_test['database_code'] = 'guwiki'\n",
    "te_edits_test['database_code'] = 'tewiki'\n",
    "bn_edits_test['database_code'] = 'bnwiki'\n",
    "\n",
    "nppt_articles_edits_test = pd.concat([pa_edits_test, \n",
    "                      ml_edits_test,\n",
    "                      hi_edits_test,\n",
    "                      or_edits_test,\n",
    "                      ur_edits_test,\n",
    "                      ta_edits_test,\n",
    "                      kn_edits_test,\n",
    "                      mr_edits_test,\n",
    "                      gu_edits_test,\n",
    "                      te_edits_test,\n",
    "                      bn_edits_test,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_articles_edits_test.reset_index(drop=True);\n",
    "\n",
    "nppt_articles_edits_test['edit_date'] = pd.to_datetime(nppt_articles_edits['edit_date'], format=\"%y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 4 columns):\n",
      "database_code     300 non-null object\n",
      "edit_date         0 non-null datetime64[ns]\n",
      "page_id           300 non-null object\n",
      "revactor_actor    300 non-null object\n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#the above returns a df with a few hundred results, none from the glow tiger 2.0 contest period, all with null date fields\n",
    "nppt_articles_edits_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QIDs & Sitelinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikidata Q item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Wikibase/Schema/wb_items_per_site\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#wb_items_per_site site:quarry.wmflabs.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY CLEAN EDITING SUBLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "#nppt_articles['article_suggestion'] = nppt_articles['article_suggestion'].str.replace('_', ' ')\n",
    "nppt_articles['page_title'] = nppt_articles['page_title'].str.replace('_', ' ')\n",
    "#create tuples of the article_suggestions and wiki_codes to use when querying for the wikidata items\n",
    "editing_titles_denormalized_CLEAN = tuple(list(editing_topic_rec_df_CLEAN['page_title']))\n",
    "editing_titles_denormalized_database_codes_CLEAN = tuple(list(editing_topic_rec_df_CLEAN['database_code']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'editing_titles_denormalized' : editing_titles_denormalized_CLEAN,\n",
    "    'editing_titles_denormalized_db_codes' : editing_titles_denormalized_database_codes_CLEAN,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_r2_editing_CLEAN = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS page_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {editing_titles_denormalized_db_codes} AND\n",
    "      ips_site_page IN {editing_titles_denormalized}\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9038 entries, 0 to 9037\n",
      "Data columns (total 3 columns):\n",
      "page_title       9038 non-null object\n",
      "QID              9038 non-null int64\n",
      "database_code    9038 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 212.0+ KB\n"
     ]
    }
   ],
   "source": [
    "qid_r2_editing_CLEAN.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in en query results to nppt_articles\n",
    "#editing_topic_rec_df_CLEAN_ids_q = editing_topic_rec_df_CLEAN_ids.merge(qid_r2_editing_CLEAN, how=\"left\", on=['page_title', 'database_code'])\n",
    "nppt_articles_q = qid_r2_editing_CLEAN.merge(nppt_articles, how=\"left\", on=['page_title', 'database_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9038 entries, 0 to 9037\n",
      "Data columns (total 5 columns):\n",
      "page_title       9038 non-null object\n",
      "QID              9038 non-null int64\n",
      "database_code    9038 non-null object\n",
      "page_id          8530 non-null float64\n",
      "page_len         8530 non-null float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 423.7+ KB\n"
     ]
    }
   ],
   "source": [
    "nppt_articles_q.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY TRANSLATION SUBLIST FOR QITEMS ON ENWIKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_topic_rec_df_CLEAN['article_suggestion'] = translation_topic_rec_df_CLEAN['article_suggestion'].str.replace('_', ' ')\n",
    "titles_denormalized_translation_CLEAN = tuple(list(translation_topic_rec_df_CLEAN['article_suggestion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get qids for translation articles\n",
    "qid_r_en_CLEAN = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS article_suggestion,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = 'enwiki' \n",
    "  AND ips_site_page IN {titles_denormalized_translation_CLEAN}\n",
    "\"\"\".format(titles_denormalized_translation_CLEAN=titles_denormalized_translation_CLEAN), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7647 entries, 0 to 7646\n",
      "Data columns (total 2 columns):\n",
      "article_suggestion    7647 non-null object\n",
      "QID                   7647 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 119.6+ KB\n"
     ]
    }
   ],
   "source": [
    "qid_r_en_CLEAN.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY translation rec qids for sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_qids = tuple(list(qid_r_en_CLEAN['QID']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'translation_qids' : translation_qids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "iwl_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  linked_item.ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM (\n",
    "      SELECT ips_item_id\n",
    "      FROM wb_items_per_site\n",
    "      WHERE ips_item_id IN {translation_qids}\n",
    "      AND ips_site_id IN {wikis}\n",
    "    ) AS linked_item\n",
    "LEFT JOIN wb_items_per_site \n",
    "  ON linked_item.ips_item_id = wb_items_per_site.ips_item_id\n",
    "LEFT JOIN page \n",
    "  ON linked_item.ips_item_id = page.page_id\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4025 entries, 0 to 4024\n",
      "Data columns (total 3 columns):\n",
      "QID            4025 non-null int64\n",
      "iwsites        4025 non-null object\n",
      "iwsitelinks    4025 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 94.5+ KB\n"
     ]
    }
   ],
   "source": [
    "iwl_r.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iwl_q = iwl_r.merge(qid_r_en_CLEAN, how=\"left\", on=['QID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4025 entries, 0 to 4024\n",
      "Data columns (total 4 columns):\n",
      "QID                   4025 non-null int64\n",
      "iwsites               4025 non-null object\n",
      "iwsitelinks           4025 non-null int64\n",
      "article_suggestion    4025 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 157.2+ KB\n"
     ]
    }
   ],
   "source": [
    "t_iwl_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 14155 entries, 8111 to 34294\n",
      "Data columns (total 8 columns):\n",
      "article_suggestion     14155 non-null object\n",
      "local_encoded_title    0 non-null object\n",
      "g_category             14155 non-null object\n",
      "language_name          14155 non-null object\n",
      "suggestion_type        14155 non-null object\n",
      "file                   14155 non-null object\n",
      "language_code          14155 non-null object\n",
      "database_code          14155 non-null object\n",
      "dtypes: object(8)\n",
      "memory usage: 995.3+ KB\n"
     ]
    }
   ],
   "source": [
    "translation_topic_rec_df_CLEAN.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rec_iwl_q = t_iwl_q.merge(translation_topic_rec_df_CLEAN, how=\"left\", on=['article_suggestion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8462 entries, 0 to 8461\n",
      "Data columns (total 11 columns):\n",
      "QID                    8462 non-null int64\n",
      "iwsites                8462 non-null object\n",
      "iwsitelinks            8462 non-null int64\n",
      "article_suggestion     8462 non-null object\n",
      "local_encoded_title    0 non-null object\n",
      "g_category             8462 non-null object\n",
      "language_name          8462 non-null object\n",
      "suggestion_type        8462 non-null object\n",
      "file                   8462 non-null object\n",
      "language_code          8462 non-null object\n",
      "database_code          8462 non-null object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 793.3+ KB\n"
     ]
    }
   ],
   "source": [
    "t_rec_iwl_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>iwsites</th>\n",
       "      <th>iwsitelinks</th>\n",
       "      <th>article_suggestion</th>\n",
       "      <th>local_encoded_title</th>\n",
       "      <th>g_category</th>\n",
       "      <th>language_name</th>\n",
       "      <th>suggestion_type</th>\n",
       "      <th>file</th>\n",
       "      <th>language_code</th>\n",
       "      <th>database_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [QID, iwsites, iwsitelinks, article_suggestion, local_encoded_title, g_category, language_name, suggestion_type, file, language_code, database_code]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_rec_iwl_q[t_rec_iwl_q.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupe_check = t_rec_iwl_q[t_rec_iwl_q.duplicated(['article_suggestion'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4437 entries, 3 to 8460\n",
      "Data columns (total 11 columns):\n",
      "QID                    4437 non-null int64\n",
      "iwsites                4437 non-null object\n",
      "iwsitelinks            4437 non-null int64\n",
      "article_suggestion     4437 non-null object\n",
      "local_encoded_title    0 non-null object\n",
      "g_category             4437 non-null object\n",
      "language_name          4437 non-null object\n",
      "suggestion_type        4437 non-null object\n",
      "file                   4437 non-null object\n",
      "language_code          4437 non-null object\n",
      "database_code          4437 non-null object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 416.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#the article is sometimes suggested as an article suggestion for more than one wiki\n",
    "dupe_check.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_rec_iwl_q = t_rec_iwl_q[['article_suggestion','QID','database_code', 'iwsites','iwsitelinks','language_code','file','suggestion_type','language_name','g_category',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8462 entries, 0 to 8461\n",
      "Data columns (total 10 columns):\n",
      "article_suggestion    8462 non-null object\n",
      "QID                   8462 non-null int64\n",
      "database_code         8462 non-null object\n",
      "iwsites               8462 non-null object\n",
      "iwsitelinks           8462 non-null int64\n",
      "language_code         8462 non-null object\n",
      "file                  8462 non-null object\n",
      "suggestion_type       8462 non-null object\n",
      "language_name         8462 non-null object\n",
      "g_category            8462 non-null object\n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 727.2+ KB\n"
     ]
    }
   ],
   "source": [
    "translation_rec_iwl_q.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create boolean column WHERE 'iwsites' column str contains 'database_code' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_rec_iwl_q['database_code_in_iwsites'] = [x[0] in x[1] if x[0] is not None else False for x in zip(translation_rec_iwl_q['database_code'], translation_rec_iwl_q['iwsites'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1873"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_rec_iwl_q['database_code_in_iwsites'].values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_created_a = translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation articles - Qid match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get a df of articles that DON'T have an interwiki link associated with the \n",
    "#suggestion database_code, aka, they weren't created as far as we know so far\n",
    "\n",
    "located =  translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == True]\n",
    "not_yet_located = translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == False]\n",
    "\n",
    "#located_list = list(translation_rec_iwl_q.loc[translation_rec_iwl_q['database_code_in_iwsites']== True, ['QID']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = translation_topic_rec_df_CLEAN.merge(qid_r_en_CLEAN, how=\"left\", on=['article_suggestion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_sugg_to_cull = x.loc[x['QID'].notnull(), ['database_code', 'article_suggestion', 'QID']].reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "located_to_drop =located[['article_suggestion','database_code','QID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_sugg_not_found_interim = pd.merge(translation_sugg_to_cull,located_to_drop, indicator=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_sugg_not_yet_found = translation_sugg_not_found_interim.loc[translation_sugg_not_found_interim['_merge']=='left_only', ['database_code', 'article_suggestion', 'QID']].reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11876 entries, 7 to 13750\n",
      "Data columns (total 3 columns):\n",
      "database_code         11876 non-null object\n",
      "article_suggestion    11876 non-null object\n",
      "QID                   11876 non-null float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 371.1+ KB\n"
     ]
    }
   ],
   "source": [
    "#fyi, titles are denormalized\n",
    "translation_sugg_not_yet_found.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fountain_titles = pd.read_csv(\"../../data/raw/articles/2019/contest_titles_n_updated.csv\", sep=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fountain_titles_to_cull = fountain_titles[['wiki_db', 'QID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "fountain_titles_to_cull = fountain_titles_to_cull.rename(columns={'wiki_db': 'database_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_sugg = translation_sugg_not_yet_found[['database_code','QID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12712 entries, 0 to 12711\n",
      "Data columns (total 2 columns):\n",
      "database_code    12712 non-null object\n",
      "QID              12712 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 198.8+ KB\n"
     ]
    }
   ],
   "source": [
    "fountain_titles_to_cull.info(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11876 entries, 7 to 13750\n",
      "Data columns (total 2 columns):\n",
      "database_code    11876 non-null object\n",
      "QID              11876 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 278.3+ KB\n"
     ]
    }
   ],
   "source": [
    "translation_sugg.info(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_created = pd.merge(fountain_titles_to_cull, translation_sugg, on=['database_code', 'QID'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suggestions_created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > How many translation articles from the Google list were created since the GLOW contest? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 2006+ articles were picked from the suggestion lists (out of 34k+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1873+ articles from the Google provided 'translation' list of articles were created, since the contest started, and had the related iwl added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1873"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_rec_iwl_q['database_code_in_iwsites'].values.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 133 articles from the Google provided 'translation' list of articles were created, since the contest started, and had a matching QID added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 133 entries, 0 to 132\n",
      "Data columns (total 2 columns):\n",
      "database_code    133 non-null object\n",
      "QID              133 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 3.1+ KB\n"
     ]
    }
   ],
   "source": [
    "suggestions_created.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes the 1873 with related interwiki links\n",
    "suggestions_created_a.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_created_a.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes 133\n",
    "suggestions_created.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_created_b.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do in the future, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_recs = qid_r2_editing_CLEAN[['page_title', 'QID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion_qids = pd.concat([qid_r_en_CLEAN, editing_recs], sort=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestion_qids.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_qids.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
