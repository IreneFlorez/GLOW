{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/chromedriver-binary/\n",
    "#https://sites.google.com/a/chromium.org/chromedriver/getting-started/chromeos\n",
    "#tools: https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en\n",
    "#https://chromedriver.chromium.org/\n",
    "import requests\n",
    "import string\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "import time\n",
    "import re\n",
    "import os\n",
    " \n",
    "from selenium import webdriver\n",
    "import chromedriver_binary  # Adds chromedriver binary to path\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--headless')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get links to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider https://meta.wikimedia.org/wiki/List_of_Wikipedias/Table to be the ultimate source of truth on wiki counts\n",
    "\n",
    "#use the api https://www.mediawiki.org/wiki/API:Revisions\n",
    "#to get revid's for pages related to https://meta.wikimedia.org/w/index.php?title=List_of_Wikipedias/Table&action=history\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://meta.wikimedia.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"prop\": \"revisions\",\n",
    "    \"titles\": \"List of Wikipedias/Table\",\n",
    "    \"rvprop\": \"timestamp|ids\",\n",
    "    \"rvdir\": \"newer\",\n",
    "    \"rvstart\": \"2016-01-01T00:00:00.000Z\",\n",
    "    \"formatversion\": \"2\",\n",
    "    \"format\": \"json\",\n",
    "    \"rvlimit\": \"500\",\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()\n",
    "\n",
    "for d in DATA['query']['pages']:\n",
    "    for k, v, in d.items():\n",
    "        inner = k, v\n",
    "        res = inner[1]\n",
    "\n",
    "rev_id_list = [i['revid'] for i in res]\n",
    "rev_df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the above code for all dates from 2016-2019 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019_Jun_Dec = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_Oct_2019_June = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018_Feb_2018_Nov = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017_Jun_2018_Feb = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_Nov_2017_May = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_Jan_Sept = rev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile a master spreadsheet\n",
    "master_revs_df = df_2016_Jan_Sept.append([df_2016_Nov_2017_May,df_2017_Jun_2018_Feb,df_2018_Feb_2018_Nov,df_2018_Oct_2019_June,df_2019_Jun_Dec])\n",
    "#format the timestamp column to datetime\n",
    "master_revs_df['timestamp'] = pd.to_datetime(master_revs_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the df just in case\n",
    "df = master_revs_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab only one revid per month\n",
    "df['year'] = df.timestamp.map(lambda x: x.year)\n",
    "df['month'] = df.timestamp.map(lambda x: x.month)\n",
    "df['day'] = df.timestamp.map(lambda x: x.day)\n",
    "select_revs = df.sort_values(['year', 'month']).groupby(['year', 'month']).first()\n",
    "select_revs.reset_index();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the two main files to csv format\n",
    "select_revs.to_csv('rev_ids_2016_2019_select') #only the first rev id in each month\n",
    "df.to_csv('rev_ids_2016_2019_all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list out of rev ids, just in case that's helpful\n",
    "rev_ids_to_query = select_revs['revid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add prefix to rev id column\n",
    "df2= select_revs.copy()\n",
    "prefix = 'https://meta.wikimedia.org/w/index.php?title=List_of_Wikipedias/Table&direction=prev&oldid='\n",
    "df2['revid'] = prefix + df2['revid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list our of urls\n",
    "urls_to_query = df2['revid'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Count per wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape article counts by language\n",
    "#pull language name, date of revision, article count on that date of revision\n",
    "language_list = ['Urdu', 'Hindi','Kannada', 'Malayalam', 'Marathi', 'Odia', 'Punjabi', 'Western Punjabi', 'Santali', 'Sanskrit', 'Tamil', 'Tulu', 'Telugu', 'Assamese', 'Bangla', 'Gujarati']\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "ac = []\n",
    "\n",
    "def getRows():\n",
    "        for name in language_list:\n",
    "            #get year, label, wiki_name, art_count\n",
    "            print(URL)\n",
    "            ac_xpath = \"//a[text()='\" + str(name) + \"']/parent::td//following-sibling::td[3]/a\"\n",
    "            wiki_xpath = \"//a[text()='\" + str(name) + \"']/parent::td\" \n",
    "            date_xpath =\"//*[@id='mw-revision-date']\"\n",
    "            \n",
    "            ac_dict = {}\n",
    "            try:\n",
    "                get_date = driver.find_element_by_xpath(date_xpath).text\n",
    "                get_wiki_name = driver.find_elements_by_xpath (wiki_xpath)\n",
    "                get_article_count = driver.find_elements_by_xpath (ac_xpath)\n",
    "            \n",
    "            \n",
    "                ac_dict['date'] = date = get_date\n",
    "                ac_dict['lang'] = \"\".join([element.text for element in get_wiki_name]) \n",
    "                ac_dict['count'] = \"\".join([element.text for element in get_article_count]) \n",
    "            \n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "           \n",
    "            #print(ac_dict)\n",
    "            t_ac.append(ac_dict.copy())\n",
    "\n",
    "for URL in urls_to_query:\n",
    "    #driver = webdriver.Chrome()\n",
    "    driver.get(URL)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    t_ac = []\n",
    "    getRows()\n",
    "    ac.append(t_ac.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results = pd.DataFrame(ac).T\n",
    "raw_results.columns=raw_results.columns//1\n",
    "lang_count_results = pd.concat([pd.DataFrame(x.values) for _,x in raw_results.groupby(level=0,axis=1)]).dropna(axis=0,thresh=1)\n",
    "wiki_art_count_results = lang_count_results[0].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_art_count_results.to_csv('wiki_counts_India_GLOW') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
