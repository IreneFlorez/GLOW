{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guiding Question:\n",
    "Q: how many articles were created from the suggestions? \n",
    "\n",
    "Process:\n",
    "\n",
    "Prep:\n",
    "1. Combine and clean suggestion lists\n",
    "2. groupby type: editing suggestions\n",
    "\n",
    "    a. get pageids (not including redirects) \n",
    "    \n",
    "    b. get wikidata items using the local language (for use with topics, if time permits)\n",
    "    \n",
    "3. groupby type: translation suggestions\n",
    "\n",
    "    a. get wikidata items using enwiki \n",
    "    \n",
    "    b. get iwlinks\n",
    "    \n",
    "4. Read and clean the list of articles created and submitted to GLOW (includes articles ultimately disapproved)\n",
    "\n",
    "\n",
    "Analysis:\n",
    "5. groupby type: editing suggestions\n",
    "\n",
    "    a. count matches by ids - count ids that were edited during the contest period\n",
    "    \n",
    "6. groupby type: translation suggestions\n",
    "\n",
    "    a. count matches by wikidata item in the suggested language\n",
    "    \n",
    "    b. count matches by iwlink in the suggested language\n",
    "    \n",
    "    c. get a sum of items a+b\n",
    "\n",
    "NOTES:\n",
    "1. Analysis notes\n",
    "    a. If the editor uses Content Translation, it should automatically assign the right QID\n",
    "    b. If the editor doesn't use CT, either they or someone else has to assign the QID\n",
    "    c. We will miss articles that were not created via Content Translation and don't have a manually added QID and/or the editor changed the suggested title to something new. \n",
    "4. Suggestion notes\n",
    "    a. From the creators of the suggestions: \"As a reminder, we have 2 lists: a list of suggested topics that exist in the local language but could be edited to be more complete based on the corresponding English page, and a list of topics that can be translated from English to the local language.  Based on feedback from the initial Project Tiger, we've separated out the topics by categories so editors can focus on the areas they like to write about.  The lists are ordered by popularity of what local language users are looking for.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import urllib\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile all of the lists\n",
    "\n",
    "f_mask = r'../../../GLOW/data/raw/g_topic_lists/*.xlsx'\n",
    "\n",
    "gtl = \\\n",
    "pd.concat([gtl.assign(file=os.path.splitext(os.path.basename(f))[0],\n",
    "                     sheet=sheet)\n",
    "           for f in glob(f_mask)\n",
    "           for sheet, gtl in pd.read_excel(f, sheet_name=None).items()],\n",
    "          ignore_index=True, sort=True)\n",
    "\n",
    "full_topic_rec_df = gtl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine Topic & entity name > article_suggestion\n",
    "full_topic_rec_df['article_suggestion'] = full_topic_rec_df['entity_name'].combine_first(full_topic_rec_df['Topic'])\n",
    "#rename 'sheet' to Google_topic\n",
    "full_topic_rec_df = full_topic_rec_df.rename(columns={'sheet':'g_category', \n",
    "                                                      'Topic': 'g_suggested_en_title', \n",
    "                                                      'entity_name': 'g_suggested_local_title',\n",
    "                                                      'english_wikipedia':'english_wikipedia_URL', \n",
    "                                                      'local_wikipedia':'local_wikipedia_URL'\n",
    "                                                     })\n",
    "\n",
    "#extract wiki name & suggestion_type (translation or edit)\n",
    "#full_topic_rec_df['wiki'] = full_topic_rec_df['file'].str.extract('(^[A-Z_]+([^\\(-]+))', expand=True)\n",
    "full_topic_rec_df[['language_name', 'suggestion_type']] = full_topic_rec_df['file'].str.split(\" \", 1, expand=True)\n",
    "full_topic_rec_df['suggestion_type'] = full_topic_rec_df['file'].str.rsplit(\"for \").str[-1]\n",
    "\n",
    "#extract url title\n",
    "full_topic_rec_df['local_encoded_title'] = full_topic_rec_df['local_wikipedia_URL'].str.extract('([^\\/]+$)', expand=True)\n",
    "\n",
    "#extract lang code\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['local_wikipedia_URL'].str.rsplit(\"http://\").str[-1]\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['url_language_code'].str.extract('([^.]+)', expand=True)\n",
    "\n",
    "#reorder for visual skimming's sake\n",
    "full_topic_rec_df = full_topic_rec_df[['article_suggestion', 'local_encoded_title','g_category', 'language_name','suggestion_type', 'file']] #'url_language_code'\n",
    "\n",
    "#replace double coded translation entries\n",
    "full_topic_rec_df['suggestion_type']=full_topic_rec_df['suggestion_type'].replace('Translating EXTERNAL', 'Translation EXTERNAL')\n",
    "full_topic_rec_df['language_name'] = full_topic_rec_df['language_name'].replace('Bengali', 'Bangla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_suggestion</th>\n",
       "      <th>local_encoded_title</th>\n",
       "      <th>g_category</th>\n",
       "      <th>language_name</th>\n",
       "      <th>suggestion_type</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_suggestion, local_encoded_title, g_category, language_name, suggestion_type, file]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_topic_rec_df[full_topic_rec_df['language_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get database_code and language_code, confirm language_code (if needed)\n",
    "lang_names =tuple(full_topic_rec_df['language_name'].unique())\n",
    "\n",
    "ci = wmf.hive.run(\"\"\"\n",
    "SELECT  language_code, database_code, language_name\n",
    "FROM canonical_data.wikis\n",
    "WHERE language_name IN {lang_names} AND database_group = 'wikipedia'\n",
    "\"\"\".format(lang_names=lang_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "full_topic_rec_df_ci = full_topic_rec_df.merge(ci, how=\"left\", on=['language_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "full_topic_rec_df_ci['article_suggestion'] = full_topic_rec_df_ci['article_suggestion'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pnb article_suggestions exist, confirm language_code = url_language_code(see code above to add in url_language_code column)\n",
    "#pnb\tpnbwiki\tWestern Punjabi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "translation_topic_rec_df = full_topic_rec_df_ci[full_topic_rec_df_ci['suggestion_type'] == 'Translation EXTERNAL'].copy(deep=False)\n",
    "\n",
    "#get clean list - drop duplicates\n",
    "translation_topic_rec_df_CLEAN = translation_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'local_encoded_title','g_category','suggestion_type', 'language_name', 'language_code', 'database_code', 'file'], keep='first').copy(deep=False)\n",
    "\n",
    "#keep just the duplicates - for checking data later on\n",
    "translation_topic_rec_df_Dupes = pd.concat([translation_topic_rec_df, translation_topic_rec_df_CLEAN]).loc[translation_topic_rec_df.index.symmetric_difference(translation_topic_rec_df_CLEAN.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "editing_topic_rec_df = full_topic_rec_df_ci.loc[full_topic_rec_df_ci['suggestion_type'] == 'Editing EXTERNAL'].copy(deep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded URL to decoded title\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['local_encoded_title'].apply(lambda x: unquote(x)).copy(deep=False)\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['page_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#encoded URL to decoded title\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['local_encoded_title'].apply(lambda x: unquote(x))\n",
    "editing_topic_rec_df['page_title'] = editing_topic_rec_df['page_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clean list - drop duplicates\n",
    "editing_topic_rec_df_CLEAN = editing_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'page_title','local_encoded_title','g_category', 'suggestion_type', 'language_name', 'language_code', 'database_code', 'file'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_vars = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get article ids, redirects <a class=\"anchor\" id=\"get_clean_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnpt = editing_topic_rec_df_CLEAN.loc[editing_topic_rec_df_CLEAN['page_title'].notnull(), ['database_code', 'page_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mlwiki', 'pawiki', 'hiwiki', 'orwiki', 'urwiki', 'tawiki',\n",
       "       'knwiki', 'mrwiki', 'guwiki', 'tewiki', 'bnwiki'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnpt['database_code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = tuple(list(nnpt['database_code'].unique()))\n",
    "\n",
    "wd_vars.update({'wikis': wikis})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pawiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'pawiki', 'page_title']))\n",
    "mlwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'mlwiki', 'page_title']))\n",
    "hiwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'hiwiki', 'page_title']))\n",
    "orwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'orwiki', 'page_title']))\n",
    "urwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'urwiki', 'page_title']))\n",
    "tawiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'tawiki', 'page_title']))\n",
    "knwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'knwiki', 'page_title']))\n",
    "mrwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'mrwiki', 'page_title']))\n",
    "guwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'guwiki', 'page_title']))\n",
    "tewiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'tewiki', 'page_title']))\n",
    "bnwiki_titles_normalized = tuple(list(nnpt.loc[nnpt['database_code'] == 'bnwiki', 'page_title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the query variable to use it in queries\n",
    "wd_vars.update({'pawiki_titles_normalized': pawiki_titles_normalized,\n",
    "                'mlwiki_titles_normalized': mlwiki_titles_normalized,\n",
    "                'hiwiki_titles_normalized': hiwiki_titles_normalized,\n",
    "                'orwiki_titles_normalized': orwiki_titles_normalized,\n",
    "                'urwiki_titles_normalized': urwiki_titles_normalized,\n",
    "                'tawiki_titles_normalized': tawiki_titles_normalized,\n",
    "                'knwiki_titles_normalized': knwiki_titles_normalized,\n",
    "                'mrwiki_titles_normalized': mrwiki_titles_normalized,\n",
    "                'guwiki_titles_normalized': guwiki_titles_normalized,\n",
    "                'tewiki_titles_normalized': tewiki_titles_normalized,\n",
    "                'bnwiki_titles_normalized': bnwiki_titles_normalized,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ids_query = \"\"\"\n",
    "    SELECT \n",
    "           DATABASE() AS database_code,\n",
    "           p1.page_id  AS page_id,\n",
    "           p1.page_title AS page_title,\n",
    "           p1.page_is_redirect AS p1_is_redirect,\n",
    "           p2.page_id AS rpage_id,\n",
    "           p2.page_title AS rpage_title,\n",
    "           p2.page_len rpage_len,\n",
    "           p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "          AND p1.page_title IN {titles_normalized}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = pawiki_titles_normalized), 'pawiki')\n",
    "ml_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = mlwiki_titles_normalized), 'mlwiki')\n",
    "hi_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = hiwiki_titles_normalized), 'hiwiki')\n",
    "or_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = orwiki_titles_normalized), 'orwiki')\n",
    "ur_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = urwiki_titles_normalized), 'urwiki')\n",
    "ta_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = tawiki_titles_normalized), 'tawiki')\n",
    "kn_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = knwiki_titles_normalized), 'knwiki')\n",
    "mr_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = mrwiki_titles_normalized), 'mrwiki')\n",
    "gu_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = guwiki_titles_normalized), 'guwiki')\n",
    "te_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = tewiki_titles_normalized), 'tewiki')\n",
    "bn_ids_r = wmf.mariadb.run(get_ids_query.format(titles_normalized = bnwiki_titles_normalized), 'bnwiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_ids = pd.concat([pa_ids_r, \n",
    "                      ml_ids_r,\n",
    "                      hi_ids_r,\n",
    "                      or_ids_r,\n",
    "                      ur_ids_r,\n",
    "                      ta_ids_r,\n",
    "                      kn_ids_r,\n",
    "                      mr_ids_r,\n",
    "                      gu_ids_r,\n",
    "                      te_ids_r,\n",
    "                      bn_ids_r,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_ids.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_code</th>\n",
       "      <th>is_double_redirect</th>\n",
       "      <th>p1_is_redirect</th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>rpage_id</th>\n",
       "      <th>rpage_len</th>\n",
       "      <th>rpage_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database_code, is_double_redirect, p1_is_redirect, page_id, page_title, rpage_id, rpage_len, rpage_title]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do not want any duplicates here\n",
    "nppt_ids[nppt_ids.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((nppt_ids['p1_is_redirect']==1) & (nppt_ids['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((nppt_ids['p1_is_redirect']==1) | (nppt_ids['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results from nppt_ids\n",
    "#create a df \n",
    "all_surviving_articles = nppt_ids[['page_id','page_title', 'database_code']] \n",
    "#seperate the redirected items into their own df\n",
    "redirects = nppt_ids.loc[nppt_ids['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title', 'database_code']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "nppt_articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "#create a new wikicode column using quality_vars['wiki_db']\n",
    "#ffill could also work here\n",
    "#articles['wikicode'] = quality_vars['wiki_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 19304 entries, 0 to 19526\n",
      "Data columns (total 3 columns):\n",
      "page_id          19304 non-null float64\n",
      "page_title       19304 non-null object\n",
      "database_code    19304 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 603.2+ KB\n"
     ]
    }
   ],
   "source": [
    "nppt_articles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing articles - edit date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pawiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'pawiki', 'page_id']))\n",
    "mlwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'mlwiki', 'page_id']))\n",
    "hiwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'hiwiki', 'page_id']))\n",
    "orwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'orwiki', 'page_id']))\n",
    "urwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'urwiki', 'page_id']))\n",
    "tawiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'tawiki', 'page_id']))\n",
    "knwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'knwiki', 'page_id']))\n",
    "mrwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'mrwiki', 'page_id']))\n",
    "guwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'guwiki', 'page_id']))\n",
    "tewiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'tewiki', 'page_id']))\n",
    "bnwiki_ids = tuple(list(nppt_articles.loc[nppt_articles['database_code'] == 'bnwiki', 'page_id']))\n",
    "\n",
    "#update the query variable to use it in queries\n",
    "wd_vars.update({'pawiki_ids': pawiki_ids,\n",
    "                     'mlwiki_ids': mlwiki_ids,\n",
    "                     'hiwiki_ids': hiwiki_ids,\n",
    "                     'orwiki_ids': orwiki_ids,\n",
    "                     'urwiki_ids': urwiki_ids,\n",
    "                     'tawiki_ids': tawiki_ids,\n",
    "                     'knwiki_ids': knwiki_ids,\n",
    "                     'mrwiki_ids': mrwiki_ids,\n",
    "                     'guwiki_ids': guwiki_ids,\n",
    "                     'tewiki_ids': tewiki_ids,\n",
    "                     'bnwiki_ids': bnwiki_ids,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Manual:Revision_table#rev_timestamp\n",
    "#https://www.mediawiki.org/wiki/Manual:Timestamp\n",
    "\n",
    "#filter for those edited during the contest - 10th oct 2019 & 11th jan 2020 ---> 20191010000000\n",
    "#yyyymmddhhmmss --  August 9th, 2010 00:30:06 --- 20100809003006\n",
    "\n",
    "get_edits_query = \"\"\"\n",
    "    SELECT \n",
    "        DATABASE() AS database_code,\n",
    "        page_title,\n",
    "        page_id,\n",
    "        DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date\n",
    "    FROM page\n",
    "    JOIN revision ON rev_page = page.page_id\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 20191010000000 \n",
    "        AND (rev_deleted & 4) = 0\n",
    "        AND rev_page IN {ids}\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "pa_edits_r= wmf.mariadb.run(get_edits_query.format(ids = pawiki_ids), 'pawiki')\n",
    "ml_edits_r= wmf.mariadb.run(get_edits_query.format(ids = mlwiki_ids), 'mlwiki')\n",
    "hi_edits_r= wmf.mariadb.run(get_edits_query.format(ids = hiwiki_ids), 'hiwiki')\n",
    "or_edits_r= wmf.mariadb.run(get_edits_query.format(ids = orwiki_ids), 'orwiki')\n",
    "ur_edits_r= wmf.mariadb.run(get_edits_query.format(ids = urwiki_ids), 'urwiki')\n",
    "ta_edits_r= wmf.mariadb.run(get_edits_query.format(ids = tawiki_ids), 'tawiki')\n",
    "kn_edits_r= wmf.mariadb.run(get_edits_query.format(ids = knwiki_ids), 'knwiki')\n",
    "mr_edits_r= wmf.mariadb.run(get_edits_query.format(ids = mrwiki_ids), 'mrwiki')\n",
    "gu_edits_r= wmf.mariadb.run(get_edits_query.format(ids = guwiki_ids), 'guwiki')\n",
    "te_edits_r= wmf.mariadb.run(get_edits_query.format(ids = tewiki_ids), 'tewiki')\n",
    "bn_edits_r= wmf.mariadb.run(get_edits_query.format(ids = bnwiki_ids), 'bnwiki')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_articles_edits = pd.concat([pa_edits_r, \n",
    "                      ml_edits_r,\n",
    "                      hi_edits_r,\n",
    "                      or_edits_r,\n",
    "                      ur_edits_r,\n",
    "                      ta_edits_r,\n",
    "                      kn_edits_r,\n",
    "                      mr_edits_r,\n",
    "                      gu_edits_r,\n",
    "                      te_edits_r,\n",
    "                      bn_edits_r,\n",
    "                     ], sort=True, ignore_index=True)\n",
    "\n",
    "nppt_articles_edits.reset_index(drop=True);\n",
    "\n",
    "nppt_articles_edits['edit_date'] = pd.to_datetime(nppt_articles_edits['edit_date'], format=\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7103"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nppt_articles_edits.groupby(['database_code', 'page_id']).ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database_code\n",
      "bnwiki     940\n",
      "guwiki     123\n",
      "hiwiki    2401\n",
      "knwiki     232\n",
      "mlwiki     377\n",
      "mrwiki     514\n",
      "orwiki      16\n",
      "pawiki     193\n",
      "tawiki     821\n",
      "tewiki    1311\n",
      "urwiki     175\n",
      "Name: page_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(nppt_articles_edits.groupby('database_code')['page_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_edits = nppt_articles_edits[(nppt_articles_edits['edit_date'] > '2019-10-10') & (nppt_articles_edits['edit_date'] < '2020-02-11')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contest_edits.groupby(['database_code', 'page_id']).ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "database_code\n",
      "bnwiki    859\n",
      "guwiki    104\n",
      "hiwiki    909\n",
      "knwiki    184\n",
      "mlwiki    317\n",
      "mrwiki    407\n",
      "orwiki     15\n",
      "pawiki    184\n",
      "tawiki    673\n",
      "tewiki    797\n",
      "urwiki    159\n",
      "Name: page_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(contest_edits.groupby('database_code')['page_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_articles_edits_contest = contest_edits[['page_title', 'page_id', 'database_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_articles_edits_contest_unique = nppt_articles_edits_contest.drop_duplicates(subset=['page_title', 'page_id', 'database_code'], keep='first').copy(deep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > How many articles from the Google list were edited since the start of the GLOW contest? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6,607 articles from the Google provided 'editing' list of articles have been edited since the contest started; 4609 of these were edited during the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = wmf.mariadb.run(\"\"\"\n",
    "SELECT \n",
    "    page_id,\n",
    "    DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,\n",
    "    revactor_actor\n",
    "FROM revision_actor_temp\n",
    "JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "JOIN page ON rev_page = page.page_id\n",
    "WHERE rev_page = page_id\n",
    "    AND rev_timestamp > 20191010000000 \n",
    "    AND (rev_deleted & 4) = 0\n",
    "GROUP BY revactor_rev\n",
    "LIMIT 10\n",
    "\"\"\", 'pawiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 3 columns):\n",
      "page_id           10 non-null int64\n",
      "edit_date         10 non-null object\n",
      "revactor_actor    10 non-null int64\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 368.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QIDs & Sitelinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikidata Q item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Wikibase/Schema/wb_items_per_site\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#wb_items_per_site site:quarry.wmflabs.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY CLEAN EDITING SUBLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "#nppt_articles['article_suggestion'] = nppt_articles['article_suggestion'].str.replace('_', ' ')\n",
    "nppt_articles['page_title'] = nppt_articles['page_title'].str.replace('_', ' ')\n",
    "#create tuples of the article_suggestions and wiki_codes to use when querying for the wikidata items\n",
    "editing_titles_denormalized_CLEAN = tuple(list(editing_topic_rec_df_CLEAN['page_title']))\n",
    "editing_titles_denormalized_database_codes_CLEAN = tuple(list(editing_topic_rec_df_CLEAN['database_code']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'editing_titles_denormalized' : editing_titles_denormalized_CLEAN,\n",
    "    'editing_titles_denormalized_db_codes' : editing_titles_denormalized_database_codes_CLEAN,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_editing_CLEAN_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS page_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {editing_titles_denormalized_db_codes} AND\n",
    "      ips_site_page IN {editing_titles_denormalized}\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge in en query results to nppt_articles\n",
    "#editing_topic_rec_df_CLEAN_ids_q = editing_topic_rec_df_CLEAN_ids.merge(qid_r2_editing_CLEAN, how=\"left\", on=['page_title', 'database_code'])\n",
    "nppt_articles_q = qid_editing_CLEAN_r.merge(nppt_articles, how=\"left\", on=['page_title', 'database_code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY TRANSLATION SUBLIST FOR QITEMS ON ENWIKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_topic_rec_df_CLEAN['article_suggestion'] = translation_topic_rec_df_CLEAN['article_suggestion'].str.replace('_', ' ')\n",
    "titles_denormalized_translation_CLEAN = tuple(list(translation_topic_rec_df_CLEAN['article_suggestion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get qids for translation articles\n",
    "qid_en_CLEAN_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS article_suggestion,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = 'enwiki' \n",
    "  AND ips_site_page IN {titles_denormalized_translation_CLEAN}\n",
    "\"\"\".format(titles_denormalized_translation_CLEAN=titles_denormalized_translation_CLEAN), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUERY translation rec qids for sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_qids = tuple(list(qid_en_CLEAN_r['QID']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'translation_qids' : translation_qids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "iwl_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  linked_item.ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM (\n",
    "      SELECT ips_item_id\n",
    "      FROM wb_items_per_site\n",
    "      WHERE ips_item_id IN {translation_qids}\n",
    "      AND ips_site_id IN {wikis}\n",
    "    ) AS linked_item\n",
    "LEFT JOIN wb_items_per_site \n",
    "  ON linked_item.ips_item_id = wb_items_per_site.ips_item_id\n",
    "LEFT JOIN page \n",
    "  ON linked_item.ips_item_id = page.page_id\n",
    "GROUP BY page_id\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to get df with article_suggestion, QID, iwsites, iwsitelinks\n",
    "t_iwl_q = iwl_r.merge(qid_en_CLEAN_r, how=\"left\", on=['QID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the translation-interwikilinks-QID df (t_iwl_q) into translation_topic_rec_df_CLEAN\n",
    "t_rec_iwl_q = t_iwl_q.merge(translation_topic_rec_df_CLEAN, how=\"left\", on=['article_suggestion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QID</th>\n",
       "      <th>iwsites</th>\n",
       "      <th>iwsitelinks</th>\n",
       "      <th>article_suggestion</th>\n",
       "      <th>local_encoded_title</th>\n",
       "      <th>g_category</th>\n",
       "      <th>language_name</th>\n",
       "      <th>suggestion_type</th>\n",
       "      <th>file</th>\n",
       "      <th>language_code</th>\n",
       "      <th>database_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [QID, iwsites, iwsitelinks, article_suggestion, local_encoded_title, g_category, language_name, suggestion_type, file, language_code, database_code]\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates\n",
    "t_rec_iwl_q[t_rec_iwl_q.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated_translation_recs 4443\n",
      "dupes 6\n"
     ]
    }
   ],
   "source": [
    "#the article is sometimes suggested as an article suggestion for more than one wiki\n",
    "duplicated_translation_recs = t_rec_iwl_q[t_rec_iwl_q.duplicated(['article_suggestion'])]\n",
    "dupe_check = t_rec_iwl_q[t_rec_iwl_q.duplicated(['article_suggestion', 'database_code', 'local_encoded_title'])]\n",
    "\n",
    "print(\"duplicated_translation_recs\", len(duplicated_translation_recs))\n",
    "print(\"dupes\", len(dupe_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop dupes\n",
    "t_rec_iwl_q = t_rec_iwl_q.drop_duplicates(subset=['article_suggestion', 'database_code', 'local_encoded_title', 'file'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_rec_iwl_q = t_rec_iwl_q[['article_suggestion',\n",
    "                                     'QID','database_code', \n",
    "                                     'iwsites',\n",
    "                                     'iwsitelinks',\n",
    "                                     'language_code','file',\n",
    "                                     'suggestion_type',\n",
    "                                     'language_name',\n",
    "                                     'g_category',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation articles - iwlinks match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create boolean column: TRUE if 'iwsites' column str contains 'database_code' value\n",
    "#substring search for the database code in the “iwsites” column, if the database code is set\n",
    "#[dbcode in iwsites if dbcode is not None else False for (dbcode, iwsites) in zip(translation_rec_iwl_q['database_code'], translation_rec_iwl_q['iwsites'])]\n",
    "translation_rec_iwl_q['database_code_in_iwsites'] = [x[0] in x[1] if x[0] is not None else False for x in zip(translation_rec_iwl_q['database_code'], translation_rec_iwl_q['iwsites'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1894"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_rec_iwl_q['database_code_in_iwsites'].values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestions_created_a = translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894\n"
     ]
    }
   ],
   "source": [
    "count_suggestions_created_a = translation_rec_iwl_q['database_code_in_iwsites'].values.sum()\n",
    "print(count_suggestions_created_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation articles - Qid match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective: count matches by wikidata item in the suggested language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get a df of articles that DON'T have an interwiki link associated with the \n",
    "#suggestion database_code, aka, they weren't created as far as we know so far\n",
    "\n",
    "located =  translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == True]\n",
    "not_yet_located = translation_rec_iwl_q[translation_rec_iwl_q['database_code_in_iwsites'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fountain_titles = pd.read_csv(\"../../data/raw/articles/2019/contest_titles_n_updated.csv\", sep=',', encoding = 'utf-8')\n",
    "fountain_titles_to_cull = fountain_titles[['wiki_db', 'QID']]\n",
    "fountain_titles_to_cull = fountain_titles_to_cull.rename(columns={'wiki_db': 'database_code'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "translation_sugg_to_cull = not_yet_located[['database_code','QID']]\n",
    "suggestions_created_b = pd.merge(fountain_titles_to_cull, translation_sugg_to_cull, on=['database_code', 'QID'], how='inner')\n",
    "count_suggestions_created_b = len(suggestions_created)\n",
    "print(count_suggestions_created_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > How many translation articles from the Google list were created since the GLOW contest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage(part, whole):\n",
    "  return 100 * float(part)/float(whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total values in full rec list: 34295\n",
      "***\n",
      "total recs in translation: 14155\n",
      "total recs in editing: 20102\n",
      "***\n",
      "35.33479255795443 % created from editing list\n",
      "13.811374072765807 % created from translation list\n"
     ]
    }
   ],
   "source": [
    "translation_recs = len(translation_topic_rec_df_CLEAN)\n",
    "editing_recs = len(editing_topic_rec_df_CLEAN)\n",
    "edited_count = nppt_articles_edits.groupby(['database_code', 'page_id']).ngroups\n",
    "translation_count = suggestions_created_a+suggestions_created_b\n",
    "print(\"Total values in full rec list:\", len(full_topic_rec_df_ci))\n",
    "print('***')\n",
    "print(\"total recs in translation:\", len(translation_topic_rec_df_CLEAN))\n",
    "print(\"total recs in editing:\", len(editing_topic_rec_df_CLEAN))\n",
    "print('***')\n",
    "print(percentage(edited_count,editing_recs),\"% created from editing list\")\n",
    "print(percentage(translation_count,translation_recs),\"% created from translation list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > 1955+ articles were picked from the suggestion lists (out of 34k+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1894+ articles from the Google provided 'translation' list of articles were created, since the contest started, and had the related iwl added "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 61 articles from the Google provided 'translation' list of articles were created, since the contest started, and had a matching QID added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes the 1894 with related interwiki links\n",
    "suggestions_created_a.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_created_a.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#includes the 61\n",
    "suggestions_created_b.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_created_b.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nppt_articles_edits_contest_unique.to_csv(\"../../data/processed/query_results/topic_lists/stubs_edited_during_contest.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Do in the future, topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "get latest topic model - see Isaac\n",
    "Answer the following questions in the future, if time/project level interest:\n",
    "Q: which topics did editors write about?\n",
    "Q: what did editors select from these suggestions? \n",
    "Q: which topics most resonated or were most popular to write about from these lists? by wiki?\n",
    "Q: which topics did our partner pass on to us...which search terms made their way to us?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "editing_recs = qid_r2_editing_CLEAN[['page_title', 'QID']]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suggestion_qids = pd.concat([qid_r_en_CLEAN, editing_recs], sort=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "suggestion_qids.to_csv(\"../../data/processed/query_results/topic_lists/suggestions_qids.csv\", sep=',', encoding = 'utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
