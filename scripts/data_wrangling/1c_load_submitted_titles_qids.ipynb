{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata 0.1.0 (latest).\n",
      "\n",
      "You can find the source for `wmfdata` at https://github.com/neilpquinn/wmfdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/raw/articles/2019/N_India_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add wiki_db column for querying\n",
    "df['wiki_db'] = df['Language']+'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_titles_denormalized = tuple(list(df['Articles']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vars = dict(\n",
    "    contest_titles_denormalized = contest_titles_denormalized,\n",
    "    wiki_dbs = wiki_dbs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get qids \n",
    "qid_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS article,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {wiki_dbs} \n",
    "      AND ips_site_page IN {contest_titles_denormalized}\n",
    "\"\"\".format(**article_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clean list\n",
    "\n",
    "#merge in the clean list into the df\n",
    "df_w_ids = pd.merge(df, qid_r[['article', 'QID']], how=\"left\", left_on=['Articles'], right_on=['article']).drop('article', axis=1)\n",
    "\n",
    "#CLEAN DF - ready to use, drop na, drop all dupes\n",
    "df_w_ids_no_nulls = df_w_ids[df_w_ids['QID'].notna()]\n",
    "df_w_ids_clean = df_w_ids_no_nulls.drop_duplicates(subset=['Language', 'Articles', 'wiki_db', 'QID'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DF - to add data\n",
    "articles_w_QID_missing = df_w_ids[df_w_ids['QID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify dupes \n",
    "#articles_w_QID_duplicated = df_w_ids_no_nulls[df_w_ids_no_nulls.duplicated(subset=['Language', 'Articles', 'wiki_db', 'QID'], keep=False)]\n",
    "\n",
    "#keep only first instance of dupes of full row duplicates\n",
    "articles_w_QID_duplicated = df_w_ids_no_nulls[df_w_ids_no_nulls.duplicated(subset=['Language', 'Articles', 'wiki_db', 'QID'], keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MISSING DF - to add data\n",
    "articles_w_QID_missing\n",
    "\n",
    "#DUPES DF - to clean \n",
    "articles_w_QID_duplicated\n",
    "\n",
    "#CLEAN DF - ready to use\n",
    "df_w_ids_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated = articles_w_QID_duplicated.append(df_w_ids_clean)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "articles_w_QID_missing.sample(8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_updated.sample(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_w_QID_missing.to_csv(\"../../data/raw/articles/2019/articles_QID_missing.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_updated.to_csv(\"../../data/raw/articles/2019/contest_titles_n_updated.csv\", sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.replace(['ਸੁਦਾਮਾ_ਪਾਂਡੇ_ਧੂਮਿਲ\"\"'], [\"ਸੁਦਾਮਾ_ਪਾਂਡੇ_ਧੂਮਿਲ\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of wikis_in_submitted_articles_df from 1b notebook\n",
    "%store -r wikis_in_submitted_articles_df\n",
    "\n",
    "#get working contest api urls\n",
    "ptp_2018_base = 'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2018-{}'\n",
    "ptp_2_base = 'https://tools.wmflabs.org/fountain/api/editathons/project-tiger-2.0-{}'\n",
    "\n",
    "#wikicodes = wikis_in_submitted_articles_df\n",
    "wikicodes = ['as', 'bn', 'gu', 'hi', 'mli', 'or', 'pa', 'sat', 'ta', 'ur', 'te', 'mr', 'tcy', 'kn', 'sa', 'pnb']\n",
    "\n",
    "def get_contest_urls(url_base):\n",
    "    urls_to_review = list()\n",
    "    for wikicode in wikicodes:\n",
    "        urls = (url_base.format(wikicode))\n",
    "        urls_to_review.append(urls)\n",
    "\n",
    "    not_found_urls = []\n",
    "    working_urls = []\n",
    "    final_urls_list = []\n",
    "    # Iterate here on the urls\n",
    "    # The below code could be executed for each url\n",
    "    for url in urls_to_review:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 404:\n",
    "            not_found_urls.append(url)\n",
    "\n",
    "    working_urls = list(set(urls_to_review)-set(not_found_urls))\n",
    "    return working_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contest_data(working_urls):\n",
    "    collected_data = pd.DataFrame([])\n",
    "    for url in working_urls:\n",
    "        URL = url\n",
    "        r = requests.get(URL)\n",
    "        pretty_json = json.loads(r.text)\n",
    "        #pretty_data_dump = json.dumps(pretty_json, indent=2)\n",
    "        data = json_normalize(pretty_json)\n",
    "        core = data[['wiki','code', 'finish', 'start', 'jury', 'articles']]\n",
    "        lens = [len(item) for item in core['articles']]\n",
    "        explode_elongate_prep = pd.DataFrame({\n",
    "            'wiki':np.repeat(core['wiki'].values, lens),\n",
    "            'code':np.repeat(core['code'].values, lens),\n",
    "            'finish':np.repeat(core['finish'].values, lens),\n",
    "            'start':np.repeat(core['start'].values, lens),\n",
    "            'jury':np.repeat(core['jury'].values, lens),\n",
    "            'articles':np.hstack(core['articles']),\n",
    "                              })\n",
    "        explode_elongate = pd.concat([explode_elongate_prep.drop(['articles'], axis=1), explode_elongate_prep['articles'].apply(pd.Series)],axis=1)\n",
    "        df = explode_elongate.rename(columns={'name': 'article_name', 'user': 'user_name', 'code': 'contest_code'})\n",
    "        \n",
    "        collected_data = collected_data.append(df, ignore_index=True, sort=False)\n",
    "    return collected_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#run\n",
    "wlw_2019_w_urls = get_contest_urls(wlw_2019_base)\n",
    "ptp_2018_w_urls = get_contest_urls(ptp_2018_base)\n",
    "wam_2019_w_urls = get_contest_urls(wam_2019_base)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "wlw_2019_data = get_contest_data(wlw_2019_w_urls)\n",
    "ptp_2018_data = get_contest_data(ptp_2018_w_urls)\n",
    "wam_2019_data = get_contest_data(wam_2019_w_urls)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#clean data each df\n",
    "\n",
    "df_wam_2019 = wam_2019_data[['wiki', 'contest_code', 'start', 'finish', 'jury', 'dateAdded', 'article_name', 'user_name']]\n",
    "cols_to_drop = ['marks','id']\n",
    "df_wlw_2019 = wlw_2019_data[wlw_2019_data.columns.drop(cols_to_drop)]\n",
    "df_ptp_2018 = ptp_2018_data[ptp_2018_data.columns.drop(cols_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp_2_w_urls = get_contest_urls(ptp_2_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptp_2_data = get_contest_data(ptp_2_w_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data each df\n",
    "\n",
    "df_ptp_2 = ptp_2_data[['wiki', 'contest_code', 'start', 'finish', 'jury', 'dateAdded', 'article_name', 'user_name']]\n",
    "#cols_to_drop = ['marks','id']\n",
    "#df_wlw_2019 = wlw_2019_data[wlw_2019_data.columns.drop(cols_to_drop)]\n",
    "df_ptp_2.rename(columns={'article_name':'page_title'}, inplace=True)\n",
    "df_ptp_2['database_code'] = df_ptp_2['wiki']+'wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
