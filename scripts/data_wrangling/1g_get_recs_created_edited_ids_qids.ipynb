{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Recs Created/Edited - Ids & QIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guiding Question:\n",
    "Q: how many articles were created from the suggestions? \n",
    "\n",
    "Process:\n",
    "\n",
    "Prep:\n",
    "1. Combine and clean suggestion lists\n",
    "2. groupby type: editing suggestions\n",
    "\n",
    "    a. get pageids (not including redirects) \n",
    "    \n",
    "    b. get wikidata items using the local language (for use with topics, if time permits)\n",
    "    \n",
    "3. groupby type: translation suggestions\n",
    "\n",
    "    a. get wikidata items using enwiki \n",
    "    \n",
    "    b. get iwlinks\n",
    "    \n",
    "4. Read and clean the list of articles created and submitted to GLOW (includes articles ultimately disapproved)\n",
    "\n",
    "\n",
    "Analysis:\n",
    "5. groupby type: editing suggestions\n",
    "\n",
    "    a. count matches by ids - count ids that were edited during the contest period\n",
    "    \n",
    "6. groupby type: translation suggestions\n",
    "\n",
    "    a. count matches by wikidata item in the suggested language\n",
    "    \n",
    "    b. count matches by iwlink in the suggested language\n",
    "    \n",
    "    c. get a sum of items a+b\n",
    "\n",
    "NOTES:\n",
    "1. Analysis notes\n",
    "    a. If the editor uses Content Translation, it should automatically assign the right QID\n",
    "    b. If the editor doesn't use CT, either they or someone else has to assign the QID\n",
    "    c. We will miss articles that were not created via Content Translation and don't have a manually added QID and/or the editor changed the suggested title to something new. \n",
    "4. Suggestion notes\n",
    "    a. From the creators of the suggestions: \"As a reminder, we have 2 lists: a list of suggested topics that exist in the local language but could be edited to be more complete based on the corresponding English page, and a list of topics that can be translated from English to the local language.  Based on feedback from the initial Project Tiger, we've separated out the topics by categories so editors can focus on the areas they like to write about.  The lists are ordered by popularity of what local language users are looking for.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using wmfdata v1.0.3, but v1.0.4 is available.\n",
      "\n",
      "To update, run `pip install --upgrade git+https://github.com/neilpquinn/wmfdata.git@release`.\n",
      "\n",
      "To see the changes, refer to https://github.com/neilpquinn/wmfdata/blob/release/CHANGELOG.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "import urllib\n",
    "from urllib.parse import unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indonesia - reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile all of the lists\n",
    "\n",
    "f_mask = r'../../../GLOW/data/raw/g_topic_lists/Indonesia/*.xlsx'\n",
    "\n",
    "gtl = \\\n",
    "pd.concat([gtl.assign(file=os.path.splitext(os.path.basename(f))[0],\n",
    "                     sheet=sheet)\n",
    "           for f in glob(f_mask)\n",
    "           for sheet, gtl in pd.read_excel(f, sheet_name=None).items()],\n",
    "          ignore_index=True, sort=True)\n",
    "\n",
    "full_topic_rec_df = gtl.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del full_topic_rec_df['local_wikipedia.1']\n",
    "\n",
    "#get article suggestion from url, as opposed to getting it from the google 'topic' which is sometimes a short hand version of the title\n",
    "full_topic_rec_df['article_suggestion'] = full_topic_rec_df['english_wikipedia'].str.rsplit(\".org/wiki/\").str[-1]\n",
    "full_topic_rec_df.loc[full_topic_rec_df[\"article_suggestion\"].isnull(),'article_suggestion'] = full_topic_rec_df[\"Topic\"] \n",
    "\n",
    "#encoded to decoded -- if there are nulls it will provide a float type error\n",
    "full_topic_rec_df['article_suggestion'] = full_topic_rec_df['article_suggestion'].apply(lambda x: unquote(x) if pd.notnull(x) else x).copy(deep=False) #apply if value not null\n",
    "\n",
    "full_topic_rec_df = full_topic_rec_df.rename(columns={#'Topic': 'article_suggestion',\n",
    "                                                      'sheet':'g_category',\n",
    "                                                      'wikidata_item':'QID',\n",
    "                                                     }\n",
    "                                            )\n",
    "full_topic_rec_df[['language_name', 'suggestion_type']] = full_topic_rec_df['file'].str.split(\" \", 1, expand=True)\n",
    "full_topic_rec_df['suggestion_type'] = full_topic_rec_df['file'].str.rsplit(\" \").str[-1]\n",
    "del full_topic_rec_df['file']\n",
    "full_topic_rec_df = full_topic_rec_df.replace({'language_name' : { 'Sunda' : 'Sundanese', 'Jawa' : 'Javanese', 'Bahasa' : 'Indonesian' }})\n",
    "\n",
    "#extract url title\n",
    "full_topic_rec_df['local_encoded_title'] = full_topic_rec_df['local_wikipedia'].str.extract('([^\\/]+$)', expand=True)\n",
    "\n",
    "#encoded URL to decoded title -- if there are nulls it will provide a float type error\n",
    "full_topic_rec_df['page_title'] = full_topic_rec_df['local_encoded_title'].apply(lambda x: unquote(x) if pd.notnull(x) else x).copy(deep=False) #apply if value not null\n",
    "\n",
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "full_topic_rec_df['page_title'] = full_topic_rec_df['page_title'].str.replace(' ', '_')\n",
    "\n",
    "#remove Q from QID if exists\n",
    "full_topic_rec_df['QID'] = full_topic_rec_df['QID'].str.replace('Q', '')\n",
    "\n",
    "#replace empty strings with Nans\n",
    "full_topic_rec_df['QID'] = full_topic_rec_df['QID'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "#confirm\n",
    "sub = '   '\n",
    "full_topic_rec_df[full_topic_rec_df['QID'].str.contains(sub, na=False)]\n",
    "\n",
    "\n",
    "#make int\n",
    "#full_topic_rec_df['QID'] = full_topic_rec_df['QID'].astype(int)\n",
    "full_topic_rec_df['QID'] = pd.to_numeric(full_topic_rec_df['QID'], errors='coerce', downcast='integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### India - reading data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#compile all of the lists\n",
    "\n",
    "f_mask = r'../../../GLOW/data/raw/g_topic_lists/*.xlsx'\n",
    "\n",
    "gtl = \\\n",
    "pd.concat([gtl.assign(file=os.path.splitext(os.path.basename(f))[0],\n",
    "                     sheet=sheet)\n",
    "           for f in glob(f_mask)\n",
    "           for sheet, gtl in pd.read_excel(f, sheet_name=None).items()],\n",
    "          ignore_index=True, sort=True)\n",
    "\n",
    "full_topic_rec_df = gtl.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#combine Topic & entity name > article_suggestion\n",
    "full_topic_rec_df['article_suggestion'] = full_topic_rec_df['entity_name'].combine_first(full_topic_rec_df['Topic'])\n",
    "#rename 'sheet' to Google_topic\n",
    "full_topic_rec_df = full_topic_rec_df.rename(columns={'sheet':'g_category', \n",
    "                                                      'Topic': 'g_suggested_en_title', \n",
    "                                                      'entity_name': 'g_suggested_local_title',\n",
    "                                                      'english_wikipedia':'english_wikipedia_URL', \n",
    "                                                      'local_wikipedia':'local_wikipedia_URL'\n",
    "                                                     })\n",
    "\n",
    "#extract wiki name & suggestion_type (translation or edit)\n",
    "#full_topic_rec_df['wiki'] = full_topic_rec_df['file'].str.extract('(^[A-Z_]+([^\\(-]+))', expand=True)\n",
    "full_topic_rec_df[['language_name', 'suggestion_type']] = full_topic_rec_df['file'].str.split(\" \", 1, expand=True)\n",
    "full_topic_rec_df['suggestion_type'] = full_topic_rec_df['file'].str.rsplit(\"for \").str[-1]\n",
    "\n",
    "#extract url title\n",
    "full_topic_rec_df['local_encoded_title'] = full_topic_rec_df['local_wikipedia_URL'].str.extract('([^\\/]+$)', expand=True)\n",
    "\n",
    "#extract lang code\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['local_wikipedia_URL'].str.rsplit(\"http://\").str[-1]\n",
    "#full_topic_rec_df['url_language_code'] = full_topic_rec_df['url_language_code'].str.extract('([^.]+)', expand=True)\n",
    "\n",
    "#reorder for visual skimming's sake\n",
    "full_topic_rec_df = full_topic_rec_df[['article_suggestion', 'local_encoded_title','g_category', 'language_name','suggestion_type', 'file']] #'url_language_code'\n",
    "\n",
    "#replace double coded translation entries\n",
    "full_topic_rec_df['suggestion_type']=full_topic_rec_df['suggestion_type'].replace('Translating EXTERNAL', 'Translation EXTERNAL')\n",
    "full_topic_rec_df['language_name'] = full_topic_rec_df['language_name'].replace('Bengali', 'Bangla')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_topic_rec_df[full_topic_rec_df['language_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if pnb article_suggestions exist, confirm language_code = url_language_code(see code above to add in url_language_code column)\n",
    "#pnb\tpnbwiki\tWestern Punjabi "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get database_code and language_code, confirm language_code (if needed)\n",
    "lang_names =tuple(full_topic_rec_df['language_name'].unique())\n",
    "\n",
    "ci = wmf.hive.run(\"\"\"\n",
    "SELECT  language_code, database_code, language_name\n",
    "FROM canonical_data.wikis\n",
    "WHERE language_name IN {lang_names} AND database_group = 'wikipedia'\n",
    "\"\"\".format(lang_names=lang_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "full_topic_rec_df_ci = full_topic_rec_df.merge(ci, how=\"left\", on=['language_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Translating', 'Editing'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_topic_rec_df_ci['suggestion_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "translation_topic_rec_df = full_topic_rec_df_ci[full_topic_rec_df_ci['suggestion_type'] == 'Translating'].copy(deep=False)\n",
    "\n",
    "#get clean list - drop duplicates\n",
    "translation_topic_rec_df_CLEAN = translation_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'local_encoded_title','g_category','suggestion_type', 'language_code'], keep='first').copy(deep=False)\n",
    "\n",
    "#keep just the duplicates - for checking data later on\n",
    "translation_topic_rec_df_Dupes = pd.concat([translation_topic_rec_df, translation_topic_rec_df_CLEAN]).loc[translation_topic_rec_df.index.symmetric_difference(translation_topic_rec_df_CLEAN.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use groupby to get two seperate dfs for each suggestion_type\n",
    "editing_topic_rec_df = full_topic_rec_df_ci.loc[full_topic_rec_df_ci['suggestion_type'] == 'Editing'].copy(deep=False)\n",
    "\n",
    "#get clean list - drop duplicates\n",
    "editing_topic_rec_df_CLEAN = editing_topic_rec_df.drop_duplicates(subset=['article_suggestion', 'local_encoded_title','g_category','suggestion_type', 'language_code'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editing subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get article ids, redirects <a class=\"anchor\" id=\"get_clean_list\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "\n",
    "articles = []\n",
    "\n",
    "def get_clean_ids_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with pageids for non redirect articles\n",
    "    '''\n",
    "\n",
    "    clean_id_query = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title IN {raw_articles}\n",
    "    '''\n",
    "    \n",
    "    clean_id_query_one_article = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title = '{raw_articles}'\n",
    "    '''\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        print('***')\n",
    "        print(wiki)\n",
    "        \n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_title'].apply(pd.DataFrame)\n",
    "        raw_articles = tuple(list(grouping[wiki]))\n",
    "        article = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        \n",
    "        if len(raw_articles)>= 2:\n",
    "            redirects_r = mariadb.run(clean_id_query.format(raw_articles=raw_articles), wiki )\n",
    "        else: redirects_r = mariadb.run(clean_id_query_one_article.format(raw_articles=article), wiki )\n",
    "        articles.append(redirects_r)   \n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "jvwiki\n",
      "***\n",
      "idwiki\n",
      "***\n",
      "suwiki\n"
     ]
    }
   ],
   "source": [
    "get_clean_ids_mariadb(editing_topic_rec_df);\n",
    "edit_id_results = pd.concat(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "editing_df = editing_topic_rec_df_CLEAN.merge(edit_id_results, how=\"left\", on=['page_title', 'database_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((editing_df['p1_is_redirect']==1) & (editing_df['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((editing_df['p1_is_redirect']==1) | (editing_df['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# act on the results from nppt_ids\n",
    "#create a df \n",
    "all_surviving_articles = nppt_ids[['page_id','page_title', 'database_code']] \n",
    "#seperate the redirected items into their own df\n",
    "redirects = nppt_ids.loc[nppt_ids['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title', 'database_code']] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "nppt_articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "#create a new wikicode column using quality_vars['wiki_db']\n",
    "#ffill could also work here\n",
    "#articles['wikicode'] = quality_vars['wiki_db']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_df_missing_pageid = editing_df[editing_df['page_id'].isnull()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# manual check of editing_df_missing_pageid\n",
    "# to see if the article suggestions exist and were missed by the code (aka update/improve the code)\n",
    "# or if they are faulty recs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### wikidata Q item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Wikibase/Schema/wb_items_per_site\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#wb_items_per_site site:quarry.wmflabs.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wikidata items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "editing_df['page_title'] = editing_df['page_title'].str.replace('_', ' ')\n",
    "\n",
    "#select rows that are missing wd items & split into two dfs\n",
    "missing_wd = editing_df[editing_df['QID'].isnull()]\n",
    "w_wd = editing_df[~editing_df['QID'].isnull()]\n",
    "\n",
    "#create tuples of the article_suggestions and wiki_codes to use when querying for the wikidata items\n",
    "editing_titles_denormalized_CLEAN = tuple(list(missing_wd['page_title']))\n",
    "editing_titles_denormalized_database_codes_CLEAN = tuple(list(missing_wd['database_code']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars = {}\n",
    "wd_vars.update({\n",
    "    'editing_titles_denormalized' : editing_titles_denormalized_CLEAN,\n",
    "    'editing_titles_denormalized_db_codes' : editing_titles_denormalized_database_codes_CLEAN,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO if redirect then page_title = rpage_title\n",
    "\n",
    "qid_editing_CLEAN_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS page_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {editing_titles_denormalized_db_codes} AND\n",
    "      ips_site_page IN {editing_titles_denormalized}\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_editing_query_results = qid_editing_CLEAN_r.copy()\n",
    "editing_articles_qid_query = missing_wd.merge(qid_editing_query_results, how= 'left', on=['page_title', 'database_code']).drop('QID_x', axis=1).rename({'QID_y': 'QID'}, axis=1)\n",
    "\n",
    "editing_df_qid_base = pd.concat([editing_articles_qid_query, w_wd], sort=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Redirects from the edit df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_df_qid_base['rpage_title'] = editing_df_qid_base['rpage_title'].str.replace('_', ' ')\n",
    "\n",
    "e_missing_qid = editing_df_qid_base[editing_df_qid_base.rpage_title.notna() & editing_df_qid_base['QID'].isnull()]\n",
    "\n",
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "\n",
    "#create tuples of the article_suggestions and wiki_codes to use when querying for the wikidata items\n",
    "editing_titles_denormalized_CLEAN_redirects = tuple(list(e_missing_qid['rpage_title']))\n",
    "editing_titles_denormalized_database_codes_CLEAN_redirects = tuple(list(e_missing_qid['database_code']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars = {}\n",
    "wd_vars.update({\n",
    "    'editing_titles_denormalized_redirects' : editing_titles_denormalized_CLEAN_redirects,\n",
    "    'editing_titles_denormalized_db_codes_redirects' : editing_titles_denormalized_database_codes_CLEAN_redirects,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_editing_CLEAN_redirects = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS rpage_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {editing_titles_denormalized_db_codes_redirects} AND\n",
    "      ips_site_page IN {editing_titles_denormalized_redirects}\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_df_qid = editing_df_qid_base.merge(qid_editing_CLEAN_redirects, how= 'left', on=['rpage_title', 'database_code'], suffixes=('_x', '_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_df_qid[\"QID_x\"] = editing_df_qid[\"QID_x\"].fillna(0)\n",
    "editing_df_qid[\"QID_y\"] = editing_df_qid[\"QID_y\"].fillna(0)\n",
    "editing_df_qid['QID'] = editing_df_qid['QID_x'] + editing_df_qid['QID_y']\n",
    "editing_df_qid['QID'] = editing_df_qid['QID'].replace(0,np.nan)\n",
    "editing_df_qid.drop(['QID_x', 'QID_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = editing_df_qid[~editing_df_qid['QID'].isnull()]\n",
    "editing_qids = tuple(list(clean['QID'])) #w/o nulls\n",
    "\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'editing_qids' : editing_qids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "iwl_e_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM wb_items_per_site\n",
    "      WHERE ips_item_id IN {editing_qids}\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_e_query_results = iwl_e_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "editing_df_qid_iwl = editing_df_qid.merge(qid_e_query_results, how='left', on=['QID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of suggestions: 8393\n",
      "missing_wd: 5360\n",
      "w_wd: 3033\n",
      "qid_editing_query_results: 7218\n",
      "editing_articles_qid_query: 5360\n",
      "editing_df_qid: 8393\n"
     ]
    }
   ],
   "source": [
    "print(\"# of suggestions:\", len(editing_df))\n",
    "print('missing_wd:', len(missing_wd))\n",
    "print('w_wd:', len(w_wd))\n",
    "print('qid_editing_query_results:', len(qid_editing_query_results))\n",
    "print(\"editing_articles_qid_query:\", len(editing_articles_qid_query))\n",
    "print('editing_df_qid:', len(editing_df_qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSLATION SUBLIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUERY FOR QITEMS ON ENWIKI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "translation_topic_rec_df_CLEAN['article_suggestion'] = translation_topic_rec_df['article_suggestion'].str.replace('_', ' ')\n",
    "translation_topic_rec_df_CLEAN['Topic'] = translation_topic_rec_df['Topic'].str.replace('_', ' ')\n",
    "\n",
    "t = translation_topic_rec_df_CLEAN.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikidata items via article_suggestion\n",
    "#select only rows that don't have missing article_suggestion\n",
    "ttrd_not_null = t[~t['article_suggestion'].isnull()] #check nulls\n",
    "\n",
    "#select rows that are missing wd items\n",
    "t_missing_wd = ttrd_not_null[ttrd_not_null['QID'].isnull()]\n",
    "\n",
    "titles_denormalized_translation_CLEAN = tuple(list(t_missing_wd['article_suggestion']))\n",
    "\n",
    "#get qids for translation articles\n",
    "qid_en_CLEAN_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS article_suggestion,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = 'enwiki' \n",
    "  AND ips_site_page IN {titles_denormalized_translation_CLEAN}\n",
    "\"\"\".format(titles_denormalized_translation_CLEAN=titles_denormalized_translation_CLEAN), \"wikidatawiki\")\n",
    "\n",
    "qid_t_query_results = qid_en_CLEAN_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_w_t_query = t.merge(qid_t_query_results, how='left', on=['article_suggestion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_w_t_query['QID_x'].equals(t_w_t_query['QID_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_w_t_query.loc[~t_w_t_query[\"QID_y\"].isnull(),'QID_x'] = t_w_t_query[\"QID_y\"] \n",
    "t_w_t_query = t_w_t_query.drop('QID_y', axis=1).rename({'QID_x': 'QID'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikidata items via Topic, for those that are still nulls after the above query\n",
    "#select only rows that don't have missing topic\n",
    "qrwdmq_not_null = t_w_t_query[~t_w_t_query['Topic'].isnull()] #check nulls\n",
    "\n",
    "#select rows that are missing wd items \n",
    "qrwdmq_missing_wd = qrwdmq_not_null[qrwdmq_not_null['QID'].isnull()]\n",
    "\n",
    "topics_denormalized_translation_CLEAN = tuple(list(qrwdmq_missing_wd['Topic']))\n",
    "\n",
    "#get qids for translation articles\n",
    "qid_en_topics_CLEAN_raw = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_site_page AS Topic,\n",
    "  ips_item_id AS QID\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = 'enwiki' \n",
    "  AND ips_site_page IN {topics_denormalized_translation_CLEAN}\n",
    "\"\"\".format(topics_denormalized_translation_CLEAN=topics_denormalized_translation_CLEAN), \"wikidatawiki\")\n",
    "qid_en_topics_CLEAN_r = qid_en_topics_CLEAN_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results_w_df_missing_qids2 = t_w_t_query.merge(qid_en_topics_CLEAN_r, how='left', on=['Topic'])#.drop('QID_x', axis=1).rename({'QID_y': 'QID'}, axis=1)\n",
    "query_results_w_df_missing_qids2.loc[~query_results_w_df_missing_qids2[\"QID_y\"].isnull(),'QID_x'] = query_results_w_df_missing_qids2[\"QID_y\"] \n",
    "query_results_w_df_missing_qids2 = query_results_w_df_missing_qids2.drop('QID_y', axis=1).rename({'QID_x': 'QID'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with redirect suggestions - part 1: get redir titles\n",
    "#change titles from denormalized (spaces) to normalized (underscore) for querying the page table etc.\n",
    "translation_df_qid_base['article_suggestion'] = query_results_w_df_missing_qids2['article_suggestion'].str.replace(' ', '_')\n",
    "\n",
    "t_qid_nulls = query_results_w_df_missing_qids2[query_results_w_df_missing_qids2['QID'].isnull()]\n",
    "\n",
    "article_suggestions_tuple = tuple(list(t_qid_nulls['article_suggestion']))\n",
    "\n",
    "#get ids for translation articles\n",
    "t_ids_raw = wmf.mariadb.run(\"\"\"\n",
    "    SELECT \n",
    "       p1.page_title AS article_suggestion,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title IN {article_suggestions_tuple}\n",
    "\"\"\".format(article_suggestions_tuple=article_suggestions_tuple), 'enwiki')\n",
    "\n",
    "t_ids_r = t_ids_raw.copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "#deal with redirect suggestions - part 2: get wiki data items\n",
    "t_ids_r['rpage_title'] = t_ids_r['rpage_title'].str.replace('_', ' ')\n",
    "t_ids_r['article_suggestion'] = t_ids_r['article_suggestion'].str.replace('_', ' ')\n",
    "\n",
    "r_page_titles = t_ids_r[~t_ids_r['rpage_id'].isnull()]\n",
    "r_page_title_tuple = tuple(list(r_page_titles['rpage_title']))\n",
    "\n",
    "#get qids for translation articles\n",
    "qid_en_from_rtitles_raw = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_item_id AS QID,\n",
    "  ips_site_page AS rpage_title\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id = 'enwiki' \n",
    "  AND ips_site_page IN {r_page_title_tuple}\n",
    "\"\"\".format(r_page_title_tuple=r_page_title_tuple), \"wikidatawiki\")\n",
    "\n",
    "qid_en_from_rtitles_r = qid_en_from_rtitles_raw.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirect_suggs_info = t_ids_r.merge(qid_en_from_rtitles_r, how='left', on=['rpage_title'])\n",
    "\n",
    "redirect_suggs_info_df = t_qid_nulls.merge(redirect_suggs_info, how='left', on=['article_suggestion'])\n",
    "#z.drop('QID_x', axis=1, inplace=True)\n",
    "\n",
    "redirect_suggs_info_df[\"QID_x\"] = redirect_suggs_info_df[\"QID_x\"].fillna(0)\n",
    "redirect_suggs_info_df[\"QID_y\"] = redirect_suggs_info_df[\"QID_y\"].fillna(0)\n",
    "redirect_suggs_info_df['QID'] = redirect_suggs_info_df['QID_x'] + redirect_suggs_info_df['QID_y']\n",
    "redirect_suggs_info_df['QID'] = redirect_suggs_info_df['QID'].replace(0,np.nan)\n",
    "redirect_suggs_info_df.drop(['QID_x', 'QID_y'], axis=1, inplace=True)\n",
    "\n",
    "redirect_suggs_info_df.drop('database_code_y', axis=1, inplace=True)\n",
    "redirect_suggs_info_df = redirect_suggs_info_df.rename(columns={'database_code_x':'database_code'})\n",
    "\n",
    "redirect_suggs_info_df['rpage_title'] = redirect_suggs_info_df['rpage_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_merge_in = redirect_suggs_info_df[~redirect_suggs_info_df['QID'].isnull()]\n",
    "to_merge_in = to_merge_in [['Topic', 'article_suggestion', 'QID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_df_qid = query_results_w_df_missing_qids2.merge(to_merge_in, how='left', on=['Topic',\n",
    "                                                                               'article_suggestion',\n",
    "                                                                              ])\n",
    "\n",
    "translation_df_qid.loc[~translation_df_qid[\"QID_y\"].isnull(),'QID_x'] = translation_df_qid[\"QID_y\"] \n",
    "translation_df_qid = translation_df_qid.drop('QID_y', axis=1).rename({'QID_x': 'QID'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = redirect_suggs_info_df[redirect_suggs_info_df['QID'].isnull()]\n",
    "\n",
    "\n",
    "left_topics = left[left['QID'].isnull()]\n",
    "left_topics_tuple = tuple(list(left_topics['Topic']))\n",
    "\n",
    "len(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&sites=enwiki&titles=Steam_(software)&normalize=1\n",
    "\n",
    "import requests\n",
    "\n",
    "S = requests.Session()\n",
    "\n",
    "URL = \"https://www.mediawiki.org/w/api.php\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"action\": \"wbgetentities\",\n",
    "    \"sites\": \"enwiki\",\n",
    "    \"titles\": \"Steam_(software)\",\n",
    "    \"normalize\": \"1\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS)\n",
    "DATA = R.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('translation_topic_rec_df_CLEAN', len(translation_topic_rec_df_CLEAN))\n",
    "print('t_missing_wd:', len(t_missing_wd))\n",
    "print('t_w_wd:', len(t_w_wd))\n",
    "print('queried & found:', len(qid_t_query_results))\n",
    "print('queried again and found redirect suggs:', len(redirect_suggs_info_df))\n",
    "print('final df len:', len(translation_df_qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query for sitelinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clean = translation_df_qid[~translation_df_qid['QID'].isnull()]#w/o nulls\n",
    "\n",
    "translation_qids = tuple(list(t_clean['QID']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars.update({\n",
    "    'translation_qids' : translation_qids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "iwl_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "  ips_item_id AS QID,\n",
    "  GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "  COUNT(ips_site_page) AS iwsitelinks\n",
    "FROM wb_items_per_site\n",
    "      WHERE ips_item_id IN {translation_qids} \n",
    "GROUP BY QID\n",
    "\"\"\".format(**wd_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge to get df with article_suggestion, QID, iwsites, iwsitelinks\n",
    "translation_df_qid_iwl = iwl_r.merge(translation_df_qid, how=\"right\", on=['QID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "translation_df_qid_iwl[translation_df_qid_iwl.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [translation_df_qid_iwl, editing_df_qid_iwl]\n",
    "\n",
    "df = pd.concat(dfs, sort=True)\n",
    "#https://stackoverflow.com/questions/59124863/how-to-concat-or-merge-three-tables-with-different-number-of-columns-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('full_topic_rec_df:', len(full_topic_rec_df))\n",
    "print('processed df:', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the article is sometimes suggested as an article suggestion for more than one wiki\n",
    "duplicated_translation_recs = df[df.duplicated(['article_suggestion'])]\n",
    "dupe_check = df[df.duplicated(['article_suggestion', 'database_code', 'local_encoded_title'])]\n",
    "\n",
    "print(\"duplicated_translation_recs\", len(duplicated_translation_recs))\n",
    "print(\"dupes\", len(dupe_check))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#drop dupes\n",
    "t_rec_iwl_q = t_rec_iwl_q.drop_duplicates(subset=['article_suggestion', 'database_code', 'local_encoded_title', 'file'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/processed/query_results/topic_lists/indonesia/rec_qids_iwls.csv\", sep=',', encoding = 'utf-8', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
