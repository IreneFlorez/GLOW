{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOW  - Content Quality & Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"toc\"></a>\n",
    "\n",
    "* [Content](#top)\n",
    "    1. Identify and filter out redirects\n",
    "    [num editors/article](#nea)\n",
    "    2. translation\n",
    "    3. [pagelen & pagelen relative (composition)](#pagelen)\n",
    "    4. wikidata item\n",
    "    5. article creation date\n",
    "    6. [pageviews(use/utility)](#pv)\n",
    "    7. filter for new articles\n",
    "    8. edits & editors per article\n",
    "    9. edits & timestamps\n",
    "    10. editors per article\n",
    "    11. [talk page activity](#tpa) \n",
    "    12. article watch count\n",
    "    13. [revert rate(use/utility)](#rr)\n",
    "    14. [links(importance/integration)](#ol)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wmfdata as wmf\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from urllib import request\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "import weakref\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pprint\n",
    "\n",
    "#import jupyter_contrib_nbextensions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import datetime as dt \n",
    "from datetime import datetime, timedelta, date\n",
    "import dateutil\n",
    "\n",
    "#%load_ext sql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_vars' (dict)\n",
      "Stored 'quality_vars' (dict)\n"
     ]
    }
   ],
   "source": [
    "#ensure proper country and contest dates selected in data handling content quality section \n",
    "%run 2b_data_handling.ipynb\n",
    "#%store -r IN_median_vi\n",
    "%store -r median_values\n",
    "#%run ./data_collection/collecting_articles.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Content Quality<a class=\"anchor\" id=\"stage1\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Indonesia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/processed/query_results/content_quality/indonesia/articles_pageids_CLEAN.csv\", sep=',', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['page_id'] = df['page_id'].astype(int)\n",
    "df['filename'] = df['filename'].replace(' ', '_', regex=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#select rows that are missing ids & split into two dfs\n",
    "missing_id = df[df['page_id'].isnull()]\n",
    "w_id = df[~df['page_id'].isnull()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Add URL & article_info columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#create URL, article info, and page info URL columns\n",
    "articles['url'] = articles.wikicode.replace({'wiki':'.wiki'}, regex=True)\n",
    "\n",
    "articles['url_article_info'] = 'https://xtools.wmflabs.org/articleinfo/'+articles['url']\n",
    "articles['url_article_info'] = articles['url_article_info']+ 'pedia.org/' \n",
    "articles['url_article_info'] = articles['url_article_info'] +articles['page_title'] \n",
    "\n",
    "articles['url'] = 'https://' + articles['url'] +'pedia.org/wiki/' \n",
    "articles['url'] = articles['url'] + articles['page_title']\n",
    "\n",
    "#interim_frame['url_page_info'] = interim_frame['url'] + '&action=info'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set variables which will be used in the notebook for querying"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#replace 1) wcode & 2)article list for each wiki 3)rename the csv file at the bottom\n",
    "wcode = 'id'  #1\n",
    "article_list = idwiki_titles_normalized      #2      \n",
    "\n",
    "#replace contest start/end date, and country for each GLOW project\n",
    "wiki = wcode+'wiki'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fyi, 'raw_articles' aka article_list, later turns into clean_page_ids > clean_new_page_ids\n",
    "\n",
    "#update the query variable \n",
    "quality_vars.update({'raw_articles':idwiki_titles_normalized,\n",
    "                    #'raw_articles': wcode+'.wikipedia',\n",
    "                     'wiki_db': wiki\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "\n",
    "page_lens_redir_articles = []\n",
    "\n",
    "def get_page_lens_redirects_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to MariaDb for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with page info for non redirect articles\n",
    "    '''\n",
    "\n",
    "    clean_id_query = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_len AS page_len,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_id IN {ids}\n",
    "    '''\n",
    "    \n",
    "    clean_id_query_one_article = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_len AS page_len,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_id = {ids}\n",
    "    '''\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        \n",
    "        if len(ids)>= 2:\n",
    "            redirects_r = mariadb.run(clean_id_query.format(ids=ids), wiki)\n",
    "        else: redirects_r = mariadb.run(clean_id_query_one_article.format(ids=_id_), wiki)\n",
    "        page_lens_redir_articles.append(redirects_r)   \n",
    "    \n",
    "    return(page_lens_redir_articles)\n",
    "\n",
    "\n",
    "\n",
    "#\"\"\".format(start=\"2017-06\", end=\"2018-06\"), \"wikishared\")\n",
    "#\"\"\".format(pa_articles_2018=pa_articles_2018)\n",
    "# MIN(p1.page_touched) AS last_modified,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_page_lens_redirects_mariadb(df)\n",
    "redirects_r = pd.concat(page_lens_redir_articles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((redirects_r['p1_is_redirect']==1) & (redirects_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((redirects_r['p1_is_redirect']==1) | (redirects_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#we do not want any duplicates here\n",
    "redirects_r[redirects_r.index.duplicated()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# act on the results from redirects_r query results\n",
    "\n",
    "#create a df \n",
    "all_surviving_articles = redirects_r[['page_id','page_title', 'page_len', 'database_code']] \n",
    "\n",
    "#seperate the redirected items into their own df\n",
    "redirects = redirects_r.loc[redirects_r['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title','page_len']] \n",
    "\n",
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')\n",
    "\n",
    "#create a new wikicode column using quality_vars['wiki_db']\n",
    "#ffill could also work here\n",
    "#articles['wikicode'] = quality_vars['wiki_db']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create a tuple of clean non redirect page_ids to query \n",
    "clean_pageids = tuple(list(articles['page_id']))\n",
    "clean_pagetitles = (articles['page_title'])\n",
    "\n",
    "#non-normalized titles, use spaces instead of underscores and may include namespace name\n",
    "#create a tuple of clean page_titles denormalized (with spaces instead of underscores) for pulling wiki data items (see below)\n",
    "clean_titles_denormalized = tuple(list(clean_pagetitles.replace('_', ' ', regex=True)))\n",
    "\n",
    "#update the query variable to use it in queries\n",
    "quality_vars.update({'clean_pageids': clean_pageids,\n",
    "                     'clean_titles_denormalized': clean_titles_denormalized\n",
    "                    })"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(clean_pagetitles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative length"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#see notebook '_collect_relative...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN_median_vi\n",
    "articles = redirects_r.merge(df, on=['database_code', 'page_id', 'page_title'], how='right').merge(median_values, on='database_code', how='left')#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['relative_page_len'] = articles['page_len']/articles['FA_median_len']\n",
    "articles['relative_page_len'] = articles['relative_page_len'].clip(upper=1)\n",
    "del articles['FA_median_len']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://www.mediawiki.org/wiki/Content_translation\n",
    "https://www.mediawiki.org/wiki/Content_translation/Machine_Translation\n",
    "https://www.mediawiki.org/wiki/Extension:Translate\n",
    "https://www.mediawiki.org/wiki/Content_translation/Published_translations\n",
    "https://en.wikipedia.org//w/api.php?action=query&format=json&list=cxpublishedtranslations&to=hi&offset=500\n",
    "https://en.wikipedia.org//w/api.php?action=query&format=json&list=cxpublishedtranslations&to=hi&offset=500\n",
    "https://paws-public.wmflabs.org/paws-public/User:Isaac_(WMF)/content-translation-basics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mediawiki_page_history - https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/Mediawiki_page_history\n",
    "#fyi you need to truncate. Recommend 30 day period: https://meta.wikimedia.org/wiki/Research:Autoconfirmed_article_creation_trial#H14:_The_survival_rate_of_newly_created_articles_by_autoconfirmed_users_will_remain_stable\n",
    "# Article deletion rates and article timespan - http://files.grouplens.org/papers/lam_group2009_wikipedia-longer-tail.pdf\n",
    "\n",
    "#note - archive table in Mariadb holds revisions for pages that have been deleted\n",
    "\n",
    "#snapshot = \"{MWH_SNAPSHOT}\"\n",
    "#AND event_timestamp >=\"{contest_start}\"\n",
    "#AND event_timestamp <\"{contest_end}\"\n",
    "        \n",
    "at_edit_articles = []\n",
    "\n",
    "def get_translation_edits_hive(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with translation tool use data \n",
    "    '''\n",
    "    \n",
    "    #https://phabricator.wikimedia.org/T201539\n",
    "    #get list of articles that have an edit associated with the content translation tool\n",
    "\n",
    "    at_edits = \"\"\"\n",
    "    SELECT\n",
    "        page_id, \n",
    "        revision_tags AS at_edits,\n",
    "        wiki_db AS database_code\n",
    "    FROM wmf.mediawiki_history\n",
    "    WHERE\n",
    "        snapshot = '2020-07'\n",
    "        AND event_timestamp >= '2019-11-01'\n",
    "        AND event_timestamp < '2020-07-01'\n",
    "        AND page_namespace = 0\n",
    "        AND event_entity = 'revision'\n",
    "        AND revision_is_identity_reverted = False \n",
    "        AND revision_is_deleted_by_page_deletion = False\n",
    "        AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "        AND wiki_db = '{wiki_db}' \n",
    "        AND page_id IN {ids}\n",
    "    GROUP BY \n",
    "        page_id, revision_tags, wiki_db\n",
    "    \"\"\"\n",
    "        \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        translation_edits = spark.run(at_edits.format(ids=ids, wiki_db=wiki_db))                \n",
    "        at_edit_articles.append(translation_edits)   \n",
    "    \n",
    "    return(at_edit_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_create_articles = []\n",
    "\n",
    "def get_translation_create_hive(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with translation tool use data \n",
    "    '''\n",
    "\n",
    "    #https://phabricator.wikimedia.org/T201539\n",
    "    #get list of articles that were created, instep with use of the article translation tool \n",
    "    at_create = \"\"\"\n",
    "    SELECT\n",
    "        page_id, \n",
    "        revision_tags AS at_create,\n",
    "        wiki_db AS database_code\n",
    "    FROM wmf.mediawiki_history\n",
    "    WHERE\n",
    "        snapshot = '2020-07'\n",
    "        AND event_timestamp >= '2019-11-01'\n",
    "        AND event_timestamp < '2020-07-01'\n",
    "        AND page_namespace = 0\n",
    "        AND event_entity = 'page'\n",
    "        AND event_type = 'create'\n",
    "        AND revision_is_identity_reverted = False \n",
    "        AND revision_is_deleted_by_page_deletion = False\n",
    "        AND array_contains(revision_tags, \"contenttranslation\")   \n",
    "        AND wiki_db = '{wiki_db}' \n",
    "        AND page_id IN {ids}\n",
    "    GROUP BY \n",
    "        page_id, revision_tags, wiki_db\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        translation_edits = spark.run(at_create.format(ids=ids, wiki_db=wiki_db))                \n",
    "        at_create_articles.append(translation_edits)   \n",
    "    \n",
    "    return(at_create_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation_edits_hive(articles)\n",
    "at_edits = pd.concat(at_edit_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation_create_hive(articles)\n",
    "at_create = pd.concat(at_create_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = pd.merge(at_edits, at_create, how=\"left\", on=['page_id', 'database_code']).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column from at_edits, if there's data in at_edits\n",
    "translation['translation_tool'] = np.where(translation.at_edits.str.len()>1, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge \n",
    "articles = pd.merge(articles, translation, how=\"left\", on=['page_id', 'database_code'])#.fillna(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikidata Q item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.mediawiki.org/wiki/Wikibase/Schema/wb_items_per_site\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#wb_items_per_site site:quarry.wmflabs.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from normalized (underscore) to denormalized (spaces) for querying the wikidata table etc.\n",
    "articles['page_title'] = articles['page_title'].str.replace('_', ' ')\n",
    "articles['rpage_title'] = articles['rpage_title'].str.replace('_', ' ')\n",
    "\n",
    "#select rows that have rtitles\n",
    "rtitles = articles[~articles['rpage_id'].isnull()]\n",
    "\n",
    "#create tuples of the article_suggestions and wiki_codes to use when querying for the wikidata items\n",
    "editing_titles_denormalized_CLEAN = tuple(list(articles['page_title']))\n",
    "editing_titles_denormalized_database_codes_CLEAN = tuple(list(articles['database_code']))\n",
    "\n",
    "r_editing_titles_denormalized_CLEAN = tuple(list(rtitles['rpage_title']))\n",
    "r_editing_titles_denormalized_database_codes_CLEAN = tuple(list(rtitles['database_code']))\n",
    "\n",
    "#set up a dict variable to use with .format when querying\n",
    "wd_vars_3b = {}\n",
    "wd_vars_3b.update({\n",
    "    'editing_titles_denormalized' : editing_titles_denormalized_CLEAN,\n",
    "    'editing_titles_denormalized_db_codes' : editing_titles_denormalized_database_codes_CLEAN,\n",
    "    'r_editing_titles_denormalized': r_editing_titles_denormalized_CLEAN,\n",
    "    'r_editing_titles_denormalized_db_codes': r_editing_titles_denormalized_database_codes_CLEAN,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_simple_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS page_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {editing_titles_denormalized_db_codes} AND\n",
    "      ips_site_page IN {editing_titles_denormalized}\n",
    "\"\"\".format(**wd_vars_3b), \"wikidatawiki\")\n",
    "#\"\"\".format(**quality_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_qid_simple_r = wmf.mariadb.run(\"\"\"\n",
    "SELECT\n",
    "      ips_site_page AS rpage_title,\n",
    "      ips_item_id AS QID,\n",
    "      ips_site_id AS database_code\n",
    "FROM  wb_items_per_site  \n",
    "WHERE ips_site_id IN {r_editing_titles_denormalized_db_codes} AND\n",
    "      ips_site_page IN {r_editing_titles_denormalized}\n",
    "\"\"\".format(**wd_vars_3b), \"wikidatawiki\")\n",
    "#\"\"\".format(**quality_vars), \"wikidatawiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(qid_simple_r, on=['page_title', 'database_code'], how=\"left\")#.fillna(0)\n",
    "articles = articles.merge(r_qid_simple_r, on=['rpage_title', 'database_code'], how=\"left\")#.fillna(0)\n",
    "articles.loc[articles[\"QID_x\"].isnull(),'QID_x'] = articles[\"QID_y\"] \n",
    "articles = articles.drop(['QID_y'], axis=1).rename(columns={'QID_x':'QID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change titles from denormalized (spaces)  to normalized (underscore) \n",
    "articles['page_title'] = articles['page_title'].str.replace(' ', '_')\n",
    "articles['rpage_title'] = articles['rpage_title'].str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iwsitelinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create qid list from column for pulling categories (use quid_simple_r from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "ips_sites_articles = []\n",
    "qids = tuple(list(articles['QID']))\n",
    "\n",
    "def get_ips_sites_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MariaDB for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with interwiki links data \n",
    "    '''\n",
    "    \n",
    "    ips_sites_query = \"\"\"\n",
    "    SELECT\n",
    "      linked_item.ips_item_id AS QID,\n",
    "      GROUP_CONCAT(ips_site_id SEPARATOR ', ') AS iwsites,\n",
    "      COUNT(ips_site_page) AS iwsitelinks\n",
    "    FROM (\n",
    "          SELECT ips_item_id\n",
    "          FROM wb_items_per_site\n",
    "          WHERE ips_site_id = '{wiki_db}' \n",
    "          AND ips_item_id IN {qids}\n",
    "        ) AS linked_item\n",
    "    LEFT JOIN wb_items_per_site \n",
    "      ON linked_item.ips_item_id = wb_items_per_site.ips_item_id\n",
    "    LEFT JOIN page \n",
    "      ON linked_item.ips_item_id = page.page_id\n",
    "    GROUP BY page_id\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['QID'].apply(pd.DataFrame)\n",
    "        qids = tuple(list(grouping[wiki]))\n",
    "        _qid_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        ips_sites_query_results = wmf.mariadb.run(ips_sites_query.format(qids=qids, wiki_db=wiki_db), \"wikidatawiki\")                \n",
    "        ips_sites_articles.append(ips_sites_query_results)   \n",
    "    \n",
    "    return(ips_sites_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ips_sites_mariadb(qid_simple_r)\n",
    "ips_sites = pd.concat(ips_sites_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(ips_sites[['iwsitelinks', 'iwsites', 'QID']], on='QID', how=\"left\")#.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['QID'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean QID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['QID'] = articles['QID'].astype(int)\n",
    "#articles['QID'] = 'Q' + articles['QID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.drop_duplicates(subset=['page_title', 'database_code', 'page_id', 'QID'], keep='first') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# links <a class=\"anchor\" id=\"links\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagelinks and redirects examples\n",
    "#https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L227\n",
    "\n",
    "# incoming pagelinks: links from within the same wiki\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#resource: https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L229\n",
    "#\tpl_from, pl_from_namespace #anchor\n",
    "#\tpl_namespace\tpl_title #target\n",
    "\n",
    "#linking\n",
    "#https://en.wikipedia.org/wiki/Special:WhatLinksHere/Wikipedia:Manual_of_Style/Linking\n",
    "#https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Linking#General_principles\n",
    "#https://www.mediawiki.org/wiki/API:Links\n",
    "\n",
    "#backlinks+\n",
    "#https://dispenser.info.tm/~dispenser/cgi-bin/backlinkscount.py (backlinks)\n",
    "#https://github.com/wikimedia/mediawiki-api-demos/blob/master/python/get_backlinks.py\n",
    "\n",
    "#tables\n",
    "#pagelinks contains links to other pages on the same wiki...provide cohesion and utility\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table (internal links in the same wiki, from the page)\n",
    "\n",
    "#externallinks contains links to elsewhere, outside of all wikis\n",
    "#https://www.mediawiki.org/wiki/Manual:Externallinks_table (external links, from the page)\n",
    "\n",
    "#interwikilinks links an article in one language to the same article in another language. For most articles these are stored on Wikidata. \n",
    "#https://en.wikipedia.org/wiki/Help:Interwiki_linking\n",
    "#https://www.mediawiki.org/wiki/Manual:Iwlinks_table\n",
    "\n",
    "#langlinks links that point to a page on another wiki (e.g. [[mw:Product Analytics]] links to the PA team’s page on MediaWiki-wiki.\n",
    "#https://en.wikipedia.org/wiki/Help:Interlanguage_links#Local_links\n",
    "#https://www.mediawiki.org/wiki/Manual:Langlinks_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* pagelinks contains links to other pages on the same wiki, \n",
    "\n",
    "* externallinks contains links to elsewhere, outside of all wikis\n",
    "\n",
    "* interwikilinks links an article in one language to the same article in another language. For most articles these are stored on Wikidata. \n",
    "https://en.wikipedia.org/wiki/Help:Interwiki_linking\n",
    "\n",
    "* langlinks links that point to a page on another wiki (e.g. [[mw:Product Analytics]] links to the PA team’s page on MediaWiki-wiki.\n",
    "https://en.wikipedia.org/wiki/Help:Interlanguage_links#Local_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Handle entries where we didn't get page_id at the top (below queries rely on page_id)\n",
    "\n",
    "\n",
    "oel_articles = []\n",
    "ipl_articles = []\n",
    "opl_articles = []\n",
    "\n",
    "def get_links_info(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MariaDB for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with outgoing external links, outgoing page links \n",
    "    and incoming page links data \n",
    "    '''\n",
    "    \n",
    "    #pl.DATABASE() AS database_code,\n",
    "    opl_r = \"\"\"\n",
    "    SELECT \n",
    "        DATABASE() AS database_code,\n",
    "        pl.pl_from, \n",
    "        link.page_id AS plpage,\n",
    "        link.page_title AS plpage_title,\n",
    "        redir.page_id AS rpage,\n",
    "        redir.page_title AS rpage_title,\n",
    "        redir.page_is_redirect AS is_double_redirect\n",
    "    FROM pagelinks AS pl\n",
    "    JOIN page AS link\n",
    "        ON (pl.pl_namespace=link.page_namespace\n",
    "        AND pl.pl_title=link.page_title)\n",
    "    LEFT JOIN redirect AS rd\n",
    "        ON link.page_id=rd.rd_from\n",
    "    LEFT JOIN page AS redir\n",
    "        ON (rd.rd_namespace=redir.page_namespace\n",
    "        AND rd.rd_title=redir.page_title)\n",
    "    WHERE pl.pl_from IN {ids}\n",
    "    \"\"\"\n",
    "    \n",
    "    #link.DATABASE() AS database_code,\n",
    "    ipl_r = \"\"\"\n",
    "    SELECT \n",
    "        DATABASE() AS database_code,\n",
    "        link.page_id AS page_id,\n",
    "        pl.pl_title AS page_title,\n",
    "        pl.pl_from AS in_pagelinks\n",
    "    FROM pagelinks AS pl\n",
    "    JOIN page AS link\n",
    "        ON (pl.pl_namespace=link.page_namespace\n",
    "        AND pl.pl_title=link.page_title)\n",
    "    LEFT JOIN redirect AS rd\n",
    "        ON link.page_id=rd.rd_from\n",
    "    LEFT JOIN page AS redir\n",
    "        ON (rd.rd_namespace=redir.page_namespace\n",
    "        AND rd.rd_title=redir.page_title)\n",
    "    WHERE pl.pl_namespace=0\n",
    "        AND link.page_id IN {ids}\n",
    "    \"\"\"\n",
    "    \n",
    "    oel_r = \"\"\"\n",
    "    SELECT el_from AS page_id,\n",
    "    el_to AS oel_links\n",
    "    FROM externallinks\n",
    "    WHERE el_from IN {ids}\n",
    "    \"\"\"\n",
    "    \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        oel_query_results = wmf.mariadb.run(oel_r.format(ids=ids), wiki)                \n",
    "        oel_articles.append(oel_query_results)   \n",
    "        ipl_query_results = wmf.mariadb.run(ipl_r.format(ids=ids), wiki) \n",
    "        ipl_articles.append(ipl_query_results) \n",
    "        opl_query_results = wmf.mariadb.run(opl_r.format(ids=ids), wiki)                \n",
    "        opl_articles.append(opl_query_results) \n",
    "    \n",
    "    return(oel_articles, ipl_articles, opl_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_links_info(articles);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pagelinks <a class=\"anchor\" id=\"pagelinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagelinks: linking to articles within the same wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "opl = pd.concat(opl_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if any of the page_ids are double redirects\n",
    "((opl['is_double_redirect']==1).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>database_code</th>\n",
       "      <th>pl_from</th>\n",
       "      <th>plpage</th>\n",
       "      <th>plpage_title</th>\n",
       "      <th>rpage</th>\n",
       "      <th>rpage_title</th>\n",
       "      <th>is_double_redirect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [database_code, pl_from, plpage, plpage_title, rpage, rpage_title, is_double_redirect]\n",
       "Index: []"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if an anchor's target is duplicated...for duplicate instances of a link within a single page\n",
    "#pagelinks_r[(pagelinks_r.duplicated('pl_from') & pagelinks_r.duplicated('lpage'))] #checks for duplicates in either column\n",
    "opl[opl.duplicated(['pl_from','plpage'])] #checks for duplicates in two columns at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "opl = opl[['database_code', 'pl_from']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there are no duplicated targets from each anchor, we can count the number of occurrences for each anchor as the target_count\n",
    "#opls = opl['pl_from'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'pl_from':'oplinks'})\n",
    "\n",
    "opls = opl.groupby(['pl_from', 'database_code']).size().reset_index().rename(columns={'pl_from':'page_id', 0:'oplinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_in_content(opls)\n",
    "articles = articles.merge(opls, on=['page_id', 'database_code'], how=\"left\")#.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### external links <a class=\"anchor\" id=\"extlinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "oel = pd.concat(oel_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no external links are duplicated, then count targets arising from each anchor:\n",
    "#because there are no duplicated targets from each anchor, we can count the number of occurrences for each anchor as the target_count\n",
    "#oextlinks = oel_r['el_from'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'el_from':'oelinks'})\n",
    "\n",
    "#count unique links \n",
    "oextlinks = oel.groupby('page_id')['oel_links'].nunique().to_frame().reset_index()#.rename(columns={'el_from':'page_id', 'el_to': 'oelinks'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(oextlinks, on=['page_id'], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoming pagelinks <a class=\"anchor\" id=\"inpagelinks\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTES FROM MORTEN:\n",
    "Use backlinks query from here: https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/inlink-table-updater.py#L229 It already uses a list of page IDs as the basis for the query, but does limit links to within the article namespace.\n",
    "\n",
    "Then I'd just remove `AS ilc_page_id` and change `AS ilc_numlinks` to `AS numlinks` or something. That query is pretty optimized, and also counts inlinks coming in through redirects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipl = pd.concat(ipl_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incoming_pagelinks = incoming_pagelinks_r['page_id'].value_counts().to_frame().reset_index().rename(columns={'index':'page_id', 'page_id':'ipl_count'})\n",
    "\n",
    "#count unique links \n",
    "incoming_pagelinks = ipl.groupby(['page_id', 'database_code'])['in_pagelinks'].nunique().to_frame().reset_index().rename(columns={'in_pagelinks':'ipl_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_in_content(incoming_pagelinks)\n",
    "articles = articles.merge(incoming_pagelinks, on=['page_id', 'database_code'], how=\"left\")#.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article's talk page activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://meta.wikimedia.org/wiki/Research:Usage_of_talk_pages/2019-11-11#arwiki\n",
    "#https://meta.wikimedia.org/wiki/Research:Newsletter/2011/August\n",
    "#http://jodischneider.com/pubs/sac2011.pdf\n",
    "#https://meta.wikimedia.org/wiki/Research:Newsletter/2015/May#Editors_who_use_user_talk_pages_are_more_involved_in_high-quality_articles\n",
    "#https://meta.wikimedia.org/wiki/Research:New     vI will be happy to take a look at your queries. I will be happy to take a look at your queries. vsletter/2017/May#cite_note-9\n",
    "#https://www.opensym.org/wp-content/uploads/2018/07/OpenSym2018_paper_14.pdf\n",
    "#https://phabricator.wikimedia.org/T214935 -- on talk page click through rates\n",
    "#SQL:\n",
    "#https://github.com/wikimedia-research/Talkcicity/blob/master/retrieve_talkpage_data.R\n",
    "#https://github.com/x-tools/xtools/blob/master/src/AppBundle/Repository/ArticleInfoRepository.php#L221-L226\n",
    "#https://github.com/wikimedia-research/2019-10-talk-pages-baseline-metrics/blob/master/2019-10-talk-page-contributors-analysis.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.wikidata.org/wiki/Help:Sitelinks\n",
    "#https://www.wikidata.org/w/api.php?action=wbgetentities&ids=Q42&props=sitelinks\n",
    "\n",
    "talk_page_articles = []\n",
    "article_watch_count_articles = []\n",
    "rr_replicas_articles = []\n",
    "\n",
    "def get_talk_watch_rr_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MariaDB for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with data for talk page activity, watch count and reverts \n",
    "    '''\n",
    "    \n",
    "    # https://phabricator.wikimedia.org/source/mediawiki/browse/master/maintenance/tables.sql\n",
    "    article_watch_count_query = \"\"\"\n",
    "    SELECT \n",
    "           page_id,\n",
    "           DATABASE() AS database_code,\n",
    "           COUNT(*) AS watch_count\n",
    "    FROM watchlist\n",
    "    JOIN page \n",
    "         ON (wl_title = page_title AND wl_namespace = page_namespace)\n",
    "    WHERE page_namespace = 0 \n",
    "        AND page_id IN {ids}\n",
    "    GROUP BY page_id\n",
    "    \"\"\" \n",
    "    \n",
    "    talk_page_edits_query = \"\"\"\n",
    "    SELECT \n",
    "        DATABASE() AS database_code,\n",
    "        pa.page_id, \n",
    "        SUM(IF(rev_id IS NOT NULL, 1, 0)) AS talk_page_edits\n",
    "    FROM page pa\n",
    "    LEFT JOIN page pt\n",
    "        ON pa.page_title = pt.page_title\n",
    "        AND pt.page_namespace = 1\n",
    "    LEFT JOIN revision\n",
    "        ON pt.page_id = rev_page\n",
    "    WHERE pa.page_id IN {ids}\n",
    "        AND pt.page_namespace = 1\n",
    "        AND (rev_deleted & 4) = 0\n",
    "    GROUP BY pa.page_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # see the revert rate (rr) notebook\n",
    "    rr_replicas_query = \"\"\"\n",
    "    SELECT page_id, \n",
    "           DATABASE() AS database_code,\n",
    "           COUNT(DISTINCT rev_id) AS revertrate\n",
    "    FROM revision\n",
    "     JOIN change_tag \n",
    "         ON ct_rev_id = rev_id\n",
    "     JOIN change_tag_def \n",
    "         ON ct_id = ctd_id\n",
    "     JOIN page \n",
    "         ON rev_page = page_id\n",
    "    WHERE ctd_name IN ('mw-rollback', 'mw-undo')\n",
    "        AND page_id IN {ids}\n",
    "    GROUP BY page_id\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        \n",
    "        talk_page_edits_results = wmf.mariadb.run(talk_page_edits_query.format(ids=ids), wiki)\n",
    "        talk_page_articles.append(talk_page_edits_results)\n",
    "        \n",
    "        article_watch_count_results = wmf.mariadb.run(article_watch_count_query.format(ids=ids), wiki)\n",
    "        article_watch_count_articles.append(article_watch_count_results)\n",
    "        \n",
    "        rr_replicas_results = wmf.mariadb.run(rr_replicas_query.format(ids=ids), wiki) \n",
    "        rr_replicas_articles.append(rr_replicas_results)\n",
    "        \n",
    "    return(talk_page_articles, article_watch_count_articles, rr_replicas_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_talk_watch_rr_mariadb(articles);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk_page_edits = pd.concat(talk_page_articles)\n",
    "article_watch_count = pd.concat(article_watch_count_articles)\n",
    "rr_replicas = pd.concat(rr_replicas_articles)\n",
    "\n",
    "#merge_in_content(talk_page_edits)\n",
    "#merge_in_content(article_watch_count)\n",
    "#merge_in_content(rr_replicas)\n",
    "articles = articles.merge(talk_page_edits, on=['page_id', 'database_code'], how=\"left\").merge(article_watch_count, on=['page_id', 'database_code'], how=\"left\").merge(rr_replicas, on=['page_id', 'database_code'], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuepa_articles = []\n",
    "tunbre_articles = []\n",
    "tueme_articles = []\n",
    "tuipe_articles = []\n",
    "\n",
    "def get_editor_data_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MariaDB for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with data for the articles' editor data \n",
    "    '''\n",
    "    \n",
    "    # editors: total number of unique editors, including IP editors and bots...all editors of all edits, microcontributions\n",
    "    #https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "    #https://www.mediawiki.org/wiki/Manual:Revision_actor_temp_table\n",
    "    #https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "    #https://www.mediawiki.org/wiki/Help:RevisionDelete\n",
    "    #adapted from https://phabricator.wikimedia.org/T231598#5465711\n",
    "    #questioned in https://phabricator.wikimedia.org/T234560#5545319\n",
    "    #taken into account: rev_deleted to avoid leaking information re: how many distinct users were involved in revision-deleted edits\n",
    "\n",
    "    tuepa_q = \"\"\"\n",
    "    SELECT \n",
    "        page.page_id AS page_id,\n",
    "        COUNT(DISTINCT revactor_actor) AS all_editors_of_all_edits,\n",
    "        DATABASE() AS database_code\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "    JOIN page ON rev_page = page.page_id  \n",
    "    WHERE rev_page = page.page_id \n",
    "    AND (rev_deleted & 4) = 0\n",
    "    AND page.page_id IN {ids}\n",
    "    GROUP BY page_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # query post Morten review\n",
    "    #editors: total, unique, non-bot, registered editors that made non-minor edits\n",
    "    tunbre_q = \"\"\"\n",
    "    SELECT \n",
    "        revision.rev_page AS page_id,\n",
    "        COUNT(DISTINCT revactor_actor) AS editors_nm,\n",
    "        DATABASE() AS database_code\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON (revactor_rev = rev_id)\n",
    "    JOIN actor ON (revactor_actor = actor_id)\n",
    "    WHERE (rev_deleted & 4) = 0\n",
    "    AND rev_minor_edit = 0\n",
    "    AND actor_user IS NOT NULL -- user cannot be non-registered\n",
    "    AND actor_user NOT IN (SELECT ug_user FROM user_groups WHERE ug_group = \"bot\") -- not a bot\n",
    "    AND revision.rev_page IN {ids}\n",
    "    GROUP BY revision.rev_page\n",
    "    \"\"\"\n",
    "    \n",
    "    # editors: total number of unique editors, including IP editors and bots...all editors of all edits, microcontributions\n",
    "    #https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "    #https://www.mediawiki.org/wiki/Manual:Revision_actor_temp_table\n",
    "    #https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "    #https://www.mediawiki.org/wiki/Help:RevisionDelete\n",
    "    #adapted from https://phabricator.wikimedia.org/T231598#5465711\n",
    "    #questioned in https://phabricator.wikimedia.org/T234560#5545319\n",
    "    #taken into account: rev_deleted to avoid leaking information re: how many distinct users were involved in revision-deleted edits\n",
    "    \n",
    "    tueme_q = \"\"\"\n",
    "    SELECT \n",
    "        page.page_id,\n",
    "        COUNT(DISTINCT revactor_actor) AS micro_editors,\n",
    "        DATABASE() AS database_code\n",
    "    FROM revision_actor_temp\n",
    "      JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "      JOIN page ON rev_page = page.page_id  \n",
    "    WHERE rev_page = page.page_id \n",
    "      AND (rev_deleted & 4) = 0\n",
    "      AND page.page_id IN {ids}\n",
    "      AND rev_minor_edit = 1\n",
    "    GROUP BY page_id\n",
    "    \"\"\"\n",
    "    \n",
    "    # updated!\n",
    "    #total unique IP editors, \n",
    "    #see also: https://phabricator.wikimedia.org/T231605\n",
    "    #https://meta.wikimedia.org/wiki/IP_Editing:_Privacy_Enhancement_and_Abuse_Mitigation/Research\n",
    "    #https://github.com/nettrom/AHT-block-effectiveness-2018\n",
    "    #https://github.com/wikimedia-research/AHT-IP-edits-2019/blob/master/edit_usefulness.ipynb\n",
    "    #https://meta.wikimedia.org/wiki/User:Benjamin_Mako_Hill/Research_on_the_value_of_IP_Editing\n",
    "    tuipe_q = \"\"\"\n",
    "    SELECT \n",
    "      revision.rev_page AS page_id,\n",
    "      COUNT(DISTINCT revactor_actor) AS IP_editors,\n",
    "      DATABASE() AS database_code\n",
    "    FROM revision\n",
    "      JOIN revision_actor_temp ON (rev_id = revactor_rev)\n",
    "      JOIN actor ON (revactor_actor = actor_id)\n",
    "    WHERE (rev_deleted & 4) = 0\n",
    "      AND actor_user IS NULL -- non-registered user\n",
    "      AND revision.rev_page IN {ids}\n",
    "    GROUP BY rev_page\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        \n",
    "        tuepa_results = wmf.mariadb.run(tuepa_q.format(ids=ids), wiki)\n",
    "        tuepa_articles.append(tuepa_results)\n",
    "        \n",
    "        tunbre_results = wmf.mariadb.run(tunbre_q.format(ids=ids), wiki)\n",
    "        tunbre_articles.append(tunbre_results)\n",
    "        \n",
    "        tueme_results = wmf.mariadb.run(tueme_q.format(ids=ids), wiki) \n",
    "        tueme_articles.append(tueme_results)\n",
    "        \n",
    "        tuipe_results = wmf.mariadb.run(tuipe_q.format(ids=ids), wiki) \n",
    "        tuipe_articles.append(tuipe_results)\n",
    "        \n",
    "    return(tuepa_articles, tunbre_articles, tueme_articles, tuipe_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_editor_data_mariadb(articles);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuepa_r = pd.concat(tuepa_articles)\n",
    "tunbre_r = pd.concat(tunbre_articles)\n",
    "tueme_r = pd.concat(tueme_articles)\n",
    "tuipe_r = pd.concat(tuipe_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### editor calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_calculations = pd.merge(tuepa_r, tunbre_r[['page_id', 'editors_nm', 'database_code']], on=['page_id','database_code'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_calculations = pd.merge(editor_calculations, tuipe_r[['page_id', 'IP_editors','database_code']],\n",
    "                               on=['page_id','database_code'], \n",
    "                               how='left').merge(tueme_r[['page_id', 'micro_editors','database_code']], \n",
    "                               on=['page_id','database_code'], \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.merge(editor_calculations, on=['page_id', 'database_code'], how=\"left\")\n",
    "#merge_in_content(editor_calculations);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This timestamp is updated whenever the page changes in a way requiring it to be re-rendered, invalidating caches. \n",
    "#Aside from editing, this includes permission changes, creation or deletion of linked pages, and alteration of contained templates. \n",
    "#[[mw:Manual:Revision_table]] and [[mw:Manual:Page_table]]. Only show latest edits does an inner join from revision table to page table on rev_id = page_latest .\n",
    "#https://www.mediawiki.org/wiki/Manual:Revision_table\n",
    "#https://github.com/x-tools/xtools/blob/master/src/AppBundle/Repository/ArticleInfoRepository.php#L162-L171\n",
    "#https://xtools.wmflabs.org/articleinfo/pa.wikipedia.org/ਏਸ਼ੀਆ\n",
    "\n",
    "edits_articles = []\n",
    "timestamps_articles = []\n",
    "\n",
    "def get_edits_timestamps_mariadb(df):\n",
    "    \n",
    "    edits_q = \"\"\"\n",
    "    SELECT \n",
    "        rev_page AS page_id, \n",
    "        DATABASE() AS database_code,\n",
    "        COUNT(rev_id) AS num_edits_all_time,\n",
    "        SUM(rev_minor_edit) AS minor_edits_all_time\n",
    "    FROM revision\n",
    "    JOIN page ON page_id = rev_page\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 0 \n",
    "        AND rev_page IN {ids}\n",
    "    GROUP BY rev_page\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    timestamps_q = \"\"\"\n",
    "    SELECT \n",
    "        rev_page AS page_id, \n",
    "        DATABASE() AS database_code,\n",
    "        max(rev_timestamp) AS last_edited \n",
    "    FROM revision \n",
    "    JOIN page \n",
    "      ON page_id = rev_page\n",
    "    WHERE rev_page IN {ids}\n",
    "    AND rev_id = page_latest\n",
    "    GROUP BY rev_page\n",
    "    \"\"\"\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        \n",
    "        edits_results = wmf.mariadb.run(edits_q.format(ids=ids), wiki)\n",
    "        edits_articles.append(edits_results)\n",
    "        \n",
    "        timestamps_results = wmf.mariadb.run(timestamps_q.format(ids=ids), wiki)\n",
    "        timestamps_articles.append(timestamps_results)\n",
    "        \n",
    "    return(edits_articles, timestamps_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edits_timestamps_mariadb(articles);\n",
    "\n",
    "edits = pd.concat(edits_articles)\n",
    "timestamps = pd.concat(timestamps_articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_in_content(edits)\n",
    "#merge_in_content(timestamps)\n",
    "\n",
    "timestamps['last_edited']= pd.to_datetime(timestamps['last_edited']) \n",
    "timestamps['last_edited'] = timestamps['last_edited'].dt.normalize()\n",
    "\n",
    "articles = articles.merge(edits, on=['page_id', 'database_code'], how=\"left\").merge(timestamps, on=['page_id', 'database_code'], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Creation Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS first_edited\n",
    "\n",
    "first_edit_timestamp_articles = []\n",
    "\n",
    "def get_first_edit_timestamp_mariadb(df):\n",
    "    \n",
    "    first_edit_timestamp_q = \"\"\"\n",
    "    SELECT \n",
    "        page_id, \n",
    "        DATABASE() AS database_code,\n",
    "        rev_timestamp AS first_edited\n",
    "    FROM revision \n",
    "    JOIN page ON page_id = rev_page\n",
    "    WHERE rev_page IN {ids}\n",
    "    GROUP BY page_id\n",
    "    \"\"\"\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        \n",
    "        first_edit_timestamp_results = wmf.mariadb.run(first_edit_timestamp_q.format(ids=ids), wiki)\n",
    "        first_edit_timestamp_articles.append(first_edit_timestamp_results)\n",
    "        \n",
    "    return(first_edit_timestamp_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_first_edit_timestamp_mariadb(articles);\n",
    "first_edit_timestamp = pd.concat(first_edit_timestamp_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_edit_timestamp['first_edited']= pd.to_datetime(first_edit_timestamp['first_edited']) \n",
    "first_edit_timestamp['first_edited'] = first_edit_timestamp['first_edited'].dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge_in_content(first_edit_timestamp)\n",
    "articles = articles.merge(first_edit_timestamp, on=['page_id', 'database_code'], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.drop_duplicates(subset=['page_title', 'database_code', 'page_id', 'QID','contest_article_type'], keep='first') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify article types (expanded, new, post)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#India articles which have 1 category with a single start and end date\n",
    "articles['first_edited']= pd.to_datetime(articles['first_edited']) \n",
    "articles['first_edited'] = articles['first_edited'].dt.normalize()\n",
    "\n",
    "#identify article types based on time of first edit: 'new', 'expanded', or 'post'\n",
    "articles['article_type'] = articles['first_edited'].apply(create_fill_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indonesia articles which have three categories with different start/end dates\n",
    "articles['filename'] = articles['filename'].replace(' ', '_', regex=True)\n",
    "articles['first_edited'] = pd.to_datetime(articles['first_edited'], errors='coerce')\n",
    "\n",
    "mc = ['monthly_challenge.csv']\n",
    "wc = ['writing_contest.csv']\n",
    "gc = ['Indonesia_grant.csv', 'Jawa_grant.csv', 'Minangkabau_grant.csv',\n",
    "       'Sunda_grant.csv']\n",
    "\n",
    "\n",
    "#articles[\"article_type\"] = np.nan #add new empty column to fill below\n",
    "\n",
    "articles.loc[(articles[\"filename\"].isin(mc)) & (articles[\"first_edited\"] < contest_start_indonesia_monthly_challenge), \"article_type\"] = 'expanded'\n",
    "articles.loc[(articles[\"filename\"].isin(mc)) & (articles[\"first_edited\"] >= contest_start_indonesia_monthly_challenge), \"article_type\"] = 'new'\n",
    "articles.loc[(articles[\"filename\"].isin(mc)) & (articles[\"first_edited\"] > contest_end_indonesia_monthly_challenge), \"article_type\"] = 'post'\n",
    "\n",
    "articles.loc[(articles[\"filename\"].isin(wc)) & (articles[\"first_edited\"] < contest_start_indonesia_writing_contest), \"article_type\"] = 'expanded'\n",
    "articles.loc[(articles[\"filename\"].isin(wc)) & (articles[\"first_edited\"] >= contest_start_indonesia_writing_contest), \"article_type\"] = 'new'\n",
    "articles.loc[(articles[\"filename\"].isin(wc)) & (articles[\"first_edited\"] > contest_end_indonesia_writing_contest), \"article_type\"] = 'post'\n",
    "\n",
    "articles.loc[(articles[\"filename\"].isin(gc)) & (articles[\"first_edited\"] < contest_start_indonesia_grantee_comm_gathering), \"article_type\"] = 'expanded'\n",
    "articles.loc[(articles[\"filename\"].isin(gc)) & (articles[\"first_edited\"] >= contest_start_indonesia_grantee_comm_gathering), \"article_type\"] = 'new'\n",
    "articles.loc[(articles[\"filename\"].isin(gc)) & (articles[\"first_edited\"] > contest_end_indonesia_grantee_comm_gathering), \"article_type\"] = 'post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edits & Editors per article for time filtering <a class=\"anchor\" id=\"editors_active\"></a>\n",
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#select only new articles\n",
    "#articles_new = articles[articles.article_type =='new']\n",
    "select_list = ['new', 'post']\n",
    "articles_new = articles[articles['article_type'].isin(select_list)]\n",
    "\n",
    "#clean_new_pageids = tuple(list(articles_new['page_id']))\n",
    "#clean_new_pagetitles_list = list(articles_new['page_title'])\n",
    "#quality_vars.update({'clean_new_pageids': clean_new_pageids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS first_edited\n",
    "#https://phabricator.wikimedia.org/T231598\n",
    "#The (rev_deleted & 4) = 0 condition is to exclude revisions where the user has been RevDeled, as we don't want to leak information about how many such users there are.\n",
    "\n",
    "edits_editors_all_articles = []\n",
    "edits_editors_reg_articles = []\n",
    "\n",
    "def get_edits_editors_mariadb(df):\n",
    "    \n",
    "    edits_editors_all_q = \"\"\"\n",
    "    SELECT \n",
    "        page_id,\n",
    "        DATABASE() AS database_code,\n",
    "        DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,    \n",
    "        revactor_actor\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "    JOIN page ON rev_page = page.page_id\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 0 \n",
    "        AND (rev_deleted & 4) = 0\n",
    "        AND rev_page IN {ids}\n",
    "    GROUP BY revactor_rev\n",
    "    \"\"\"\n",
    "\n",
    "    edits_editors_reg_q = \"\"\"\n",
    "    SELECT \n",
    "        page_id,\n",
    "        DATABASE() AS database_code,\n",
    "        DATE_FORMAT(rev_timestamp,\"%y-%m-%d\") AS edit_date,    \n",
    "        revactor_actor\n",
    "    FROM revision_actor_temp\n",
    "    JOIN revision ON(revactor_rev = rev_id AND revactor_page = rev_page)\n",
    "    JOIN page ON rev_page = page.page_id\n",
    "    JOIN actor ON (revactor_actor = actor_id)\n",
    "    WHERE rev_page = page_id\n",
    "        AND rev_timestamp > 0 \n",
    "        AND (rev_deleted & 4) = 0\n",
    "        AND actor_user IS NOT NULL -- user cannot be non-registered\n",
    "        AND actor_user NOT IN (SELECT ug_user FROM user_groups WHERE ug_group = \"bot\") -- not a bot\n",
    "        AND rev_page IN {ids}\n",
    "    GROUP BY revactor_rev\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    for wiki in df['database_code'].unique():\n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_id'].apply(pd.DataFrame)\n",
    "        ids = tuple(list(grouping[wiki]))\n",
    "        _id_ = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        wiki_db = wiki\n",
    "        \n",
    "        edits_editors_all_results = wmf.mariadb.run(edits_editors_all_q.format(ids=ids), wiki)\n",
    "        edits_editors_all_articles.append(edits_editors_all_results)\n",
    "        \n",
    "        edits_editors_reg_results = wmf.mariadb.run(edits_editors_reg_q.format(ids=ids), wiki)\n",
    "        edits_editors_reg_articles.append(edits_editors_reg_results)\n",
    "        \n",
    "    return(edits_editors_all_articles, edits_editors_reg_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_edits_editors_mariadb(articles_new)\n",
    "edits_editors_all = pd.concat(edits_editors_all_articles)\n",
    "edits_editors_reg_r = pd.concat(edits_editors_reg_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_editors_all['edit_date'] = pd.to_datetime(edits_editors_all['edit_date'], format=\"%y-%m-%d\")\n",
    "edits_editors_reg_r['edit_date'] = pd.to_datetime(edits_editors_reg_r['edit_date'], format=\"%y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge \n",
    "ee_fe = pd.merge(edits_editors_all, articles[['page_id','page_title', 'first_edited', 'database_code']], on=['page_id', 'database_code'], how='left').fillna(0)\n",
    "# merge \n",
    "eern_fe = pd.merge(edits_editors_reg_r, articles[['page_id','page_title', 'first_edited','database_code']], on=['page_id', 'database_code'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a timedelta column\n",
    "eern_fe['edit_td'] = eern_fe['edit_date']-eern_fe['first_edited']\n",
    "\n",
    "# filter for only edits in first 30 days\n",
    "m1 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(30, unit='d')]\n",
    "\n",
    "# filter for only edits in first 60 days\n",
    "m2 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(60, unit='d')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter for edits by time period and use those in groupby counts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1['revactor_actor'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for only edits in first 30 days\n",
    "m1 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(30, unit='d')]\n",
    "\n",
    "# filter for only edits in first 60 days\n",
    "m2 = eern_fe[eern_fe['edit_td'] <= pd.Timedelta(60, unit='d')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### editor counts per article at 1M, 2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_1M_counts = m1.groupby(['page_id', 'page_title', 'database_code'])['revactor_actor'].nunique().reset_index(name='editors_1stM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_2M_counts = m2.groupby(['page_id', 'page_title', 'database_code'])['revactor_actor'].nunique().reset_index(name='editors_2ndM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "editors_by_M_calculations = pd.merge(editors_1M_counts, editors_2M_counts[['page_id', 'editors_2ndM', 'database_code']],\n",
    "                               on=['page_id', 'database_code'], \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### edit counts per article at 1M, 2M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_counts_1M = m1.groupby(['page_id', 'page_title', 'database_code'])['edit_date'].agg('count').reset_index(name='edits_1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_counts_2M = m2.groupby(['page_id', 'page_title', 'database_code'])['edit_date'].agg('count').reset_index(name='edits_2M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits_by_M_calculations = pd.merge(edit_counts_1M, edit_counts_2M[['page_id', 'edits_2M', 'database_code']],\n",
    "                               on=['page_id', 'database_code'], \n",
    "                               how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.merge(articles, editors_1M_counts[['page_id', 'editors_1stM', 'database_code']],\n",
    "                               on=['page_id', 'database_code'], \n",
    "                               how='left').merge(edit_counts_1M[['page_id', 'edits_1M', 'database_code']], \n",
    "                               on=['page_id', 'database_code'], \n",
    "                               how='left')#.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.drop_duplicates(subset=['page_title', 'database_code', 'page_id', 'filename'], keep='first') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.loc[articles['QID']==0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#manual review and data entry for those which obtained o results in query\n",
    "\n",
    "articles.at[1791, 'page_id'] = 7450773 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.to_csv(\"../../data/processed/query_results/content_quality/indonesia/CQ_all_articles.csv\", sep=',', encoding = 'utf-8', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
