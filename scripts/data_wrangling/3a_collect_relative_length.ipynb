{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "#from ratelimiter import RateLimiter\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "#%load_ext sql_magic\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import wmfdata as wmf\n",
    "from wmfdata import charting, mariadb, hive\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r query_vars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TO DO FIX SPARQL LOOP CALL. At present, this code successfully gets and collects data for the first wiki in the list... then it gets stuck after line 32 and doesn't make it to line 34 for any of the following wikis in the list. \n",
    "\n",
    "ac_dict = {}\n",
    "wikicodeslist = (query_vars[\"india_wiki_codes\"])\n",
    "\n",
    "for wikicode in wikicodeslist.lower().split(\",\"):\n",
    "    ac_dict = {}\n",
    "    \n",
    "    @RateLimiter(max_calls=10, period=1)\n",
    "    def do_something():\n",
    "        pass\n",
    "\n",
    "    rate_limiter = RateLimiter(max_calls=10, period=1)\n",
    "\n",
    "    for i in range(100):\n",
    "        with rate_limiter:\n",
    "            do_something()\n",
    "                \n",
    "    try: \n",
    "        \n",
    "        endpoint = \"https://query.wikidata.org/sparql\"        \n",
    "        sparql = SPARQLWrapper(endpoint)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        sparql.setQuery(\"\"\"\n",
    "        SELECT ?item ?sitelink ?itemLabel \n",
    "        WHERE {\n",
    "          ?sitelink schema:isPartOf <https://%s.wikipedia.org/>;\n",
    "             schema:about ?item;\n",
    "             wikibase:badge wd:Q17437796 . # Sitelink is badged as a Featured Article\n",
    "            SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\" } .\n",
    "        } ORDER BY ?itemLabel\n",
    "        \"\"\"% wikicode) \n",
    "    \n",
    "        results = sparql.query().convert()\n",
    "        ac_dict.update(results)\n",
    "        t_ac = []\n",
    "        t_ac.append(ac_dict.copy())\n",
    "    except Exception:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the absence of a functional SPARQL LOOP CALL, run SPARQL Query manually for each wiki: https://w.wiki/Gh$\n",
    "\n",
    "#download results into csv files, one for each wiki\n",
    "#note, some wikis don't have featured articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data/raw/relative_length/indonesia/\")\n",
    "\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all files in the list\n",
    "combined = pd.concat([pd.read_csv(f) for f in all_filenames ], sort=True)\n",
    "#export to csv\n",
    "combined.to_csv( \"_combined.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"./data/raw/relative_length\")\n",
    "combined_r = pd.read_csv(\"_combined.csv\")\n",
    "combined_r2 = combined_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract wikicode\n",
    "combined_r2['wikicode'] = combined_r2['sitelink'].str.extract('([a-z]+(?=\\.))', expand=True)\n",
    "\n",
    "#add 'wiki'\n",
    "combined_r2['wikicode'] = combined_r2['wikicode']+'wiki'\n",
    "\n",
    "#extract url title\n",
    "combined_r2['encoded_title'] = combined_r2['sitelink'].str.extract('([^\\/]+$)', expand=True)\n",
    "\n",
    "#encoded URL to decoded title\n",
    "combined_r2['page_title'] = combined_r2['encoded_title'].apply(lambda x: unquote(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_r2 = combined_r2.rename(columns={'wikicode':'database_code'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#create tuple object containing page_title values given wikicode value in wikicode column\n",
    "grouped_titles = combined_r2.groupby('wikicode')['page_title'].apply(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/nettrom/suggestbot/blob/master/tool-labs/link-rec/link-recommender.py#L208\n",
    "#https://www.mediawiki.org/wiki/Manual:Redirect_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Page_table\n",
    "#https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "#--rd.redirect_id -- where is this field located? in which table can it be found?\n",
    "\n",
    "articles = []\n",
    "\n",
    "def get_clean_ids_mariadb(df):\n",
    "    \n",
    "    '''\n",
    "    Connect to the MediaWiki databases for the wikis found\n",
    "    in the given `pandas.DataFrame` `df` and return a\n",
    "    DataFrame with pageids for non redirect articles\n",
    "    '''\n",
    "\n",
    "    clean_id_query = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_len page_len,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title IN {raw_articles}\n",
    "    '''\n",
    "    \n",
    "    clean_id_query_one_article = '''\n",
    "    SELECT \n",
    "       p1.page_title AS page_title,\n",
    "       DATABASE() AS database_code,\n",
    "       p1.page_id  AS page_id,\n",
    "       p1.page_len page_len,\n",
    "       p1.page_is_redirect AS p1_is_redirect,\n",
    "       p2.page_id AS rpage_id,\n",
    "       p2.page_title AS rpage_title,\n",
    "       p2.page_len rpage_len,\n",
    "       p2.page_is_redirect AS is_double_redirect\n",
    "    FROM page AS p1 \n",
    "    LEFT JOIN redirect AS rd \n",
    "        ON p1.page_id=rd.rd_from \n",
    "    LEFT JOIN page AS p2 \n",
    "        ON (rd_namespace = p2.page_namespace)\n",
    "            AND rd_title = p2.page_title  \n",
    "    WHERE p1.page_namespace = 0\n",
    "        AND p1.page_title = '{raw_articles}'\n",
    "    '''\n",
    "\n",
    "    for wiki in df['database_code'].unique():\n",
    "        print('***')\n",
    "        print(wiki)\n",
    "        \n",
    "        grouping = df.loc[df['database_code'] == wiki].groupby('database_code')['page_title'].apply(pd.DataFrame)\n",
    "        raw_articles = tuple(list(grouping[wiki]))\n",
    "        article = grouping.reset_index(drop=True).iloc[0][0]\n",
    "        \n",
    "        if len(raw_articles)>= 2:\n",
    "            redirects_r = mariadb.run(clean_id_query.format(raw_articles=raw_articles), wiki )\n",
    "        else: redirects_r = mariadb.run(clean_id_query_one_article.format(raw_articles=article), wiki )\n",
    "        articles.append(redirects_r)   \n",
    "    \n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "idwiki\n",
      "***\n",
      "jvwiki\n",
      "***\n",
      "suwiki\n",
      "***\n",
      "minwiki\n"
     ]
    }
   ],
   "source": [
    "get_clean_ids_mariadb(combined_r2)\n",
    "featuredtitles = pd.concat(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((featuredtitles['p1_is_redirect']==1) & (featuredtitles['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((featuredtitles['p1_is_redirect']==1) | (featuredtitles['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results ...featured_pages_redirects_r\n",
    "\n",
    "#create a df \n",
    "all_surviving_articles = featuredtitles[['page_id','page_title', 'page_len']] \n",
    "\n",
    "#seperate the redirected items into their own df\n",
    "redirects = featuredtitles.loc[featuredtitles['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df = redirects[['page_id','page_title','page_len']] \n",
    "\n",
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "articles =  all_surviving_articles[~all_surviving_articles.isin(redirect_df)].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicates\n",
    "article_dupe_count = articles.groupby(['page_id','page_title', 'page_len']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicates\n",
    "articles_clean = articles.drop_duplicates(subset=['page_id', 'page_title'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>page_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [page_id, page_title, page_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for nulls\n",
    "articles_clean[articles_clean.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine combined_r2 and articles_clean3 on page_title\n",
    "df = pd.merge(combined_r2, articles_clean, on='page_title', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nulls\n",
    "df[df.isnull().any(axis=1)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df of just the null rows\n",
    "df_nulls = df[df.isnull().any(axis=1)]\n",
    "\n",
    "#remove the null items from the df \n",
    "df_clean =  df[~df.isin(df_nulls)].dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nulls.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#prepare for #manual review and data entry for those nulls which obtained NAN results in mariadb query\n",
    "#make sure I'm getting what I think I'm getting (see next cell)\n",
    "df_nulls.iat[12,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#manual review and data entry for those which obtained NAN results in mariadb query\n",
    "#fyi: no info for Pankaj Jaiswal[0], Barak[12] nor Rajkumar [14]\n",
    "\n",
    "df_nulls.iat[3,6] =  17150 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a df of just the null rows\n",
    "df_no_info = df_nulls[df_nulls.isnull().any(axis=1)]\n",
    "\n",
    "#remove the null items from the df \n",
    "df_formerly_nulls =  df_nulls[~df_nulls.isin(df_no_info)].dropna(how='all')\n",
    "\n",
    "df_no_info.reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the formerly nulls back into the df\n",
    "df_relative_length = pd.concat([df_clean, df_formerly_nulls], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert page_len to int\n",
    "df_relative_length['page_len'] = df_relative_length['page_len'].astype(str).astype(int);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate results by pulling a sample and checking page info\n",
    "#https://{}.wikipedia.org/w/index.php?title={}&action=info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df to csv\n",
    "path = '../../../../data/processed/relative_length/'\n",
    "df_relative_length.to_csv(path+'indonesia_relative_lengths.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of clean-non-null entries of featured articles in each wiki\n",
    "df_relative_length.groupby(df_relative_length['wikicode'])['page_len'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count of clean-non-null entries of featured articles in each wiki\n",
    "df_relative_length.groupby(df_relative_length['database_code'])['page_len'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values = df_relative_length.groupby(df_relative_length['database_code'])['page_len'].median().rename_axis(['database_code']).rename('FA_median_len').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'median_values' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store median_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address pawiki which only had one featured article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikicode_pa has only one featured article\n",
    "df_relative_length.loc[df_relative_length['wikicode'] == 'pawiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value_pa = df_relative_length[df_relative_length['wikicode']== 'pawiki'].page_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Address tcywiki and satwiki which had zero featured articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcy_articles_r = wmf.mariadb.run(articles_len_wiki_query, 'tcywiki')\n",
    "sat_articles_r = wmf.mariadb.run(articles_len_wiki_query, 'satwiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start by cleaning tcy_articles_r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((tcy_articles_r['p1_is_redirect']==1) & (tcy_articles_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((tcy_articles_r['p1_is_redirect']==1) | (tcy_articles_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results ...featured_pages_redirects_r\n",
    "\n",
    "#create a df \n",
    "all_surviving_articles_tcy = tcy_articles_r[['page_id','page_title', 'page_len']] \n",
    "\n",
    "#seperate the redirected items into their own df\n",
    "redirects_tcy = tcy_articles_r.loc[tcy_articles_r['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df_tcy = redirects_tcy[['page_id','page_title','page_len']] \n",
    "\n",
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "articles_tcy =  all_surviving_articles_tcy[~all_surviving_articles_tcy.isin(redirect_df_tcy)].dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now clean sat_articles_r data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now with sat_articles_r\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((sat_articles_r['p1_is_redirect']==1) & (sat_articles_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# |\n",
    "#check to see if any of the page_ids are redirects or double redirects\n",
    "((sat_articles_r['p1_is_redirect']==1) | (sat_articles_r['is_double_redirect']==1)).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act on the results ...featured_pages_redirects_r\n",
    "\n",
    "#create a df \n",
    "all_surviving_articles_sat = sat_articles_r[['page_id','page_title', 'page_len']] \n",
    "\n",
    "#seperate the redirected items into their own df\n",
    "redirects_sat = sat_articles_r.loc[sat_articles_r['p1_is_redirect']==1]\n",
    "#pull only p1.page_id, p1.page_title, p1.page_len \n",
    "redirect_df_sat = redirects_sat[['page_id','page_title','page_len']] \n",
    "\n",
    "#remove the redirect items from the all_surviving_articles df & create global articles df\n",
    "articles_sat =  all_surviving_articles_sat[~all_surviving_articles_sat.isin(redirect_df_sat)].dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values_sat = articles_sat['page_len'].median()\n",
    "median_values_sat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create an index of median values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first convert tcy and sat median values into a df\n",
    "tcy_sat = pd.DataFrame({'wikicode': {0:'satwiki', 1: 'tcywiki'},\n",
    "                        'page_len': {0:6438, 1:3133}\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_values = median_values.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value_pa = median_value_pa.to_frame().reset_index().rename(columns={'index':'wikicode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value_pa['wikicode']= median_value_pa['wikicode'].map({461:'pawiki'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/venv/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "IN_median_vi = pd.concat([median_values,median_value_pa, tcy_sat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_median_vi.sort_values(['page_len']).reset_index(drop=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_median_vi.rename(columns={'page_len': 'mpl_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'IN_median_vi' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store IN_median_vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
